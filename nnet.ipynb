{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNET\n",
    "\n",
    "This is my Jupyter notebook with notes / implementation of a shallow neural network in Julia. All materials are based on deeplearning.ai's Deep Learning Specialization. I refer to specific videos and other materials where applicable. **The notebook is not yet finished**\n",
    "\n",
    "I created this notebook to review some of the concepts I learned in the first three courses of the deep learning specialization. Its purpose is primarily for me to process the information and produce the implementation from my own understanding rather than simply copying it from another place. As a result, you may find that the implementation notes are somewhat long-winded or tangential.\n",
    "\n",
    "If you find a mistake or think this code can be improved, it would be great if you could fork the repository and make a pull request. You could also leave a comment in the issues section.\n",
    "\n",
    "### This notebook contains\n",
    "\n",
    "- Loading & pre-processing the MNIST dataset\n",
    "- forward and backward propagation using a 2-layer neural network\n",
    "- ReLU / sigmoid / 'safe' softmax activation functions\n",
    "- Inverted dropout implementation\n",
    "- Weight initialization \n",
    "- RMSprop\n",
    "- Learning rate decay\n",
    "- mini-batch gradient descent\n",
    "\n",
    "### References\n",
    "\n",
    "- I refer to the deep learning specialization course materials where applicable\n",
    "- I translated some of the numpy code from Jonathan Weisberg's \"Building a Neural Network from Scratch\" [part I](https://jonathanweisberg.org/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%201/) and [part II](https://jonathanweisberg.org/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%202/) to Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the MNIST dataset\n",
    "\n",
    "The MNIST dataset contains $60.000$ images of digits $1-10$. We use the MLDatasets module to retrieve and load the data. Normally, we would [normalize](https://www.coursera.org/learn/deep-neural-network/lecture/lXv6U/normalizing-inputs) the input data such that it has mean $0$ and standard deviation $1$ (also known as a standard normal distribution). However, this has already been done for this version of the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets\n",
    "\n",
    "# Train & test data\n",
    "train_x, train_y = MNIST.traindata();\n",
    "test_x, test_y = MNIST.testdata();\n",
    "\n",
    "# Note that the data have been normalized already\n",
    "# See: https://juliaml.github.io/MLDatasets.jl/latest/datasets/MNIST/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shapes of the data are as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 60000)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "## Get shapes\n",
    "println(size(train_x))\n",
    "println(size(train_y))\n",
    "trainsize = size(train_x)[3];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find it easier to think about the data if the examples are in the row dimension. We will reshape the data such that $X$ is a tensor of shape $(60.000, 784)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape x\n",
    "train_x = transpose(reshape(train_x, 28 * 28, size(train_x)[3]));\n",
    "test_x = transpose(reshape(test_x, 28 * 28, size(test_x)[3]));\n",
    "\n",
    "# Reshape Y\n",
    "train_y = reshape(train_y, size(train_y)[1], 1);\n",
    "test_y = reshape(test_y, size(test_y)[1], 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y$ has $10$ unique labels ranging from $0-9$. However, this is actually a recoding of the data. MORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Int64,1}:\n",
       " 5\n",
       " 0\n",
       " 4\n",
       " 1\n",
       " 9\n",
       " 2\n",
       " 3\n",
       " 6\n",
       " 7\n",
       " 8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data until now\n",
    "\n",
    "- $X$ is a tensor with dimension $(60.000, 784)$\n",
    "- $Y$ is a tensor with dimension $(60.000, 1)$\n",
    "\n",
    "<img src=\"img/XandY2.png\" width = \"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a one-hot encoding for $Y$\n",
    "\n",
    "In order to train the neural network, we need to one-hot-encode <link> the outcome variable $Y$.\n",
    "\n",
    "The trick to doing this is as follows:\n",
    "\n",
    "- Create a $k \\times k$ identity matrix. In our case, we have $k=10$, so:\n",
    "\n",
    "$$\n",
    "I = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & \\dots & 0 \\\\\n",
    "0 & 1 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & 0 \\\\\n",
    "0 & 0 & \\dots & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- For each example in $Y$, *index the column of the value of the label*. For example, if the the label equals '5', we index the $5^{th}$ column, which has a $1$ on the $5^{th}$ element and zeroes elsewhere\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Because we do this $n$ times (the number of training examples), we will end up with a tensor of size $(10, 60.000)$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & \\dots & 1 \\\\\n",
    "1 & 1 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 1 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Again, I like to think in terms of (examples, dimensions) so we will transpose the one-hot-encoded matrix such that it has the dimensions $(60.000, 10)$\n",
    "\n",
    "<img src=\"img/y_one_hot.png\" width = \"300px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for y\n",
    "# We need a shape for outcome variable of size [num_labels, num_examples]\n",
    "\n",
    "# Number of distinct classes\n",
    "k = length(unique(train_y));\n",
    "# Number of training examples\n",
    "n = size(train_y)[1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "function one_hot_encode(y_labels)\n",
    "    \n",
    "    #=\n",
    "    From multiclass labels to one-hot encoding\n",
    "    \n",
    "    :param y_labels: column vector of shape (number of examples, 1) containing k > 2 classes\n",
    "    \n",
    "    :return: one-hot encoded labels\n",
    "    =#\n",
    "    \n",
    "    # Number of distinct classes\n",
    "    k = length(unique(y_labels));\n",
    "    # Number of rows\n",
    "    n = size(y_labels)[1];\n",
    "    \n",
    "    # 1. Create k x k identity matrix \n",
    "\n",
    "    # Arrange the identity matrix such that, for each class, we get a one-hot encoded vector.\n",
    "    # This means that the rows are of length k (the number of distinct classes)\n",
    "    # The columns are of length n (the number of examples).\n",
    "    y_ohe = Matrix{Float64}(I, k, k)\n",
    "\n",
    "    # 2. Organize such that we get a k x m matrix. We do this by letting the label index\n",
    "    #     the column value. Since we have m labels, we index the columns m times.\n",
    "    #     So for m = 1.000 where m_1000 = 3, we index the kth column [0,0,1,0,...,k]\n",
    "    \n",
    "    # We have to add +1 to the classes (because Julia indexes from 1)\n",
    "    # Unlike e.g. numpy we need to explicitly call 'broadcast' to match shapes between\n",
    "    # two elements that we're adding\n",
    "    y_ohe = y_ohe[:, broadcast(+, y_labels, 1)][:,:];\n",
    "    \n",
    "    # Return\n",
    "    return(y_ohe)\n",
    "    \n",
    "    end;\n",
    "\n",
    "# One-hot encoding\n",
    "y_train_ohe = transpose(one_hot_encode(train_y));\n",
    "y_test_ohe = transpose(one_hot_encode(test_y));\n",
    "\n",
    "println(size(y_train_ohe))\n",
    "println(size(y_test_ohe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just to make sure that the one-hot encoding was done properly, we can randomly sample a number of values and check if the one-hot encoded examples line up with the original labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "\n",
    "function sanity(y_ohe, train_y; n_sample = 5)\n",
    "    \n",
    "    #=\n",
    "    Ensure that one-hot encoded labels are encoded properly by transforming them back into\n",
    "     labels.\n",
    "    \n",
    "    :param y_ohe: one-hot encoded training labels\n",
    "    :param train_y: array of size (examples, 1) containing the labels\n",
    "    :param n_sample: number of examples to sample randomly\n",
    "    \n",
    "    :return: This function does not return a value\n",
    "    =#\n",
    "    \n",
    "    # Shapes\n",
    "    n, k = size(y_ohe)\n",
    "    \n",
    "    # Pick a random example\n",
    "    ind_shuffled = shuffle(1:n)\n",
    "    \n",
    "    # Subset\n",
    "    ind = ind_shuffled[1:n_sample]\n",
    "    \n",
    "    # For each, print OHE + convert back to class\n",
    "    for i in ind\n",
    "        \n",
    "        # Find position of 1\n",
    "        pos = findall(x -> x==1, y_ohe[i, :])\n",
    "        \n",
    "        # Subtract 1\n",
    "        pos = pos[1] - 1\n",
    "        \n",
    "        # Print\n",
    "        println(\"Example \", i, \":\\n\", \n",
    "                \"\\t[OHE position: \", pos, \"] ==> [label: \", train_y[i,1], \"]\")\n",
    "        \n",
    "    end\n",
    "    \n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 50075:\n",
      "\t[OHE position: 8] ==> [label: 8]\n",
      "Example 35275:\n",
      "\t[OHE position: 5] ==> [label: 5]\n",
      "Example 39240:\n",
      "\t[OHE position: 4] ==> [label: 4]\n",
      "Example 41723:\n",
      "\t[OHE position: 4] ==> [label: 4]\n",
      "Example 16998:\n",
      "\t[OHE position: 0] ==> [label: 0]\n"
     ]
    }
   ],
   "source": [
    "sanity(y_train_ohe, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNet functions\n",
    "\n",
    "Plan: build a neural net with one hidden layer and $n_h$ hidden units\n",
    "\n",
    "TODO:\n",
    "\n",
    "- Notation\n",
    "- Matrix calculus (see https://atmos.washington.edu/~dennis/MatrixCalculus.pdf)\n",
    "- Images of the neural network structure\n",
    "- Backprop computation graph image + derivatives\n",
    "    * Also when using Tanh\n",
    "- References\n",
    "    * Chollet Keras\n",
    "    * Coursera course\n",
    "- 'safe' softmax function\n",
    "- dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "function cross_validation(X, Y, split_prop = 0.05)\n",
    "    \n",
    "    #=\n",
    "    Create train/test split\n",
    "    \n",
    "    :param X: Input data X\n",
    "    :param Y: output data Y\n",
    "    :param split_prop: percentage of data to use as validation set\n",
    "    \n",
    "    :return: dictionary with train & validation data\n",
    "    :seealso: https://www.coursera.org/learn/deep-neural-network/lecture/cxG1s/train-dev-test-sets\n",
    "    =#\n",
    "    \n",
    "    # Number of training examples\n",
    "    trainsize = size(X)[1]\n",
    "    \n",
    "    # Number of validation examples\n",
    "    ndev = Integer(floor(trainsize * split_prop))\n",
    "    \n",
    "    # Shuffle indices\n",
    "    ind = shuffle(1:trainsize);\n",
    "    # Rearrange train x and y\n",
    "    X = X[ind, :];\n",
    "    Y = Y[ind, :];\n",
    "\n",
    "    # Validation split\n",
    "    dev_x = X[1:ndev, :];\n",
    "    dev_y = Y[1:ndev, :];\n",
    "\n",
    "    # Remove from train\n",
    "    train_x = X[ndev+1:end, :];\n",
    "    train_y = Y[ndev+1:end, :];\n",
    "    \n",
    "    # To dict\n",
    "    split = Dict(\n",
    "        \"X_train\" => train_x,\n",
    "        \"Y_train\" => train_y,\n",
    "        \"X_dev\" => dev_x,\n",
    "        \"Y_dev\" => dev_y\n",
    "    )\n",
    "    \n",
    "    # Return\n",
    "    return(split)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight initialization\n",
    "\n",
    "$$\n",
    "W^{[l]} = (n^{[l]},n^{[l-1]}) \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "function initialize_parameters(n_x, n_h1, n_h2, n_y)\n",
    "    \n",
    "    #=\n",
    "    Initialize the weight matrices and bias vectors\n",
    "    \n",
    "    :param n_x: number of observations in the input data\n",
    "    :param n_h1: number of hidden units in the first layer\n",
    "    :param n_h2: number of hidden units in the second layer\n",
    "    :param n_y: number of labels in the output data\n",
    "    \n",
    "    :return:\n",
    "       - parameters: dict containing weights W and bias vectors b for each layer\n",
    "       - cache:      dict containing the adjusted weights used for RMSprop (see below)\n",
    "    \n",
    "    :seealso: \n",
    "       - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/XtFPI/random-initialization\n",
    "       - https://www.coursera.org/learn/deep-neural-network/lecture/C9iQO/vanishing-exploding-gradients\n",
    "       - https://www.coursera.org/learn/deep-neural-network/lecture/RwqYe/weight-initialization-for-deep-networks\n",
    "       - https://stats.stackexchange.com/questions/47590/what-are-good-initial-weights-in-a-neural-network\n",
    "       - https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94\n",
    "    =#\n",
    "    \n",
    "    # For sigmoid / tanh / softmax --> Xavier weight initialization --> 1 / sqrt(units in previous layer)\n",
    "    # For ReLU weight initialization --> 2 / sqrt(units in previous layer)\n",
    "    \n",
    "    W1 = randn(n_h1, n_x) .* sqrt(2 / n_x) # \n",
    "    b1 = zeros(n_h1, 1)\n",
    "    W2 = randn(n_h2, n_h1) .* sqrt(2 / n_h1) \n",
    "    b2 = zeros(n_h2, 1)\n",
    "    W3 = randn(n_y, n_h2) .* sqrt(1 / n_h2)\n",
    "    b3 = zeros(n_y, 1)\n",
    "    \n",
    "    # Exponentially weighted values initialization\n",
    "    vdW1 = zeros(n_h1, n_x)\n",
    "    vdb1 = zeros(n_h1, 1)\n",
    "    vdW2 = zeros(n_h2, n_h1)\n",
    "    vdb2 = zeros(n_h2, 1)\n",
    "    vdW3 = zeros(n_y, n_h2)\n",
    "    vdb3 = zeros(n_y, 1)\n",
    "    \n",
    "    # Create parameters dict\n",
    "    parameters = Dict(\n",
    "        \"W1\" => W1,\n",
    "        \"b1\" => b1,\n",
    "        \"W2\" => W2,\n",
    "        \"b2\" => b2,\n",
    "        \"W3\" => W3,\n",
    "        \"b3\" => b3\n",
    "    )\n",
    "    \n",
    "    # Create cache dict\n",
    "    cache = Dict(\n",
    "        \"vdW1\" => vdW1,\n",
    "        \"vdb1\" => vdb1,\n",
    "        \"vdW2\" => vdW2,\n",
    "        \"vdb2\" => vdb2,\n",
    "        \"vdW3\" => vdW3,\n",
    "        \"vdb3\" => vdb3\n",
    "    )\n",
    "    \n",
    "    # Return\n",
    "    return((parameters, cache))\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch gradient descent\n",
    "\n",
    "An algorithm for splitting the data into batches is given below. Here:\n",
    "\n",
    "- $n$ is the number of training examples\n",
    "- $k$ is the batch size\n",
    "- $j$ is the number of total batches that can be created **using the batch size $k$**\n",
    "\n",
    "That is, $j$ is the integer part of the division of $\\frac{n}{k}$.\n",
    "\n",
    "<img src=\"img/minibatch.png\" width = \"500px\">\n",
    "\n",
    "This toy example is given as an algorithm below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch 1\n",
      "\t[Begin 1] ==> [end 10]\n",
      "Minibatch 2\n",
      "\t[Begin 11] ==> [end 20]\n",
      "Minibatch 3\n",
      "\t[Begin 21] ==> [end 30]\n",
      "Minibatch 4\n",
      "\t[Begin 31] ==> [end 40]\n",
      "Minibatch 5\n",
      "\t[Begin 41] ==> [end 50]\n",
      "Minibatch 6\n",
      "\t[Begin 51] ==> [end 60]\n",
      "Minibatch 7\n",
      "\t[Begin 61] ==> [end 70]\n",
      "Minibatch 8\n",
      "\t[Begin 71] ==> [end 80]\n",
      "Minibatch 9\n",
      "\t[Begin 81] ==> [end 90]\n",
      "Minibatch 10\n",
      "\t[Begin 91] ==> [end 100]\n",
      "Minibatch 11\n",
      "\t[Begin 101] ==> [end 101]\n"
     ]
    }
   ],
   "source": [
    "# Number of examples\n",
    "n = 101;\n",
    "# Batch size\n",
    "k = 10;\n",
    "# Number of batches of size k (discounting any remainders)\n",
    "j = floor(n/k);\n",
    "\n",
    "# For each minibatch\n",
    "for i = 1:j\n",
    "    \n",
    "    # Start at 1 if minibatch is 1\n",
    "    if i == 1\n",
    "        index_begin = 1\n",
    "    else\n",
    "        # Else start at the end of the previous minibatch plus one\n",
    "        index_begin = Integer(((i - 1) * k) + 1)\n",
    "        end;\n",
    "    \n",
    "    # End \n",
    "    index_end = Integer((i * k))\n",
    "    \n",
    "    println(\"Minibatch \", Integer(i), \"\\n\\t[Begin \", index_begin, \"] ==> [end \", index_end, \"]\")\n",
    "    \n",
    "    end;\n",
    "\n",
    "# If there is a remainder\n",
    "if n - (j * k) > 0\n",
    "    \n",
    "    index_begin = Integer((j * k) + 1)\n",
    "    index_end = n\n",
    "    \n",
    "    println(\"Minibatch \", Integer(j+1), \"\\n\\t[Begin \", index_begin, \"] ==> [end \", index_end, \"]\")\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below implements mini-batches for real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_batches(X, Y, batch_size = 128)\n",
    "    \n",
    "    #=\n",
    "    Creates b batches of size batch_size\n",
    "    \n",
    "    :param X:          input data X\n",
    "    :param Y:          output data Y\n",
    "    :param batch_size: size of each batch\n",
    "    \n",
    "    :return: list containing m batches of length batch_size and possibly 1 batch of size batch_size_remainder < batch_size\n",
    "    :seealso: \n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/qcogH/mini-batch-gradient-descent\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/lBXu8/understanding-mini-batch-gradient-descent\n",
    "    =#\n",
    "    \n",
    "    # Shuffle training examples randomly\n",
    "    n = size(X)[1]\n",
    "    ind = shuffle(1:n)\n",
    "    \n",
    "    # Rearrange data in X and Y\n",
    "    X = X[ind, :]\n",
    "    Y = Y[ind, :]\n",
    "    \n",
    "    # List to store minibatches\n",
    "    mbatches = []\n",
    "    \n",
    "    # Number of complete training examples. \n",
    "    #  This means: n / 128 leaves a remainder (most likely)\n",
    "    #  Therefore, there will be (b - 1) batches with size batch_size\n",
    "    #  and 1 batch with size last_batch_size < batch_size\n",
    "    b_first = Integer(floor(n / batch_size))\n",
    "    \n",
    "    # First, loop through the (b_first - 1) examples\n",
    "    #  We need to loop through b_first - 1 because we construct \n",
    "    #    @ index_begin => i * batch_size (e.g. 9 * 128 = 1152)\n",
    "    #    @ index_end => (i + 1) * batch_size (e.g. 10 * 128 = 1280)\n",
    "    #  index_end needs (i + 1). If i == k where k is the last possible index, we cannot subset index_end\n",
    "    #  because it would require (k + 1) indices.\n",
    "    for i = 1:b_first\n",
    "        \n",
    "        # Beginning and end indices\n",
    "        if i == 1\n",
    "            index_begin = 1\n",
    "        else\n",
    "            index_begin = Integer(((i - 1) * batch_size) + 1)\n",
    "            end;\n",
    "        \n",
    "        index_end = Integer(i * (batch_size))\n",
    "        \n",
    "        X_current_batch = X[index_begin:index_end, :]\n",
    "        Y_current_batch = Y[index_begin:index_end, :]\n",
    "\n",
    "        # Add to array of minibatches\n",
    "        push!(mbatches, [X_current_batch, Y_current_batch])\n",
    "        \n",
    "        end;\n",
    "    \n",
    "    # Then, if necessary, make a batch for the remainder\n",
    "    b_rem = n - (b_first * batch_size)\n",
    "    if b_rem != 0\n",
    "        \n",
    "        # Subset X & Y\n",
    "        index_begin = Integer(((b_first) * batch_size) + 1) # i+1 is from the for-loop above (i.e. the last 'full' minibatch of size batch_size)\n",
    "        index_end = n \n",
    "        \n",
    "        X_current_batch = X[index_begin:index_end, :]\n",
    "        Y_current_batch = Y[index_begin:index_end, :]\n",
    "        \n",
    "        # Append\n",
    "        push!(mbatches, [X_current_batch, Y_current_batch])\n",
    "        \n",
    "        end;\n",
    "    \n",
    "    # Return mini batches\n",
    "    return(mbatches)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "function softmax(z)\n",
    "    \n",
    "    #=\n",
    "    Softmax function\n",
    "    \n",
    "    :param z: result of the linear transformation of the final layer\n",
    "    :return: activations for z\n",
    "    \n",
    "    :seealso:\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/4dDC1/activation-functions\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/OASKH/why-do-you-need-non-linear-activation-functions\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/HRy7y/softmax-regression\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/LCsCH/training-a-softmax-classifier\n",
    "    =#\n",
    "    \n",
    "    # Subtract the max of z to avoid exp(z) getting too large\n",
    "    z = z .- maximum(z)\n",
    "    \n",
    "    # Softmax & return\n",
    "    return(exp.(z) ./ sum(exp.(z),dims=1))\n",
    "    \n",
    "    end;\n",
    "\n",
    "function sigmoid(z)\n",
    "    \n",
    "    #=\n",
    "    Sigmoid function\n",
    "    \n",
    "    :param z: result of the linear transformation for layer l\n",
    "    :return: activations for z\n",
    "    \n",
    "    :seealso:\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/4dDC1/activation-functions\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/OASKH/why-do-you-need-non-linear-activation-functions\n",
    "    =#\n",
    "    \n",
    "    1 ./ (1 .+ exp.(-z))\n",
    "    \n",
    "    end;\n",
    "\n",
    "function ReLU(z)\n",
    "    \n",
    "    #=\n",
    "    ReLU implementation\n",
    "    \n",
    "    :param z: result of the linear transformation for layer l\n",
    "    :return: activations for z\n",
    "    \n",
    "    :seealso:\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/4dDC1/activation-functions\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/OASKH/why-do-you-need-non-linear-activation-functions\n",
    "    =#\n",
    "    \n",
    "    ((z .> 0) * 1) .* z\n",
    "    \n",
    "    end;\n",
    "\n",
    "function crossentropy_cost(y, yhat)\n",
    "    \n",
    "    #=\n",
    "    Crossentropy cost function for m classes\n",
    "    \n",
    "    :param y: actual (one-hot encoded) values for y\n",
    "    :param yhat: predicted (one-hot encoded) values for yhat\n",
    "    \n",
    "    :return: loss value\n",
    "    \n",
    "    :seealso: https://www.coursera.org/learn/neural-networks-deep-learning/lecture/yWaRd/logistic-regression-cost-function\n",
    "    =#\n",
    "    \n",
    "    loss = sum(transpose(y) .* log.(yhat))\n",
    "    n = size(y)[1]\n",
    "    \n",
    "    return(-(1/n) * loss)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized dropout\n",
    "function dropout(X, drop_chance = 0.8)\n",
    "    \n",
    "    #=\n",
    "    Dropout implementation. Randomly sets the weights of some hidden units to 0\n",
    "    \n",
    "    :param X: input data\n",
    "    :param drop_chance: probability of keeping a hidden unit\n",
    "    \n",
    "    :return: dropout matrix containing 1 for each unit that should be kept and 0 for each unit that should be \n",
    "              dropped. also returns X in which some units are dropped and the remaining units are scaled by \n",
    "              drop_chance.\n",
    "    \n",
    "    :seealso:\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/eM33A/dropout-regularization\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/YaGbR/understanding-dropout\n",
    "    =#\n",
    "    \n",
    "    n = size(X)[1]\n",
    "    k = size(X)[2]\n",
    "    \n",
    "    # Uniformly distributed probabilities\n",
    "    a = rand(n, k)\n",
    "    \n",
    "    # Evaluate against dropout probability\n",
    "    #  run the mask and turn boolean into integer\n",
    "    D = (a .< drop_chance) * 1\n",
    "    \n",
    "    # Element-wise multiplication\n",
    "    X = D .* X\n",
    "\n",
    "    # Scale\n",
    "    X = X ./ drop_chance\n",
    "    \n",
    "    # Return\n",
    "    return((D, X))\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "\n",
    "<img src=\"img/dims.png\" width = \"300px\">\n",
    "\n",
    "<img src=\"img/forwardprop.png\" width = \"800px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "function forward_prop(parameters, cache, X, drop_chance = 0.2)\n",
    "    \n",
    "    #=\n",
    "    Forward propagation\n",
    "    \n",
    "    :param parameters:  dict containing weights and bias vectors for each layer\n",
    "    :param cache:       dict containing computations, dropout vectors and exponentially weighted gradients\n",
    "    :param X:           input data\n",
    "    :param drop_chance: probability of keeping a hidden unit\n",
    "    \n",
    "    :return: updated cache with dropout vectors and the computations for each forward pass for the current \n",
    "              minibatch\n",
    "    \n",
    "    :seealso:\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/4WdOY/computation-graph\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/tyAGh/computing-a-neural-networks-output\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/MijzH/forward-propagation-in-a-deep-network\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/Rz47X/getting-your-matrix-dimensions-right\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/znwiG/forward-and-backward-propagation\n",
    "    =#\n",
    "    \n",
    "    # Retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    ## First layer\n",
    "    \n",
    "    # Forward propagation\n",
    "    Z1 = broadcast(+, W1 * transpose(X), b1, 1)\n",
    "    A1 = ReLU(Z1) \n",
    "    \n",
    "    # Apply dropout\n",
    "    D1, A1 = dropout(A1, drop_chance)\n",
    "    \n",
    "    ## Second layer\n",
    "    \n",
    "    # Forward\n",
    "    Z2 = broadcast(+, W2 * A1, b2, 1)\n",
    "    A2 = ReLU(Z2) \n",
    "    \n",
    "    # Apply dropout\n",
    "    D2, A2 = dropout(A2, drop_chance)\n",
    "    \n",
    "    ## Output layer\n",
    "    \n",
    "    # Forward\n",
    "    Z3 = broadcast(+, W3 * A2, b3, 1)\n",
    "    A3 = softmax(Z3)\n",
    "    \n",
    "    # Return values \n",
    "    cache[\"Z1\"] = Z1\n",
    "    cache[\"A1\"] = A1\n",
    "    cache[\"Z2\"] = Z2\n",
    "    cache[\"A2\"] = A2\n",
    "    cache[\"Z3\"] = Z3\n",
    "    cache[\"A3\"] = A3\n",
    "    cache[\"D1\"] = D1\n",
    "    cache[\"D2\"] = D2\n",
    "    \n",
    "    # Return\n",
    "    return(cache)   \n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivatives of activation functions\n",
    "function dSigmoid(z)\n",
    "    \n",
    "    #=\n",
    "    Derivative of the sigmoid function\n",
    "    \n",
    "    :param z: result of the linear transformation computed during forward propagation. Generally:\n",
    "                   Z = W_lA_{l-1} + b_l\n",
    "    :return: derivative of the activation function\n",
    "    \n",
    "    :seealso: \n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/qcG1j/derivatives-of-activation-functions\n",
    "      - https://deepnotes.io/softmax-crossentropy\n",
    "      - https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "    =#\n",
    "    \n",
    "    sigmoid(z) .* (1 .- sigmoid(z))\n",
    "    \n",
    "    end;\n",
    "\n",
    "function dReLU(z)\n",
    "    \n",
    "    #=\n",
    "    Derivative of ReLU function\n",
    "    \n",
    "    :param z: result of the linear transformation computed during forward propagation. Generally:\n",
    "                   Z = W_lA_{l-1} + b_l\n",
    "    :return: derivative of the activation function\n",
    "    \n",
    "    :seealso: https://www.coursera.org/learn/neural-networks-deep-learning/lecture/qcG1j/derivatives-of-activation-functions\n",
    "    =#\n",
    "    \n",
    "    (z .> 0) * 1\n",
    "    \n",
    "    end;\n",
    "\n",
    "function dTanh(z)\n",
    "    \n",
    "    #=\n",
    "    Derivative of tanh function\n",
    "    \n",
    "    :param z: result of the linear transformation computed during forward propagation. Generally:\n",
    "                   Z = W_lA_{l-1} + b_l\n",
    "    :return: derivative of the activation function\n",
    "    \n",
    "    :seealso: https://www.coursera.org/learn/neural-networks-deep-learning/lecture/qcG1j/derivatives-of-activation-functions\n",
    "    =# \n",
    "    \n",
    "    1 .- (tanh.(z) .* tanh.(z))\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "function backward_prop(cache, parameters, X, Y, drop_chance)\n",
    "    \n",
    "    #=\n",
    "    Backward propagation\n",
    "    \n",
    "    :param parameters:  dict containing weights and bias vectors for each layer\n",
    "    :param cache:       dict containing computations, dropout vectors and exponentially weighted gradients\n",
    "    :param X:           input data\n",
    "    :param drop_chance: probability of keeping a hidden unit\n",
    "    \n",
    "    :return: updated cache with new gradients for the current minibatch\n",
    "    \n",
    "    :seealso:\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/0ULGt/derivatives\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/oEcPT/more-derivative-examples\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/4WdOY/computation-graph\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/0VSHe/derivatives-with-a-computation-graph\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/6dDj7/backpropagation-intuition-optional\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/znwiG/forward-and-backward-propagation\n",
    "      Docs about dims argument for sum()\n",
    "      - https://docs.julialang.org/en/v0.6.1/stdlib/collections/#Base.sum\n",
    "    =#\n",
    "    \n",
    "    # Dims\n",
    "    n = size(Y)[1]\n",
    "    \n",
    "    # Unroll cache\n",
    "    A3 = cache[\"A3\"]\n",
    "    Z3 = cache[\"Z3\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    Z2 = cache[\"Z2\"]\n",
    "    A1 = cache[\"A1\"]\n",
    "    Z1 = cache[\"Z1\"]\n",
    "    D1 = cache[\"D1\"]\n",
    "    D2 = cache[\"D2\"]\n",
    "    \n",
    "    # Retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    ## Output layer\n",
    "    dZ3 = A3 .- transpose(Y)                # Softmax derivative ==> d/dZ softmax(Z3)\n",
    "    dW3 = (1/n) .* (dZ3 * transpose(A2))    # W3 derivative      ==> d/W3 W3A3 + b3\n",
    "    db3 = (1/n) .* sum(dZ3, dims = 2)       # b3 derivative      ==> d/b3 W3A3 + b3\n",
    "    \n",
    "    ## Second layer\n",
    "    dA2 = transpose(W3) * dZ3               # Derivative of the activation of 2nd layer (A2 = ReLU(W2A2 + b2))\n",
    "                                            \n",
    "    # Apply dropout\n",
    "    dA2 = D2 .* dA2                         # Apply dropout to the derivatives\n",
    "    dA2 = dA2 ./ drop_chance                # scale the activations\n",
    "    \n",
    "    # Derivatives of weights\n",
    "    dZ2 = dA2 .* dReLU(Z2)                  # etc . . . \n",
    "    dW2 = (1/n) .* (dZ2 * transpose(A1))\n",
    "    db2 = (1/n) .* sum(dZ2, dims=2)\n",
    "    \n",
    "    ## First layer\n",
    "    dA1 = transpose(W2) * dZ2\n",
    "    \n",
    "    # Apply dropout\n",
    "    dA1 = D1 .* dA1\n",
    "    dA1 = dA1 ./ drop_chance\n",
    "    \n",
    "    # Derivatives of weights\n",
    "    dZ1 = dA1 .* dReLU(Z1)\n",
    "    dW1 = (1/n) .* (dZ1 * X)\n",
    "    db1 = (1/n) .* sum(dZ1, dims=2)\n",
    "    \n",
    "    # Add to cache and return\n",
    "    cache[\"dW1\"] = dW1\n",
    "    cache[\"db1\"] = db1\n",
    "    cache[\"dW2\"] = dW2\n",
    "    cache[\"db2\"] = db2\n",
    "    cache[\"dW3\"] = dW3\n",
    "    cache[\"db3\"] = db3\n",
    "    \n",
    "    # Return cache\n",
    "    return(cache)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating weights: RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "function RMSprop(parameters, cache, alpha, beta = 0.9, epsilon = 1e-8)\n",
    "    \n",
    "    #=\n",
    "    Update parameters and implement exponentially weighted averages\n",
    "    \n",
    "    :param parameters:  dict containing weights and bias vectors for each layer\n",
    "    :param cache:       dict containing computations, dropout vectors and exponentially weighted gradients\n",
    "    :param alpha:       learning rate\n",
    "    :param beta:        exponential weighting value. Usually set at 0.9 <MORE>\n",
    "    :param epsilon:     small value to prevent divide by zero\n",
    "    \n",
    "    :return: updates weights in cache for current minibatch\n",
    "    \n",
    "    :seealso:\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/A0tBd/gradient-descent\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/udiAq/gradient-descent-on-m-examples\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/Wh8NI/gradient-descent-for-neural-networks\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/duStO/exponentially-weighted-averages\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/Ud7t0/understanding-exponentially-weighted-averages\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/XjuhD/bias-correction-in-exponentially-weighted-averages\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/y0m1f/gradient-descent-with-momentum\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/BhJlm/rmsprop\n",
    "    =#\n",
    "    \n",
    "    # Unroll cache\n",
    "    dW1 = cache[\"dW1\"]\n",
    "    db1 = cache[\"db1\"]\n",
    "    dW2 = cache[\"dW2\"]\n",
    "    db2 = cache[\"db2\"]\n",
    "    dW3 = cache[\"dW3\"]\n",
    "    db3 = cache[\"db3\"]\n",
    "    \n",
    "    vdW1 = cache[\"vdW1\"]\n",
    "    vdb1 = cache[\"vdb1\"]\n",
    "    vdW2 = cache[\"vdW2\"]\n",
    "    vdb2 = cache[\"vdb2\"]\n",
    "    vdW3 = cache[\"vdW3\"]\n",
    "    vdb3 = cache[\"vdb3\"]\n",
    "    \n",
    "    # Retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # Create new vdW and vdb\n",
    "    #  Notice that we square the gradients element-wise\n",
    "    vdW1 = beta .* vdW1 .+ (1 - beta) .* dW1.^2\n",
    "    vdb1 = beta .* vdb1 .+ (1 - beta) .* db1.^2\n",
    "    vdW2 = beta .* vdW2 .+ (1 - beta) .* dW2.^2\n",
    "    vdb2 = beta .* vdb2 .+ (1 - beta) .* db2.^2\n",
    "    vdW3 = beta .* vdW3 .+ (1 - beta) .* dW3.^2\n",
    "    vdb3 = beta .* vdb3 .+ (1 - beta) .* db3.^2\n",
    "    \n",
    "    # Update parameters\n",
    "    W1 = W1 .- (alpha .* (dW1 ./ (sqrt.(vdW1) .+ epsilon)))\n",
    "    b1 = b1 .- (alpha .* (db1 ./ (sqrt.(vdb1) .+ epsilon)))\n",
    "    W2 = W2 .- (alpha .* (dW2 ./ (sqrt.(vdW2) .+ epsilon)))\n",
    "    b2 = b2 .- (alpha .* (db2 ./ (sqrt.(vdb2) .+ epsilon)))\n",
    "    W3 = W3 .- (alpha .* (dW3 ./ (sqrt.(vdW3) .+ epsilon)))\n",
    "    b3 = b3 .- (alpha .* (db3 ./ (sqrt.(vdb3) .+ epsilon)))\n",
    "    \n",
    "    # Store parameters\n",
    "    parameters[\"W1\"] = W1\n",
    "    parameters[\"b1\"] = b1\n",
    "    parameters[\"W2\"] = W2\n",
    "    parameters[\"b2\"] = b2\n",
    "    parameters[\"W3\"] = W3\n",
    "    parameters[\"b3\"] = b3\n",
    "    \n",
    "    # Store exponentially weighted gradients\n",
    "    cache[\"vdW1\"] = vdW1\n",
    "    cache[\"vdb1\"] = vdb1\n",
    "    cache[\"vdW2\"] = vdW2\n",
    "    cache[\"vdb2\"] = vdb2\n",
    "    cache[\"vdW3\"] = vdW3\n",
    "    cache[\"vdb3\"] = vdb3\n",
    "    \n",
    "    # Return parameters and cache\n",
    "    return((parameters, cache))\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "function learning_rate_decay(alpha, epoch, decay = 1)\n",
    "    \n",
    "    #= \n",
    "    Decays the value of alpha (learning rate) as number of epochs increases\n",
    "    \n",
    "    :param alpha: learning rate \n",
    "    :param epoch: current epoch number\n",
    "    :param decay: decay value\n",
    "    \n",
    "    :return: updated learning rate\n",
    "    \n",
    "    :seealso: https://www.coursera.org/learn/deep-neural-network/lecture/hjgIA/learning-rate-decay\n",
    "    =#\n",
    "    \n",
    "    alpha_new = 1 / (1 + decay * epoch) * alpha\n",
    "    \n",
    "    # Return\n",
    "    return(alpha_new)\n",
    "    \n",
    "    end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(X, parameters)\n",
    "    \n",
    "    #=\n",
    "    Predict Y given X and the parameters\n",
    "    \n",
    "    :param X:          input data\n",
    "    :param parameters: parameters for each layer\n",
    "    \n",
    "    :return: predicted values given X and parameters\n",
    "    =#\n",
    "    \n",
    "    # Unroll parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    \n",
    "    # Forward prop\n",
    "    Z1 = broadcast(+, W1 * transpose(X), b1, 1)\n",
    "    A1 = ReLU(Z1) \n",
    "    # Second layer\n",
    "    Z2 = broadcast(+, W2 * A1, b2, 1)\n",
    "    A2 = ReLU(Z2) \n",
    "    # Output layer\n",
    "    Z3 = broadcast(+, W3 * A2, b3, 1)\n",
    "    A3 = softmax(Z3)\n",
    "    \n",
    "    # Return\n",
    "    return(A3)\n",
    "    \n",
    "    end;\n",
    "\n",
    "function accuracy(Y, Yhat)\n",
    "    \n",
    "    #=\n",
    "    Calculates accuracy\n",
    "    \n",
    "    :param Y:    actual outcome values (as one-hot encoded) \n",
    "    :param Yhat: predicted outcome values (as one-hot encoded)\n",
    "    \n",
    "    :return: accuracy of the predictions as a float between 0 and 1\n",
    "    =# \n",
    "    \n",
    "    # Predictions --> labels\n",
    "    vals, inds = findmax(Yhat, dims = 1);\n",
    "    Yhat_labels = map(x -> x[1] - 1, inds)\n",
    "\n",
    "    # Y ohe --> labels\n",
    "    vals, inds = findmax(transpose(Y), dims=1)\n",
    "    Y_labels = map(x -> x[1] - 1, inds)\n",
    "    \n",
    "    # Accuracy\n",
    "    return(sum(Y_labels .== Yhat_labels) / length(Y_labels))\n",
    "    \n",
    "    end;\n",
    "\n",
    "function evaluate_model(X, Y, parameters; return_accuracy = false)\n",
    "    \n",
    "    #= \n",
    "    Evaluate the model on the development & train sets\n",
    "    \n",
    "    :param X:               input data \n",
    "    :param Y:               output data\n",
    "    :param parameters:      dict containing parameters for each layer\n",
    "    :param return_accuracy: boolean. If true, returns loss and accuracy. If false, only returns loss\n",
    "    \n",
    "    :return: either loss and accuracy or only loss value\n",
    "    =#\n",
    "    \n",
    "    # Predict\n",
    "    Yhat = predict(X, parameters)\n",
    "    \n",
    "    # Loss\n",
    "    loss = crossentropy_cost(Y, Yhat)\n",
    "    \n",
    "    # Accuracy\n",
    "    if return_accuracy\n",
    "        acc = accuracy(Y, Yhat)\n",
    "        return((loss, acc))\n",
    "        \n",
    "        end;\n",
    "    \n",
    "    # Return\n",
    "    return(loss)\n",
    "    \n",
    "    end;\n",
    "\n",
    "using Plots\n",
    "\n",
    "function plot_history(history)\n",
    "    \n",
    "    #=\n",
    "    Plot loss across epochs\n",
    "    \n",
    "    :param history: nn history returned by nnet function\n",
    "    \n",
    "    :return: this function plots the history but does not return a value \n",
    "    \n",
    "    :seealso: https://docs.juliaplots.org/latest/tutorial/#plot-recipes-and-recipe-libraries\n",
    "    =#\n",
    "    \n",
    "    # Retrieve data\n",
    "    epoch = [x[1] for x in history]\n",
    "    train_loss = [x[2] for x in history]\n",
    "    dev_loss = [x[3] for x in history]\n",
    "    # Train and dev loss in one array (2 columns)\n",
    "    loss = hcat(train_loss, dev_loss);\n",
    "    \n",
    "    # Plot epochs versus loss\n",
    "    plot(epoch, loss, seriestype=:line, xlabel = \"epoch\", ylabel = \"cost\", lab = [\"Train\" \"Dev\"],\n",
    "         lw = 2)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling it all together: the nnet() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "function nnet(X, Y; n_h1 = 16, n_h2 = 8, validation_split = 0.1, alpha = 0.05, beta=0.9,\n",
    "              epsilon=10e-8, epochs = 10, batch_size=128, lr_decay=1, dropout_prop=0.3)\n",
    "    \n",
    "    #= \n",
    "    Implements a two-layer neural network\n",
    "    \n",
    "    :param X:                   input data\n",
    "    :param Y:                   actual outcome values (as one-hot encoded) \n",
    "    :param n_h1:                hidden units for layer 1\n",
    "    :param n_h2:                hidden units for layer 2\n",
    "    :param validation_split:    decimal indicating what percentage of the data should be reserved for \n",
    "                                 validation split\n",
    "    :param alpha:               learning rate \n",
    "    :param beta:                exponential weighting value. Usually set at 0.9 <MORE>\n",
    "    :param epsilon:             small value to prevent divide by zero in RMSprop \n",
    "    :param epochs:              number of passes (epochs) to train the network\n",
    "    :param batch_size:          size of the minibatches\n",
    "    :param lr_decay:            decay value for learning rate (alpha)\n",
    "    :param dropout_prop:        probability of keeping a hidden unit when performing dropout regularization. \n",
    "                                 A value of 1 keeps all hidden units\n",
    "    \n",
    "    :return: final parameters for the trained model and training history\n",
    "    =#\n",
    "    \n",
    "    # Set dimensions\n",
    "    n = size(X)[1]\n",
    "    n_x = size(X)[2]\n",
    "    n_y = size(Y)[2]\n",
    "\n",
    "    # Save alpha value\n",
    "    alpha_initial = alpha\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters, cache = initialize_parameters(n_x, n_h1, n_h2, n_y)\n",
    "    \n",
    "    # Open list for history\n",
    "    history = []\n",
    "\n",
    "    # Loop through epochs\n",
    "    for i = 1:epochs\n",
    "\n",
    "        # Validation and train set\n",
    "        data = cross_validation(X, Y, validation_split)\n",
    "\n",
    "        # Unroll\n",
    "        X_train = data[\"X_train\"]\n",
    "        Y_train = data[\"Y_train\"]\n",
    "        X_dev = data[\"X_dev\"]\n",
    "        Y_dev = data[\"Y_dev\"]\n",
    "\n",
    "        # Set up number of batches\n",
    "        batches = create_batches(X_train, Y_train, batch_size)\n",
    "        \n",
    "        ## Mini-batches (inner loop)\n",
    "        for batch in batches\n",
    "\n",
    "            # Unroll X and y\n",
    "            X = batch[1]\n",
    "            Y = batch[2]\n",
    "\n",
    "            # Forward prop\n",
    "            cache = forward_prop(parameters, cache, X, dropout_prop)\n",
    "\n",
    "            # Backward propagation\n",
    "            cache = backward_prop(cache, parameters, X, Y, dropout_prop)\n",
    "\n",
    "            # Update params\n",
    "            parameters, cache = RMSprop(parameters, cache, alpha, beta, epsilon)\n",
    "\n",
    "            end;\n",
    "\n",
    "        # Learning rate decay\n",
    "        alpha = learning_rate_decay(alpha_initial, i, lr_decay)\n",
    "\n",
    "        # Evaluate\n",
    "        eval_train = evaluate_model(X_train, Y_train, parameters)\n",
    "        eval_dev, eval_acc = evaluate_model(X_dev, Y_dev, parameters, return_accuracy=true)\n",
    "        \n",
    "        # Round\n",
    "        eval_train = round(eval_train, digits=3)\n",
    "        eval_dev = round(eval_dev, digits=3)\n",
    "\n",
    "        # Print\n",
    "        println(\"Epoch \", i, \":\\n\",\n",
    "                \"\\t[train cost: \", eval_train, \n",
    "                \"] ==> [validation cost: \", eval_dev, \"]\",\n",
    "                \"\\n\\tvalidation accuracy: \", round(eval_acc, digits=3))\n",
    "        \n",
    "        # Add to history\n",
    "        push!(history, [i, eval_train, eval_dev])\n",
    "\n",
    "        end;\n",
    "    \n",
    "    # Return the final parameters\n",
    "    return((parameters, history))\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split test into dev (70%) test (30%)\n",
    "Random.seed!(9726)\n",
    "split = cross_validation(test_x, y_test_ohe, 0.3)\n",
    "X_dev = split[\"X_train\"]\n",
    "Y_dev = split[\"Y_train\"]\n",
    "X_test = split[\"X_dev\"]\n",
    "Y_test = split[\"Y_dev\"];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\t[train cost: 0.099] ==> [validation cost: 0.125]\n",
      "\tvalidation accuracy: 0.964\n",
      "Epoch 2:\n",
      "\t[train cost: 0.018] ==> [validation cost: 0.0]\n",
      "\tvalidation accuracy: 1.0\n",
      "Epoch 3:\n",
      "\t[train cost: 0.002] ==> [validation cost: 0.002]\n",
      "\tvalidation accuracy: 1.0\n",
      "Epoch 4:\n",
      "\t[train cost: 0.001] ==> [validation cost: 0.003]\n",
      "\tvalidation accuracy: 1.0\n",
      "Epoch 5:\n",
      "\t[train cost: 0.001] ==> [validation cost: 0.0]\n",
      "\tvalidation accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# For clarity\n",
    "X = train_x\n",
    "Y = y_train_ohe\n",
    "\n",
    "# Run the neural net\n",
    "parameters, history = nnet(X, Y, n_h1 = 512, n_h2 = 512, epochs=5, beta=0.9, alpha=0.0026,\n",
    "                          lr_decay=0, dropout_prop=0.9, validation_split = 0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip5100\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2000\" height=\"2000\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<defs>\n",
       "  <clipPath id=\"clip5101\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip5101)\" points=\"\n",
       "0,1600 2400,1600 2400,0 0,0 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip5102\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip5101)\" points=\"\n",
       "251.149,1440.48 2321.26,1440.48 2321.26,47.2441 251.149,47.2441 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip5103\">\n",
       "    <rect x=\"251\" y=\"47\" width=\"2071\" height=\"1394\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip5103)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  309.737,1440.48 309.737,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5103)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  797.97,1440.48 797.97,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5103)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1286.2,1440.48 1286.2,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5103)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1774.44,1440.48 1774.44,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5103)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2262.67,1440.48 2262.67,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5103)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  251.149,1401.05 2321.26,1401.05 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5103)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  251.149,1190.75 2321.26,1190.75 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5103)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  251.149,980.451 2321.26,980.451 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5103)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  251.149,770.151 2321.26,770.151 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5103)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  251.149,559.851 2321.26,559.851 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5103)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  251.149,349.551 2321.26,349.551 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5103)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  251.149,139.25 2321.26,139.25 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5101)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,1440.48 2321.26,1440.48 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5101)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,1440.48 251.149,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5101)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  309.737,1440.48 309.737,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5101)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  797.97,1440.48 797.97,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5101)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1286.2,1440.48 1286.2,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5101)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1774.44,1440.48 1774.44,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5101)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2262.67,1440.48 2262.67,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5101)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,1401.05 282.2,1401.05 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5101)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,1190.75 282.2,1190.75 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5101)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,980.451 282.2,980.451 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5101)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,770.151 282.2,770.151 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5101)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,559.851 282.2,559.851 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5101)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,349.551 282.2,349.551 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5101)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,139.25 282.2,139.25 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip5101)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 309.737, 1494.48)\" x=\"309.737\" y=\"1494.48\">1</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5101)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 797.97, 1494.48)\" x=\"797.97\" y=\"1494.48\">2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5101)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1286.2, 1494.48)\" x=\"1286.2\" y=\"1494.48\">3</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5101)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1774.44, 1494.48)\" x=\"1774.44\" y=\"1494.48\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5101)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2262.67, 1494.48)\" x=\"2262.67\" y=\"1494.48\">5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5101)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 227.149, 1418.55)\" x=\"227.149\" y=\"1418.55\">0.00</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5101)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 227.149, 1208.25)\" x=\"227.149\" y=\"1208.25\">0.02</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5101)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 227.149, 997.951)\" x=\"227.149\" y=\"997.951\">0.04</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5101)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 227.149, 787.651)\" x=\"227.149\" y=\"787.651\">0.06</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5101)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 227.149, 577.351)\" x=\"227.149\" y=\"577.351\">0.08</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5101)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 227.149, 367.051)\" x=\"227.149\" y=\"367.051\">0.10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5101)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 227.149, 156.75)\" x=\"227.149\" y=\"156.75\">0.12</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5101)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1286.2, 1590.4)\" x=\"1286.2\" y=\"1590.4\">epoch</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5101)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 57.6, 743.863)\" x=\"57.6\" y=\"743.863\">cost</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip5103)\" style=\"stroke:#009af9; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  309.737,360.066 797.97,1211.78 1286.2,1380.02 1774.44,1390.54 2262.67,1390.54 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5103)\" style=\"stroke:#e26f46; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  309.737,86.6754 797.97,1401.05 1286.2,1380.02 1774.44,1369.51 2262.67,1401.05 \n",
       "  \"/>\n",
       "<polygon clip-path=\"url(#clip5101)\" points=\"\n",
       "1899.61,312.204 2249.26,312.204 2249.26,130.764 1899.61,130.764 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip5101)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1899.61,312.204 2249.26,312.204 2249.26,130.764 1899.61,130.764 1899.61,312.204 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5101)\" style=\"stroke:#009af9; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  1923.61,191.244 2067.61,191.244 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip5101)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2091.61, 208.744)\" x=\"2091.61\" y=\"208.744\">Train</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip5101)\" style=\"stroke:#e26f46; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  1923.61,251.724 2067.61,251.724 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip5101)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2091.61, 269.224)\" x=\"2091.61\" y=\"269.224\">Dev</text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 831,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot history\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9717"
      ]
     },
     "execution_count": 832,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train accuracy\n",
    "Yhat = predict(X, parameters);\n",
    "acc = accuracy(Y, Yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9644285714285714"
      ]
     },
     "execution_count": 833,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dev accuracy\n",
    "Yhat_dev = predict(X_dev, parameters);\n",
    "# Accuracy\n",
    "acc_dev = accuracy(Y_dev, Yhat_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9693333333333334"
      ]
     },
     "execution_count": 834,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test accuracy\n",
    "Yhat_tst = predict(X_test, parameters);\n",
    "# Accuracy\n",
    "acc_tst = accuracy(Y_test, Yhat_tst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
