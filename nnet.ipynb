{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNET\n",
    "\n",
    "This is my Jupyter notebook with notes / implementation of a shallow neural network in Julia. All materials are based on deeplearning.ai's Deep Learning Specialization. I refer to specific videos and other materials where applicable. **The notebook is not yet finished**\n",
    "\n",
    "I created this notebook to review some of the concepts I learned in the first three courses of the deep learning specialization. Its purpose is primarily for me to process the information and produce the implementation from my own understanding rather than simply copying it from another place. As a result, you may find that the implementation notes are somewhat long-winded or tangential.\n",
    "\n",
    "If you find a mistake or think this code can be improved, it would be great if you could fork the repository and make a pull request. You could also leave a comment in the issues section.\n",
    "\n",
    "### This notebook contains\n",
    "\n",
    "- Loading & pre-processing the MNIST dataset\n",
    "- forward and backward propagation using a 1-layer neural network\n",
    "- Inverted dropout implementation\n",
    "- Xavier initialization \n",
    "- RMSprop\n",
    "- Learning rate decay\n",
    "- mini-batch gradient descent\n",
    "\n",
    "### References\n",
    "\n",
    "- I refer to the deep learning specialization course materials where applicable\n",
    "- I translated some of the numpy code from Jonathan Weisberg's \"Building a Neural Network from Scratch\" [part I](https://jonathanweisberg.org/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%201/) and [part II](https://jonathanweisberg.org/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%202/) to Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the MNIST dataset\n",
    "\n",
    "The MNIST dataset contains $60.000$ images of digits $1-10$. We use the MLDatasets module to retrieve and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets\n",
    "\n",
    "# Train & test data\n",
    "train_x, train_y = MNIST.traindata();\n",
    "test_x, test_y = MNIST.testdata();\n",
    "\n",
    "# Note that the data have been normalized already\n",
    "# See: https://juliaml.github.io/MLDatasets.jl/latest/datasets/MNIST/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shapes of the data are as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 60000)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "## Get shapes\n",
    "println(size(train_x))\n",
    "println(size(train_y))\n",
    "trainsize = size(train_x)[3];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find it easier to think about the data if the examples are in the row dimension. We will reshape the data such that $X$ is a tensor of shape $(60.000, 784)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape x\n",
    "train_x = transpose(reshape(train_x, 28 * 28, size(train_x)[3]));\n",
    "test_x = transpose(reshape(test_x, 28 * 28, size(test_x)[3]));\n",
    "\n",
    "# Reshape Y\n",
    "train_y = reshape(train_y, size(train_y)[1], 1);\n",
    "test_y = reshape(test_y, size(test_y)[1], 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y$ has $10$ unique labels ranging from $0-9$. However, this is actually a recoding of the data. MORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Int64,1}:\n",
       " 5\n",
       " 0\n",
       " 4\n",
       " 1\n",
       " 9\n",
       " 2\n",
       " 3\n",
       " 6\n",
       " 7\n",
       " 8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data until now\n",
    "\n",
    "- $X$ is a tensor with dimension $(60.000, 784)$\n",
    "- $Y$ is a tensor with dimension $(60.000, 1)$\n",
    "\n",
    "<img src=\"img/XandY2.png\" width = \"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a one-hot encoding for $Y$\n",
    "\n",
    "In order to train the neural network, we need to one-hot-encode <link> the outcome variable $Y$.\n",
    "\n",
    "The trick to doing this is as follows:\n",
    "\n",
    "- Create a $k \\times k$ identity matrix. In our case, we have $k=10$, so:\n",
    "\n",
    "$$\n",
    "I = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & \\dots & 0 \\\\\n",
    "0 & 1 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & 0 \\\\\n",
    "0 & 0 & \\dots & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- For each example in $Y$, *index the column of the value of the label*. For example, if the the label equals '5', we index the $5^{th}$ column, which has a $1$ on the $5^{th}$ element and zeroes elsewhere\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Because we do this $n$ times (the number of training examples), we will end up with a tensor of size $(10, 60.000)$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & \\dots & 1 \\\\\n",
    "1 & 1 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 1 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Again, I like to think in terms of (examples, dimensions) so we will transpose the one-hot-encoded matrix such that it has the dimensions $(60.000, 10)$\n",
    "\n",
    "<img src=\"img/y_one_hot.png\" width = \"300px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for y\n",
    "# We need a shape for outcome variable of size [num_labels, num_examples]\n",
    "\n",
    "# Number of distinct classes\n",
    "k = length(unique(train_y));\n",
    "# Number of training examples\n",
    "n = size(train_y)[1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "function one_hot_encode(y_labels)\n",
    "    \n",
    "    #=\n",
    "    From multiclass labels to one-hot encoding\n",
    "    \n",
    "    :param y_labels: column vector of shape (number of examples, 1) containing k > 2 classes\n",
    "    :return: one-hot encoded labels\n",
    "    =#\n",
    "    \n",
    "    # Number of distinct classes\n",
    "    k = length(unique(y_labels));\n",
    "    # Number of rows\n",
    "    n = size(y_labels)[1];\n",
    "    \n",
    "    # 1. Create k x k identity matrix \n",
    "\n",
    "    # Arrange the identity matrix such that, for each class, we get a one-hot encoded vector.\n",
    "    # This means that the rows are of length k (the number of distinct classes)\n",
    "    # The columns are of length n (the number of examples).\n",
    "    y_ohe = Matrix{Float64}(I, k, k)\n",
    "\n",
    "    # 2. Organize such that we get a k x m matrix. We do this by letting the label index\n",
    "    #     the column value. Since we have m labels, we index the columns m times.\n",
    "    #     So for m = 1.000 where m_1000 = 3, we index the kth column [0,0,1,0,...,k]\n",
    "    \n",
    "    # We have to add +1 to the classes (because Julia indexes from 1)\n",
    "    # Unlike e.g. numpy we need to explicitly call 'broadcast' to match shapes between\n",
    "    # two elements that we're adding\n",
    "    y_ohe = y_ohe[:, broadcast(+, y_labels, 1)][:,:];\n",
    "    \n",
    "    # Return\n",
    "    return(y_ohe)\n",
    "    \n",
    "    end;\n",
    "\n",
    "# One-hot encoding\n",
    "y_train_ohe = transpose(one_hot_encode(train_y));\n",
    "y_test_ohe = transpose(one_hot_encode(test_y));\n",
    "\n",
    "println(size(y_train_ohe))\n",
    "println(size(y_test_ohe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just to make sure that the one-hot encoding was done properly, we can randomly sample a number of values and check if the one-hot encoded examples line up with the original labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "\n",
    "function sanity(y_ohe, train_y; n_sample = 5)\n",
    "    \n",
    "    #=\n",
    "    Ensure that one-hot encoded labels are encoded properly by transforming them back into\n",
    "     labels.\n",
    "    \n",
    "    :param y_ohe: one-hot encoded training labels\n",
    "    :param train_y: array of size (examples, 1) containing the labels\n",
    "    :param n_sample: number of examples to sample randomly\n",
    "    \n",
    "    :return: This function does not return a value\n",
    "    =#\n",
    "    \n",
    "    # Shapes\n",
    "    n, k = size(y_ohe)\n",
    "    \n",
    "    # Pick a random example\n",
    "    ind_shuffled = shuffle(1:n)\n",
    "    \n",
    "    # Subset\n",
    "    ind = ind_shuffled[1:n_sample]\n",
    "    \n",
    "    # For each, print OHE + convert back to class\n",
    "    for i in ind\n",
    "        \n",
    "        # Find position of 1\n",
    "        pos = findall(x -> x==1, y_ohe[i, :])\n",
    "        \n",
    "        # Subtract 1\n",
    "        pos = pos[1] - 1\n",
    "        \n",
    "        # Print\n",
    "        println(\"Example \", i, \":\\n\", \n",
    "                \"\\t[OHE position: \", pos, \"] ==> [label: \", train_y[i,1], \"]\")\n",
    "        \n",
    "    end\n",
    "    \n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 50075:\n",
      "\t[OHE position: 8] ==> [label: 8]\n",
      "Example 35275:\n",
      "\t[OHE position: 5] ==> [label: 5]\n",
      "Example 39240:\n",
      "\t[OHE position: 4] ==> [label: 4]\n",
      "Example 41723:\n",
      "\t[OHE position: 4] ==> [label: 4]\n",
      "Example 16998:\n",
      "\t[OHE position: 0] ==> [label: 0]\n"
     ]
    }
   ],
   "source": [
    "sanity(y_train_ohe, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNet functions\n",
    "\n",
    "Plan: build a neural net with one hidden layer and $n_h$ hidden units\n",
    "\n",
    "TODO:\n",
    "\n",
    "- Notation\n",
    "- Matrix calculus (see https://atmos.washington.edu/~dennis/MatrixCalculus.pdf)\n",
    "- Images of the neural network structure\n",
    "- Backprop computation graph image + derivatives\n",
    "    * Also when using Tanh\n",
    "- References\n",
    "    * Chollet Keras\n",
    "    * Coursera course\n",
    "- 'safe' softmax function\n",
    "- dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "function cross_validation(X, Y, split_prop = 0.05)\n",
    "    \n",
    "    #=\n",
    "    Create train/test split\n",
    "    =#\n",
    "    \n",
    "    # Number of training examples\n",
    "    trainsize = size(X)[1]\n",
    "    \n",
    "    # Number of validation examples\n",
    "    ndev = Integer(floor(trainsize * split_prop))\n",
    "    \n",
    "    # Shuffle indices\n",
    "    ind = shuffle(1:trainsize);\n",
    "    # Rearrange train x and y\n",
    "    X = X[ind, :];\n",
    "    Y = Y[ind, :];\n",
    "\n",
    "    # Validation split\n",
    "    dev_x = X[1:ndev, :];\n",
    "    dev_y = Y[1:ndev, :];\n",
    "\n",
    "    # Remove from train\n",
    "    train_x = X[ndev+1:end, :];\n",
    "    train_y = Y[ndev+1:end, :];\n",
    "    \n",
    "    # To dict\n",
    "    split = Dict(\n",
    "        \"X_train\" => train_x,\n",
    "        \"Y_train\" => train_y,\n",
    "        \"X_dev\" => dev_x,\n",
    "        \"Y_dev\" => dev_y\n",
    "    )\n",
    "    \n",
    "    # Return\n",
    "    return(split)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xavier initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "function initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    #=\n",
    "    Initialize the weight matrices and bias vectors\n",
    "    \n",
    "    see: https://stats.stackexchange.com/questions/47590/what-are-good-initial-weights-in-a-neural-network\n",
    "    =#\n",
    "    \n",
    "    W1 = randn(n_h, n_x) .* sqrt(1 / n_x)\n",
    "    b1 = zeros(n_h, 1)\n",
    "    W2 = randn(n_y, n_h) .* sqrt(1 / n_h)\n",
    "    b2 = zeros(n_y, 1)\n",
    "    \n",
    "    # Exponentially weighted values initialization\n",
    "    vdW1 = zeros(n_h, n_x)\n",
    "    vdb1 = zeros(n_h, 1)\n",
    "    vdW2 = zeros(n_y, n_h)\n",
    "    vdb2 = zeros(n_y, 1)\n",
    "    \n",
    "    # Create parameters dict\n",
    "    parameters = Dict(\n",
    "        \"W1\" => W1,\n",
    "        \"b1\" => b1,\n",
    "        \"W2\" => W2,\n",
    "        \"b2\" => b2\n",
    "    )\n",
    "    \n",
    "    # Create cache dict\n",
    "    cache = Dict(\n",
    "        \"vdW1\" => vdW1,\n",
    "        \"vdb1\" => vdb1,\n",
    "        \"vdW2\" => vdW2,\n",
    "        \"vdb2\" => vdb2\n",
    "    )\n",
    "    \n",
    "    # Return\n",
    "    return((parameters, cache))\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch gradient descent\n",
    "\n",
    "An algorithm for splitting the data into batches is given below. Here:\n",
    "\n",
    "- $n$ is the number of training examples\n",
    "- $k$ is the batch size\n",
    "- $j$ is the number of total batches that can be created **using the batch size $k$**\n",
    "\n",
    "That is, $j$ is the integer part of the division of $\\frac{n}{k}$.\n",
    "\n",
    "<img src=\"img/minibatch.png\" width = \"500px\">\n",
    "\n",
    "This toy example is given as an algorithm below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch 1\n",
      "\t[Begin 1] ==> [end 10]\n",
      "Minibatch 2\n",
      "\t[Begin 11] ==> [end 20]\n",
      "Minibatch 3\n",
      "\t[Begin 21] ==> [end 30]\n",
      "Minibatch 4\n",
      "\t[Begin 31] ==> [end 40]\n",
      "Minibatch 5\n",
      "\t[Begin 41] ==> [end 50]\n",
      "Minibatch 6\n",
      "\t[Begin 51] ==> [end 60]\n",
      "Minibatch 7\n",
      "\t[Begin 61] ==> [end 70]\n",
      "Minibatch 8\n",
      "\t[Begin 71] ==> [end 80]\n",
      "Minibatch 9\n",
      "\t[Begin 81] ==> [end 90]\n",
      "Minibatch 10\n",
      "\t[Begin 91] ==> [end 100]\n",
      "Minibatch 11\n",
      "\t[Begin 101] ==> [end 101]\n"
     ]
    }
   ],
   "source": [
    "# Number of examples\n",
    "n = 101;\n",
    "# Batch size\n",
    "k = 10;\n",
    "# Number of batches of size k (discounting any remainders)\n",
    "j = floor(n/k);\n",
    "\n",
    "# For each minibatch\n",
    "for i = 1:j\n",
    "    \n",
    "    # Start at 1 if minibatch is 1\n",
    "    if i == 1\n",
    "        index_begin = 1\n",
    "    else\n",
    "        # Else start at the end of the previous minibatch plus one\n",
    "        index_begin = Integer(((i - 1) * k) + 1)\n",
    "        end;\n",
    "    \n",
    "    # End \n",
    "    index_end = Integer((i * k))\n",
    "    \n",
    "    println(\"Minibatch \", Integer(i), \"\\n\\t[Begin \", index_begin, \"] ==> [end \", index_end, \"]\")\n",
    "    \n",
    "    end;\n",
    "\n",
    "# If there is a remainder\n",
    "if n - (j * k) > 0\n",
    "    \n",
    "    index_begin = Integer((j * k) + 1)\n",
    "    index_end = n\n",
    "    \n",
    "    println(\"Minibatch \", Integer(j+1), \"\\n\\t[Begin \", index_begin, \"] ==> [end \", index_end, \"]\")\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below implements mini-batches for real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_batches(X, Y, batch_size = 128)\n",
    "    \n",
    "    #=\n",
    "    Creates b batches of size batch_size\n",
    "    \n",
    "    =#\n",
    "    \n",
    "    # Shuffle training examples randomly\n",
    "    n = size(X)[1]\n",
    "    ind = shuffle(1:n)\n",
    "    \n",
    "    # Rearrange data in X and Y\n",
    "    X = X[ind, :]\n",
    "    Y = Y[ind, :]\n",
    "    \n",
    "    # List to store minibatches\n",
    "    mbatches = []\n",
    "    \n",
    "    # Number of complete training examples. \n",
    "    #  This means: n / 128 leaves a remainder (most likely)\n",
    "    #  Therefore, there will be (b - 1) batches with size batch_size\n",
    "    #  and 1 batch with size last_batch_size < batch_size\n",
    "    b_first = Integer(floor(n / batch_size))\n",
    "    \n",
    "    # First, loop through the (b_first - 1) examples\n",
    "    #  We need to loop through b_first - 1 because we construct \n",
    "    #    @ index_begin => i * batch_size (e.g. 9 * 128 = 1152)\n",
    "    #    @ index_end => (i + 1) * batch_size (e.g. 10 * 128 = 1280)\n",
    "    #  index_end needs (i + 1). If i == k where k is the last possible index, we cannot subset index_end\n",
    "    #  because it would require (k + 1) indices.\n",
    "    for i = 1:b_first\n",
    "        \n",
    "        # Beginning and end indices\n",
    "        if i == 1\n",
    "            index_begin = 1\n",
    "        else\n",
    "            index_begin = Integer(((i - 1) * batch_size) + 1)\n",
    "            end;\n",
    "        \n",
    "        index_end = Integer(i * (batch_size))\n",
    "        \n",
    "        X_current_batch = X[index_begin:index_end, :]\n",
    "        Y_current_batch = Y[index_begin:index_end, :]\n",
    "\n",
    "        # Add to array of minibatches\n",
    "        push!(mbatches, [X_current_batch, Y_current_batch])\n",
    "        \n",
    "        end;\n",
    "    \n",
    "    # Then, if necessary, make a batch for the remainder\n",
    "    b_rem = n - (b_first * batch_size)\n",
    "    if b_rem != 0\n",
    "        \n",
    "        # Subset X & Y\n",
    "        index_begin = Integer(((b_first) * batch_size) + 1) # i+1 is from the for-loop above (i.e. the last 'full' minibatch of size batch_size)\n",
    "        index_end = n \n",
    "        \n",
    "        X_current_batch = X[index_begin:index_end, :]\n",
    "        Y_current_batch = Y[index_begin:index_end, :]\n",
    "        \n",
    "        # Append\n",
    "        push!(mbatches, [X_current_batch, Y_current_batch])\n",
    "        \n",
    "        end;\n",
    "    \n",
    "    # Return mini batches\n",
    "    return(mbatches)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "function softmax(z)\n",
    "    \n",
    "    #=\n",
    "    Softmax function\n",
    "    \n",
    "    :param z: result of the linear transformation W_jZ_jprev + b_j of the <j>th layer\n",
    "    :return: activations for z\n",
    "    =#\n",
    "    \n",
    "    return(exp.(z) ./ sum(exp.(z),dims=1))\n",
    "    \n",
    "    end;\n",
    "\n",
    "function sigmoid(z)\n",
    "    \n",
    "    #=\n",
    "    Sigmoid function\n",
    "    \n",
    "    =#\n",
    "    \n",
    "    1 ./ (1 .+ exp.(-z))\n",
    "    \n",
    "    end;\n",
    "\n",
    "function crossentropy_cost(y, yhat)\n",
    "    \n",
    "    #=\n",
    "    Crossentropy cost function for m classes\n",
    "    \n",
    "    :param y: actual (one-hot encoded) values for y\n",
    "    :param yhat: predicted (one-hot encoded) values for yhat\n",
    "    \n",
    "    :return: loss value\n",
    "    =#\n",
    "    \n",
    "    loss = sum(transpose(y) .* log.(yhat))\n",
    "    n = size(y)[1]\n",
    "    \n",
    "    return(-(1/n) * loss)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized dropout\n",
    "function dropout(X, drop_chance = 0.2)\n",
    "    \n",
    "    #=\n",
    "    Dropout implementation\n",
    "    =#\n",
    "    \n",
    "    n = size(X)[1]\n",
    "    k = size(X)[2]\n",
    "    \n",
    "    # Uniformly distributed probabilities\n",
    "    a = rand(n, k)\n",
    "    \n",
    "    # Evaluate against dropout probability\n",
    "    #  run the mask and turn boolean into integer\n",
    "    D = (a .< drop_chance) * 1\n",
    "    \n",
    "    # Element-wise multiplication\n",
    "    X = D .* X\n",
    "\n",
    "    # Scale\n",
    "    X = X ./ drop_chance\n",
    "    \n",
    "    # Return\n",
    "    return((D, X))\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "\n",
    "<img src=\"img/dims.png\" width = \"300px\">\n",
    "\n",
    "<img src=\"img/forwardprop.png\" width = \"800px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "function forward_prop(parameters, cache, X, drop_chance = 0.2)\n",
    "    \n",
    "    #=\n",
    "    Forward propagation\n",
    "    \n",
    "    \n",
    "    =#\n",
    "    \n",
    "    # Retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Forward propagation\n",
    "    Z1 = broadcast(+, W1 * transpose(X), b1, 1)\n",
    "    A1 = sigmoid(Z1)\n",
    "    \n",
    "    # Apply dropout\n",
    "    D, A1 = dropout(A1, drop_chance)\n",
    "    \n",
    "    # Output layer\n",
    "    Z2 = broadcast(+, W2 * A1, b2, 1)\n",
    "    A2 = softmax(Z2)\n",
    "    \n",
    "    # Return values \n",
    "    cache[\"Z1\"] = Z1\n",
    "    cache[\"A1\"] = A1\n",
    "    cache[\"Z2\"] = Z2\n",
    "    cache[\"A2\"] = A2\n",
    "    cache[\"D\"] = D\n",
    "    \n",
    "    # Return\n",
    "    return(cache)   \n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "function backward_prop(cache, parameters, X, Y, drop_chance)\n",
    "    \n",
    "    #=\n",
    "    Backward propagation\n",
    "    \n",
    "    # Docs about dims argument for sum()\n",
    "      - https://docs.julialang.org/en/v0.6.1/stdlib/collections/#Base.sum\n",
    "    =#\n",
    "    \n",
    "    # Dims\n",
    "    n = size(Y)[1]\n",
    "    \n",
    "    # Unroll cache\n",
    "    A2 = cache[\"A2\"]\n",
    "    Z2 = cache[\"Z2\"]\n",
    "    A1 = cache[\"A1\"]\n",
    "    Z1 = cache[\"Z1\"]\n",
    "    D = cache[\"D\"]\n",
    "    \n",
    "    # Retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Gradients\n",
    "    dZ2 = A2 .- transpose(Y)\n",
    "    dW2 = (1/n) .* (dZ2 * transpose(A1))\n",
    "    db2 = (1/n) .* sum(dZ2, dims = 2)  \n",
    "    dA1 = transpose(W2) * dZ2\n",
    "    \n",
    "    # Apply dropout\n",
    "    dA1 = D .* dA1\n",
    "    dA1 = dA1 ./ drop_chance\n",
    "    \n",
    "    dZ1 = dA1 .* sigmoid(Z1) .* (1 .- sigmoid(Z1))\n",
    "    dW1 = (1/n) .* (dZ1 * X)\n",
    "    db1 = (1/n) .* sum(dZ1, dims=2)\n",
    "    \n",
    "    # Add to cache and return\n",
    "    cache[\"dW2\"] = dW2\n",
    "    cache[\"db2\"] = db2\n",
    "    cache[\"dW1\"] = dW1\n",
    "    cache[\"db1\"] = db1\n",
    "    \n",
    "    # Return cache\n",
    "    return(cache)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating weights: RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "function RMSprop(parameters, cache, alpha, beta = 0.9, epsilon = 1e-8)\n",
    "    \n",
    "    #=\n",
    "    Update parameters and implement exponentially weighted averages\n",
    "    \n",
    "    =#\n",
    "    \n",
    "    # Unroll cache\n",
    "    dW2 = cache[\"dW2\"]\n",
    "    db2 = cache[\"db2\"]\n",
    "    dW1 = cache[\"dW1\"]\n",
    "    db1 = cache[\"db1\"]\n",
    "    \n",
    "    vdW2 = cache[\"vdW2\"]\n",
    "    vdb2 = cache[\"vdb2\"]\n",
    "    vdW1 = cache[\"vdW1\"]\n",
    "    vdb1 = cache[\"vdb1\"]\n",
    "    \n",
    "    # Retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Create new vdW and vdb\n",
    "    #  Notice that we square the gradients element-wise\n",
    "    vdW2 = beta .* vdW2 .+ (1 - beta) .* dW2.^2\n",
    "    vdb2 = beta .* vdb2 .+ (1 - beta) .* db2.^2\n",
    "    vdW1 = beta .* vdW1 .+ (1 - beta) .* dW1.^2\n",
    "    vdb1 = beta .* vdb1 .+ (1 - beta) .* db1.^2\n",
    "    \n",
    "    # Update parameters\n",
    "    W2 = W2 .- (alpha .* (dW2 ./ (sqrt.(vdW2) .+ epsilon)))\n",
    "    b2 = b2 .- (alpha .* (db2 ./ (sqrt.(vdb2) .+ epsilon)))\n",
    "    W1 = W1 .- (alpha .* (dW1 ./ (sqrt.(vdW1) .+ epsilon)))\n",
    "    b1 = b1 .- (alpha .* (db1 ./ (sqrt.(vdb1) .+ epsilon)))\n",
    "    #W2 = W2 .- (alpha .* dW2)\n",
    "    #b1 = b1 .- (alpha .* db1)\n",
    "    #W1 = W1 .- (alpha .* dW1)\n",
    "    #b2 = b2 .- (alpha .* db2)\n",
    "    \n",
    "    # Store parameters\n",
    "    parameters[\"W2\"] = W2\n",
    "    parameters[\"b2\"] = b2\n",
    "    parameters[\"W1\"] = W1\n",
    "    parameters[\"b1\"] = b1\n",
    "    \n",
    "    # Store exponentially weighted gradients\n",
    "    cache[\"vdW2\"] = vdW2\n",
    "    cache[\"vdb2\"] = vdb2\n",
    "    cache[\"vdW1\"] = vdW1\n",
    "    cache[\"vdb1\"] = vdb1\n",
    "    \n",
    "    # Return parameters and cache\n",
    "    return((parameters, cache))\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "function learning_rate_decay(alpha, epoch, decay = 1)\n",
    "    \n",
    "    #= \n",
    "    Implement learning rate decay\n",
    "    \n",
    "    =#\n",
    "    \n",
    "    alpha_new = 1 / (1 + decay * epoch) * alpha\n",
    "    \n",
    "    # Return\n",
    "    return(alpha_new)\n",
    "    \n",
    "    end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(X, parameters)\n",
    "    \n",
    "    #=\n",
    "    Predict Y given X and the parameters\n",
    "    =#\n",
    "    \n",
    "    # Unroll parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Forward prop\n",
    "    Z1 = broadcast(+, W1 * transpose(X), b1, 1)\n",
    "    A1 = sigmoid(Z1)    \n",
    "    # Output layer\n",
    "    Z2 = broadcast(+, W2 * A1, b2, 1)\n",
    "    A2 = softmax(Z2)\n",
    "    \n",
    "    # Return\n",
    "    return(A2)\n",
    "    \n",
    "    end;\n",
    "\n",
    "function accuracy(Y, Yhat)\n",
    "    \n",
    "    #=\n",
    "    Calculates accuracy\n",
    "    =# \n",
    "    \n",
    "    # Predictions --> labels\n",
    "    vals, inds = findmax(Yhat, dims = 1);\n",
    "    Yhat_labels = map(x -> x[1] - 1, inds)\n",
    "\n",
    "    # Y ohe --> labels\n",
    "    vals, inds = findmax(transpose(Y), dims=1)\n",
    "    Y_labels = map(x -> x[1] - 1, inds)\n",
    "    \n",
    "    # Accuracy\n",
    "    return(sum(Y_labels .== Yhat_labels) / length(Y_labels))\n",
    "    \n",
    "    end;\n",
    "\n",
    "function evaluate_model(X, Y, parameters; return_accuracy = false)\n",
    "    \n",
    "    #= \n",
    "    Evaluate the model on the development & train sets\n",
    "    =#\n",
    "    \n",
    "    # Predict\n",
    "    Yhat = predict(X, parameters)\n",
    "    \n",
    "    # Loss\n",
    "    loss = crossentropy_cost(Y, Yhat)\n",
    "    \n",
    "    # Accuracy\n",
    "    if return_accuracy\n",
    "        acc = accuracy(Y, Yhat)\n",
    "        return((loss, acc))\n",
    "        \n",
    "        end;\n",
    "    \n",
    "    # Return\n",
    "    return(loss)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling it all together: the nnet() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "function nnet(X, Y; hidden_units = 10, validation_split = 0.1, alpha = 0.05, beta=0.9,\n",
    "              epsilon=10e-8, epochs = 10, batch_size=128, lr_decay=0.5, dropout_prop=0.3)\n",
    "    \n",
    "    # Set dimensions\n",
    "    n = size(X)[1]\n",
    "    n_x = size(X)[2]\n",
    "    n_y = size(Y)[2]\n",
    "    n_h = hidden_units\n",
    "\n",
    "    # Save alpha value\n",
    "    alpha_initial = alpha\n",
    "\n",
    "    # Initialize parameters\n",
    "    parameters, cache = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "    # Loop through epochs\n",
    "    for i = 1:epochs\n",
    "\n",
    "        # Validation and train set\n",
    "        data = cross_validation(X, Y, validation_split)\n",
    "\n",
    "        # Unroll\n",
    "        X_train = data[\"X_train\"]\n",
    "        Y_train = data[\"Y_train\"]\n",
    "        X_dev = data[\"X_dev\"]\n",
    "        Y_dev = data[\"Y_dev\"]\n",
    "\n",
    "        # Set up number of batches\n",
    "        batches = create_batches(X_train, Y_train, batch_size)\n",
    "\n",
    "        ## Mini-batches (inner loop)\n",
    "        for batch in batches\n",
    "\n",
    "            # Unroll X and y\n",
    "            X = batch[1]\n",
    "            Y = batch[2]\n",
    "\n",
    "            # Forward prop\n",
    "            cache = forward_prop(parameters, cache, X, dropout_prop)\n",
    "\n",
    "            # Compute cost\n",
    "            A2 = cache[\"A2\"]\n",
    "            cost = crossentropy_cost(Y, A2)\n",
    "\n",
    "            # Backward propagation\n",
    "            cache = backward_prop(cache, parameters, X, Y, dropout_prop)\n",
    "\n",
    "            # Update params\n",
    "            parameters, cache = RMSprop(parameters, cache, alpha, beta, epsilon)\n",
    "\n",
    "            end;\n",
    "\n",
    "        # Learning rate decay\n",
    "        alpha = learning_rate_decay(alpha_initial, i, lr_decay)\n",
    "\n",
    "        # Evaluate\n",
    "        eval_train = evaluate_model(X_train, Y_train, parameters)\n",
    "        eval_dev, eval_acc = evaluate_model(X_dev, Y_dev, parameters, return_accuracy=true)\n",
    "\n",
    "        # Print\n",
    "        println(\"Epoch \", i, \":\\n\",\n",
    "                \"\\t[train loss: \", round(eval_train, digits=3), \n",
    "                \"] ==> [validation loss: \", round(eval_dev, digits=3), \"]\",\n",
    "                \"\\n\\tvalidation accuracy: \", round(eval_acc, digits=3))\n",
    "\n",
    "        end;\n",
    "    \n",
    "    # Return the final parameters\n",
    "    return(parameters)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\t[train loss: 0.262] ==> [validation loss: 0.316]\n",
      "\tvalidation accuracy: 0.936\n",
      "Epoch 2:\n",
      "\t[train loss: 0.0] ==> [validation loss: 0.0]\n",
      "\tvalidation accuracy: 1.0\n",
      "Epoch 3:\n",
      "\t[train loss: 0.0] ==> [validation loss: 0.0]\n",
      "\tvalidation accuracy: 1.0\n",
      "Epoch 4:\n",
      "\t[train loss: 0.0] ==> [validation loss: 0.0]\n",
      "\tvalidation accuracy: 1.0\n",
      "Epoch 5:\n",
      "\t[train loss: 0.0] ==> [validation loss: 0.0]\n",
      "\tvalidation accuracy: 1.0\n",
      "Epoch 6:\n",
      "\t[train loss: 0.0] ==> [validation loss: 0.0]\n",
      "\tvalidation accuracy: 1.0\n",
      "Epoch 7:\n",
      "\t[train loss: 0.0] ==> [validation loss: 0.0]\n",
      "\tvalidation accuracy: 1.0\n",
      "Epoch 8:\n",
      "\t[train loss: 0.0] ==> [validation loss: 0.0]\n",
      "\tvalidation accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# For clarity\n",
    "X = train_x\n",
    "Y = y_train_ohe\n",
    "\n",
    "# Run the neural net\n",
    "parameters = nnet(X, Y, hidden_units=128, epochs=8, beta=0.3,\n",
    "                  lr_decay=1, dropout_prop=0.9, validation_split = 0.05);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yhat = predict(X, parameters);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9462"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = accuracy(Y, Yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On test\n",
    "Yhat_test = predict(test_x, parameters);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.945"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "acc_test = accuracy(y_test_ohe, Yhat_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Development code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized dropout\n",
    "function dropout2(X, drop_chance = 0.2)\n",
    "    \n",
    "    #=\n",
    "    Dropout implementation\n",
    "    =#\n",
    "    \n",
    "    n = size(X)[1]\n",
    "    k = size(X)[2]\n",
    "    \n",
    "    # Uniformly distributed probabilities\n",
    "    a = rand(n, k)\n",
    "    \n",
    "    # Evaluate against dropout probability\n",
    "    #  run the mask and turn boolean into integer\n",
    "    D = (a .< drop_chance) * 1\n",
    "    \n",
    "    # Element-wise multiplication\n",
    "    X = D .* X\n",
    "\n",
    "    # Scale\n",
    "    X = X ./ drop_chance\n",
    "    \n",
    "    # Return\n",
    "    return((D, X))\n",
    "    \n",
    "    end;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
