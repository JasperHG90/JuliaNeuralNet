{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNET\n",
    "\n",
    "This is my Jupyter notebook with notes / implementation of a shallow neural network in Julia. All materials are based on deeplearning.ai's Deep Learning Specialization. I refer to specific videos and other materials where applicable. **The notebook is not yet finished**\n",
    "\n",
    "I created this notebook to review some of the concepts I learned in the first three courses of the deep learning specialization. Its purpose is primarily for me to process the information and produce the implementation from my own understanding rather than simply copying it from another place. As a result, you may find that the implementation notes are somewhat long-winded or tangential.\n",
    "\n",
    "If you find a mistake or think this code can be improved, it would be great if you could fork the repository and make a pull request. You could also leave a comment in the issues section.\n",
    "\n",
    "### This notebook contains\n",
    "\n",
    "- Loading & pre-processing the MNIST dataset\n",
    "- forward and backward propagation using a 2-layer neural network\n",
    "- ReLU / sigmoid / 'safe' softmax activation functions\n",
    "- Inverted dropout implementation\n",
    "- Weight initialization \n",
    "- Adam\n",
    "- Learning rate decay\n",
    "- mini-batch gradient descent\n",
    "\n",
    "### References\n",
    "\n",
    "- I refer to the deep learning specialization course materials where applicable\n",
    "- I translated some of the numpy code from Jonathan Weisberg's \"Building a Neural Network from Scratch\" [part I](https://jonathanweisberg.org/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%201/) and [part II](https://jonathanweisberg.org/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%202/) to Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the MNIST dataset\n",
    "\n",
    "The MNIST dataset contains $60.000$ images of digits $1-10$. We use the MLDatasets module to retrieve and load the data. Normally, we would [normalize](https://www.coursera.org/learn/deep-neural-network/lecture/lXv6U/normalizing-inputs) the input data such that it has mean $0$ and standard deviation $1$ (also known as a standard normal distribution). However, this has already been done for this version of the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets\n",
    "\n",
    "# Train & test data\n",
    "train_x, train_y = MNIST.traindata();\n",
    "test_x, test_y = MNIST.testdata();\n",
    "\n",
    "# Note that the data have been normalized already\n",
    "# See: https://juliaml.github.io/MLDatasets.jl/latest/datasets/MNIST/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shapes of the data are as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 60000)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "## Get shapes\n",
    "println(size(train_x))\n",
    "println(size(train_y))\n",
    "trainsize = size(train_x)[3];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find it easier to think about the data if the examples are in the row dimension. We will reshape the data such that $X$ is a tensor of shape $(60.000, 784)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape x\n",
    "train_x = transpose(reshape(train_x, 28 * 28, size(train_x)[3]));\n",
    "test_x = transpose(reshape(test_x, 28 * 28, size(test_x)[3]));\n",
    "\n",
    "# Reshape Y\n",
    "train_y = reshape(train_y, size(train_y)[1], 1);\n",
    "test_y = reshape(test_y, size(test_y)[1], 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.13768007202881152\n",
      "Var: 0.09768609056323807\n",
      "\n",
      "Mean: 0.1555372148859544\n",
      "Var: 0.10835898025851963\n",
      "\n",
      "Mean: 0.09725390156062426\n",
      "Var: 0.06622370038341628\n",
      "\n"
     ]
    }
   ],
   "source": [
    "using Statistics\n",
    "size(train_x)\n",
    "\n",
    "for i = 1:3\n",
    "    \n",
    "    println(\"Mean: \", mean(train_x[i, :]), \"\\nVar: \", var(train_x[i,:]), \"\\n\")\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 2.8463626013986793e-17\n",
      "Var: 1.0000000000000002\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tt = train_x[1,:]\n",
    "tt_trans = (tt .- mean(tt)) / sqrt(var(tt))\n",
    "\n",
    "println(\"Mean: \", mean(tt_trans), \"\\nVar: \", var(tt_trans), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y$ has $10$ unique labels ranging from $0-9$. However, this is actually a recoding of the data. MORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Int64,1}:\n",
       " 5\n",
       " 0\n",
       " 4\n",
       " 1\n",
       " 9\n",
       " 2\n",
       " 3\n",
       " 6\n",
       " 7\n",
       " 8"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data until now\n",
    "\n",
    "- $X$ is a tensor with dimension $(60.000, 784)$\n",
    "- $Y$ is a tensor with dimension $(60.000, 1)$\n",
    "\n",
    "<img src=\"img/XandY2.png\" width = \"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a one-hot encoding for $Y$\n",
    "\n",
    "In order to train the neural network, we need to one-hot-encode <link> the outcome variable $Y$.\n",
    "\n",
    "The trick to doing this is as follows:\n",
    "\n",
    "- Create a $k \\times k$ identity matrix. In our case, we have $k=10$, so:\n",
    "\n",
    "$$\n",
    "I = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & \\dots & 0 \\\\\n",
    "0 & 1 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & 0 \\\\\n",
    "0 & 0 & \\dots & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- For each example in $Y$, *index the column of the value of the label*. For example, if the the label equals '5', we index the $5^{th}$ column, which has a $1$ on the $5^{th}$ element and zeroes elsewhere\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Because we do this $n$ times (the number of training examples), we will end up with a tensor of size $(10, 60.000)$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & \\dots & 1 \\\\\n",
    "1 & 1 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 1 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Again, I like to think in terms of (examples, dimensions) so we will transpose the one-hot-encoded matrix such that it has the dimensions $(60.000, 10)$\n",
    "\n",
    "<img src=\"img/y_one_hot.png\" width = \"300px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for y\n",
    "# We need a shape for outcome variable of size [num_labels, num_examples]\n",
    "\n",
    "# Number of distinct classes\n",
    "k = length(unique(train_y));\n",
    "# Number of training examples\n",
    "n = size(train_y)[1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "function one_hot_encode(y_labels)\n",
    "    \n",
    "    #=\n",
    "    From multiclass labels to one-hot encoding\n",
    "    \n",
    "    :param y_labels: column vector of shape (number of examples, 1) containing k > 2 classes\n",
    "    \n",
    "    :return: one-hot encoded labels\n",
    "    =#\n",
    "    \n",
    "    # Number of distinct classes\n",
    "    k = length(unique(y_labels));\n",
    "    # Number of rows\n",
    "    n = size(y_labels)[1];\n",
    "    \n",
    "    # 1. Create k x k identity matrix \n",
    "\n",
    "    # Arrange the identity matrix such that, for each class, we get a one-hot encoded vector.\n",
    "    # This means that the rows are of length k (the number of distinct classes)\n",
    "    # The columns are of length n (the number of examples).\n",
    "    y_ohe = Matrix{Float64}(I, k, k)\n",
    "\n",
    "    # 2. Organize such that we get a k x m matrix. We do this by letting the label index\n",
    "    #     the column value. Since we have m labels, we index the columns m times.\n",
    "    #     So for m = 1.000 where m_1000 = 3, we index the kth column [0,0,1,0,...,k]\n",
    "    \n",
    "    # We have to add +1 to the classes (because Julia indexes from 1)\n",
    "    # Unlike e.g. numpy we need to explicitly call 'broadcast' to match shapes between\n",
    "    # two elements that we're adding\n",
    "    y_ohe = y_ohe[:, broadcast(+, y_labels, 1)][:,:];\n",
    "    \n",
    "    # Return\n",
    "    return(y_ohe)\n",
    "    \n",
    "    end;\n",
    "\n",
    "# One-hot encoding\n",
    "y_train_ohe = transpose(one_hot_encode(train_y));\n",
    "y_test_ohe = transpose(one_hot_encode(test_y));\n",
    "\n",
    "println(size(y_train_ohe))\n",
    "println(size(y_test_ohe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just to make sure that the one-hot encoding was done properly, we can randomly sample a number of values and check if the one-hot encoded examples line up with the original labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "\n",
    "function sanity(y_ohe, train_y; n_sample = 5)\n",
    "    \n",
    "    #=\n",
    "    Ensure that one-hot encoded labels are encoded properly by transforming them back into\n",
    "     labels.\n",
    "    \n",
    "    :param y_ohe: one-hot encoded training labels\n",
    "    :param train_y: array of size (examples, 1) containing the labels\n",
    "    :param n_sample: number of examples to sample randomly\n",
    "    \n",
    "    :return: This function does not return a value\n",
    "    =#\n",
    "    \n",
    "    # Shapes\n",
    "    n, k = size(y_ohe)\n",
    "    \n",
    "    # Pick a random example\n",
    "    ind_shuffled = shuffle(1:n)\n",
    "    \n",
    "    # Subset\n",
    "    ind = ind_shuffled[1:n_sample]\n",
    "    \n",
    "    # For each, print OHE + convert back to class\n",
    "    for i in ind\n",
    "        \n",
    "        # Find position of 1\n",
    "        pos = findall(x -> x==1, y_ohe[i, :])\n",
    "        \n",
    "        # Subtract 1\n",
    "        pos = pos[1] - 1\n",
    "        \n",
    "        # Print\n",
    "        println(\"Example \", i, \":\\n\", \n",
    "                \"\\t[OHE position: \", pos, \"] ==> [label: \", train_y[i,1], \"]\")\n",
    "        \n",
    "    end\n",
    "    \n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 9455:\n",
      "\t[OHE position: 7] ==> [label: 7]\n",
      "Example 24470:\n",
      "\t[OHE position: 3] ==> [label: 3]\n",
      "Example 35866:\n",
      "\t[OHE position: 1] ==> [label: 1]\n",
      "Example 21226:\n",
      "\t[OHE position: 5] ==> [label: 5]\n",
      "Example 42609:\n",
      "\t[OHE position: 0] ==> [label: 0]\n"
     ]
    }
   ],
   "source": [
    "sanity(y_train_ohe, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNet functions\n",
    "\n",
    "Plan: build a neural net with one hidden layer and $n_h$ hidden units\n",
    "\n",
    "TODO:\n",
    "\n",
    "- Notation\n",
    "- Matrix calculus (see https://atmos.washington.edu/~dennis/MatrixCalculus.pdf)\n",
    "- Images of the neural network structure\n",
    "- Backprop computation graph image + derivatives\n",
    "    * Also when using Tanh\n",
    "- References\n",
    "    * Chollet Keras\n",
    "    * Coursera course\n",
    "- 'safe' softmax function\n",
    "- dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "function cross_validation(X, Y, split_prop = 0.05)\n",
    "    \n",
    "    #=\n",
    "    Create train/test split\n",
    "    \n",
    "    :param X: Input data X\n",
    "    :param Y: output data Y\n",
    "    :param split_prop: percentage of data to use as validation set\n",
    "    \n",
    "    :return: dictionary with train & validation data\n",
    "    :seealso: https://www.coursera.org/learn/deep-neural-network/lecture/cxG1s/train-dev-test-sets\n",
    "    =#\n",
    "    \n",
    "    # Number of training examples\n",
    "    trainsize = size(X)[1]\n",
    "    \n",
    "    # Number of validation examples\n",
    "    ndev = Integer(floor(trainsize * split_prop))\n",
    "    \n",
    "    # Shuffle indices\n",
    "    ind = shuffle(1:trainsize);\n",
    "    # Rearrange train x and y\n",
    "    X = X[ind, :];\n",
    "    Y = Y[ind, :];\n",
    "\n",
    "    # Validation split\n",
    "    dev_x = X[1:ndev, :];\n",
    "    dev_y = Y[1:ndev, :];\n",
    "\n",
    "    # Remove from train\n",
    "    train_x = X[ndev+1:end, :];\n",
    "    train_y = Y[ndev+1:end, :];\n",
    "    \n",
    "    # To dict\n",
    "    split = Dict(\n",
    "        \"X_train\" => train_x,\n",
    "        \"Y_train\" => train_y,\n",
    "        \"X_dev\" => dev_x,\n",
    "        \"Y_dev\" => dev_y\n",
    "    )\n",
    "    \n",
    "    # Return\n",
    "    return(split)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight initialization\n",
    "\n",
    "$$\n",
    "W^{[l]} = (n^{[l]},n^{[l-1]}) \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "function initialize_layer(n_h_current, n_h_last, activation, layer; drop_chance = 0.2, last=false)\n",
    "    \n",
    "    #=\n",
    "    Initialize a layer\n",
    "    =#\n",
    "    \n",
    "    # Weights\n",
    "    W = randn(n_h_current, n_h_last)\n",
    "    b = zeros(n_h_current, 1)\n",
    "\n",
    "    # Weight initialization\n",
    "    if activation == \"ReLU\"\n",
    "\n",
    "        W = W .* sqrt(2 / n_h_last)\n",
    "\n",
    "    else\n",
    "\n",
    "        W = W .* sqrt(1 / n_h_last)\n",
    "\n",
    "        end;\n",
    "    \n",
    "    # Initialization for Adam / RMSprop\n",
    "    vdW = zeros(n_h_current, n_h_last)\n",
    "    vdb = zeros(n_h_current, 1)\n",
    "    \n",
    "    sdW = zeros(n_h_current, n_h_last)\n",
    "    sdb = zeros(n_h_current, 1)\n",
    "    \n",
    "    # To dict\n",
    "    current_layer = Dict(\n",
    "        \"layer\" => layer,\n",
    "        \"last_layer\" => last,\n",
    "        \"activation\" => activation,\n",
    "        \"hidden_current\" => n_h_current,\n",
    "        \"hidden_previous\" => n_h_last,\n",
    "        \"W\" => W,\n",
    "        \"b\" => b,\n",
    "        \"vdW\" => vdW,\n",
    "        \"vdb\" => vdb,\n",
    "        \"sdW\" => sdW,\n",
    "        \"sdb\" => sdb\n",
    "    )\n",
    "    \n",
    "    # If last_layer == false, add drop chance\n",
    "    if last == false\n",
    "        \n",
    "        current_layer[\"keep_prob\"] = 1- drop_chance\n",
    "        \n",
    "        end;\n",
    "    \n",
    "    # Return\n",
    "    return(current_layer)\n",
    "    \n",
    "    end;\n",
    "\n",
    "function initialize_parameters(n_x, n_h, n_y, activations, layers, dropout)\n",
    "    \n",
    "    #=\n",
    "    Initialize the weight matrices and bias vectors\n",
    "    \n",
    "    :param n_x: number of observations in the input data\n",
    "    :param n_h1: number of hidden units in the first layer\n",
    "    :param n_h2: number of hidden units in the second layer\n",
    "    :param n_y: number of labels in the output data\n",
    "    \n",
    "    :return:\n",
    "       - parameters: dict containing weights W and bias vectors b for each layer\n",
    "       - cache:      dict containing the moving average of the gradient (first moment) and moving average of the second gradient (second moment)\n",
    "    \n",
    "    :seealso: \n",
    "       - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/XtFPI/random-initialization\n",
    "       - https://www.coursera.org/learn/deep-neural-network/lecture/C9iQO/vanishing-exploding-gradients\n",
    "       - https://www.coursera.org/learn/deep-neural-network/lecture/RwqYe/weight-initialization-for-deep-networks\n",
    "       - https://stats.stackexchange.com/questions/47590/what-are-good-initial-weights-in-a-neural-network\n",
    "       - https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94\n",
    "    =#\n",
    "    \n",
    "    # For sigmoid / tanh / softmax --> Xavier weight initialization --> 1 / sqrt(units in previous layer)\n",
    "    # For ReLU weight initialization --> 2 / sqrt(units in previous layer)\n",
    "    \n",
    "    # List to hold results\n",
    "    layer_details = []\n",
    "    \n",
    "    # Add 1 to layers (last layer is the softmax layer)\n",
    "    layers = layers + 1\n",
    "    \n",
    "    # For each layer, initialize\n",
    "    for layer = 1:layers\n",
    "        \n",
    "        # If first layer, then pass the value of n_x\n",
    "        if layer == 1\n",
    "            \n",
    "            # Expect a tuple if layers > 2\n",
    "            current_layer = initialize_layer(ifelse(layers <= 2, \n",
    "                                                    n_h, \n",
    "                                                    n_h[1]), \n",
    "                                             n_x, \n",
    "                                             ifelse(layers <= 2, \n",
    "                                                    activations, \n",
    "                                                    activations[layer]), \n",
    "                                             layer,\n",
    "                                             drop_chance = ifelse(layers <= 2, \n",
    "                                                            dropout, \n",
    "                                                            dropout[layer]))\n",
    "          \n",
    "        # If last layer, then make it a softmax and pass n_y\n",
    "        elseif(layer == layers)\n",
    "            \n",
    "            current_layer = initialize_layer(n_y, \n",
    "                                             ifelse(layers <= 2,\n",
    "                                                    n_h,\n",
    "                                                    n_h[layer-1]), \n",
    "                                             \"softmax\",\n",
    "                                             layer, \n",
    "                                             last=true)\n",
    "        \n",
    "        # Else, we have the hidden layers in between input X and output Y\n",
    "        else\n",
    "            \n",
    "            current_layer = initialize_layer(n_h[layer], n_h[layer-1], activations[layer], layer,\n",
    "                                             drop_chance = dropout[layer])\n",
    "            \n",
    "            end;\n",
    "        \n",
    "        # Push layer\n",
    "        push!(layer_details, current_layer)\n",
    "        \n",
    "        end;\n",
    "    \n",
    "    # Return\n",
    "    return(layer_details)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple layers\n",
    "layers_multiple = initialize_parameters(784, (256, 128), 10, (\"ReLU\", \"ReLU\"), 2, (0.2, 0.4));\n",
    "# Single layer\n",
    "layers_single = initialize_parameters(784, 256, 10, \"ReLU\", 1, 0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Any} with 12 entries:\n",
       "  \"W\"               => [0.0197382 0.0216007 … -0.0784624 0.0485954; -0.0959504 …\n",
       "  \"b\"               => [0.0; 0.0; … ; 0.0; 0.0]\n",
       "  \"keep_prob\"       => 0.6\n",
       "  \"vdW\"             => [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0…\n",
       "  \"sdb\"             => [0.0; 0.0; … ; 0.0; 0.0]\n",
       "  \"hidden_current\"  => 128\n",
       "  \"layer\"           => 2\n",
       "  \"activation\"      => \"ReLU\"\n",
       "  \"sdW\"             => [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0…\n",
       "  \"hidden_previous\" => 256\n",
       "  \"last_layer\"      => false\n",
       "  \"vdb\"             => [0.0; 0.0; … ; 0.0; 0.0]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_multiple[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Any} with 12 entries:\n",
       "  \"W\"               => [-0.0135244 0.0334305 … 0.0225554 0.0404762; -0.0443972 …\n",
       "  \"b\"               => [0.0; 0.0; … ; 0.0; 0.0]\n",
       "  \"keep_prob\"       => 0.5\n",
       "  \"vdW\"             => [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0…\n",
       "  \"sdb\"             => [0.0; 0.0; … ; 0.0; 0.0]\n",
       "  \"hidden_current\"  => 256\n",
       "  \"layer\"           => 1\n",
       "  \"activation\"      => \"ReLU\"\n",
       "  \"sdW\"             => [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0…\n",
       "  \"hidden_previous\" => 784\n",
       "  \"last_layer\"      => false\n",
       "  \"vdb\"             => [0.0; 0.0; … ; 0.0; 0.0]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_single[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch gradient descent\n",
    "\n",
    "An algorithm for splitting the data into batches is given below. Here:\n",
    "\n",
    "- $n$ is the number of training examples\n",
    "- $k$ is the batch size\n",
    "- $j$ is the number of total batches that can be created **using the batch size $k$**\n",
    "\n",
    "That is, $j$ is the integer part of the division of $\\frac{n}{k}$.\n",
    "\n",
    "<img src=\"img/minibatch.png\" width = \"500px\">\n",
    "\n",
    "This toy example is given as an algorithm below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch 1\n",
      "\t[Begin 1] ==> [end 10]\n",
      "Minibatch 2\n",
      "\t[Begin 11] ==> [end 20]\n",
      "Minibatch 3\n",
      "\t[Begin 21] ==> [end 30]\n",
      "Minibatch 4\n",
      "\t[Begin 31] ==> [end 40]\n",
      "Minibatch 5\n",
      "\t[Begin 41] ==> [end 50]\n",
      "Minibatch 6\n",
      "\t[Begin 51] ==> [end 60]\n",
      "Minibatch 7\n",
      "\t[Begin 61] ==> [end 70]\n",
      "Minibatch 8\n",
      "\t[Begin 71] ==> [end 80]\n",
      "Minibatch 9\n",
      "\t[Begin 81] ==> [end 90]\n",
      "Minibatch 10\n",
      "\t[Begin 91] ==> [end 100]\n",
      "Minibatch 11\n",
      "\t[Begin 101] ==> [end 101]\n"
     ]
    }
   ],
   "source": [
    "# Number of examples\n",
    "n = 101;\n",
    "# Batch size\n",
    "k = 10;\n",
    "# Number of batches of size k (discounting any remainders)\n",
    "j = floor(n/k);\n",
    "\n",
    "# For each minibatch\n",
    "for i = 1:j\n",
    "    \n",
    "    # Start at 1 if minibatch is 1\n",
    "    if i == 1\n",
    "        index_begin = 1\n",
    "    else\n",
    "        # Else start at the end of the previous minibatch plus one\n",
    "        index_begin = Integer(((i - 1) * k) + 1)\n",
    "        end;\n",
    "    \n",
    "    # End \n",
    "    index_end = Integer((i * k))\n",
    "    \n",
    "    println(\"Minibatch \", Integer(i), \"\\n\\t[Begin \", index_begin, \"] ==> [end \", index_end, \"]\")\n",
    "    \n",
    "    end;\n",
    "\n",
    "# If there is a remainder\n",
    "if n - (j * k) > 0\n",
    "    \n",
    "    index_begin = Integer((j * k) + 1)\n",
    "    index_end = n\n",
    "    \n",
    "    println(\"Minibatch \", Integer(j+1), \"\\n\\t[Begin \", index_begin, \"] ==> [end \", index_end, \"]\")\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below implements mini-batches for real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_batches(X, Y, batch_size = 128)\n",
    "    \n",
    "    #=\n",
    "    Creates b batches of size batch_size\n",
    "    \n",
    "    :param X:          input data X\n",
    "    :param Y:          output data Y\n",
    "    :param batch_size: size of each batch\n",
    "    \n",
    "    :return: list containing m batches of length batch_size and possibly 1 batch of size batch_size_remainder < batch_size\n",
    "    :seealso: \n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/qcogH/mini-batch-gradient-descent\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/lBXu8/understanding-mini-batch-gradient-descent\n",
    "    =#\n",
    "    \n",
    "    # Shuffle training examples randomly\n",
    "    n = size(X)[1]\n",
    "    ind = shuffle(1:n)\n",
    "    \n",
    "    # Rearrange data in X and Y\n",
    "    X = X[ind, :]\n",
    "    Y = Y[ind, :]\n",
    "    \n",
    "    # List to store minibatches\n",
    "    mbatches = []\n",
    "    \n",
    "    # Number of complete training examples. \n",
    "    #  This means: n / 128 leaves a remainder (most likely)\n",
    "    #  Therefore, there will be (b - 1) batches with size batch_size\n",
    "    #  and 1 batch with size last_batch_size < batch_size\n",
    "    b_first = Integer(floor(n / batch_size))\n",
    "    \n",
    "    # First, loop through the (b_first - 1) examples\n",
    "    #  We need to loop through b_first - 1 because we construct \n",
    "    #    @ index_begin => i * batch_size (e.g. 9 * 128 = 1152)\n",
    "    #    @ index_end => (i + 1) * batch_size (e.g. 10 * 128 = 1280)\n",
    "    #  index_end needs (i + 1). If i == k where k is the last possible index, we cannot subset index_end\n",
    "    #  because it would require (k + 1) indices.\n",
    "    for i = 1:b_first\n",
    "        \n",
    "        # Beginning and end indices\n",
    "        if i == 1\n",
    "            index_begin = 1\n",
    "        else\n",
    "            index_begin = Integer(((i - 1) * batch_size) + 1)\n",
    "            end;\n",
    "        \n",
    "        index_end = Integer(i * (batch_size))\n",
    "        \n",
    "        X_current_batch = X[index_begin:index_end, :]\n",
    "        Y_current_batch = Y[index_begin:index_end, :]\n",
    "\n",
    "        # Add to array of minibatches\n",
    "        push!(mbatches, [X_current_batch, Y_current_batch])\n",
    "        \n",
    "        end;\n",
    "    \n",
    "    # Then, if necessary, make a batch for the remainder\n",
    "    b_rem = n - (b_first * batch_size)\n",
    "    if b_rem != 0\n",
    "        \n",
    "        # Subset X & Y\n",
    "        index_begin = Integer(((b_first) * batch_size) + 1) # i+1 is from the for-loop above (i.e. the last 'full' minibatch of size batch_size)\n",
    "        index_end = n \n",
    "        \n",
    "        X_current_batch = X[index_begin:index_end, :]\n",
    "        Y_current_batch = Y[index_begin:index_end, :]\n",
    "        \n",
    "        # Append\n",
    "        push!(mbatches, [X_current_batch, Y_current_batch])\n",
    "        \n",
    "        end;\n",
    "    \n",
    "    # Return mini batches\n",
    "    return(mbatches)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "function softmax(z)\n",
    "    \n",
    "    #=\n",
    "    Softmax function\n",
    "    \n",
    "    :param z: result of the linear transformation of the final layer\n",
    "    :return: activations for z\n",
    "    \n",
    "    :seealso:\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/4dDC1/activation-functions\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/OASKH/why-do-you-need-non-linear-activation-functions\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/HRy7y/softmax-regression\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/LCsCH/training-a-softmax-classifier\n",
    "    =#\n",
    "    \n",
    "    # Subtract the max of z to avoid exp(z) getting too large\n",
    "    z = z .- maximum(z)\n",
    "    \n",
    "    # Softmax & return\n",
    "    return(exp.(z) ./ sum(exp.(z),dims=1))\n",
    "    \n",
    "    end;\n",
    "\n",
    "function sigmoid(z)\n",
    "    \n",
    "    #=\n",
    "    Sigmoid function\n",
    "    \n",
    "    :param z: result of the linear transformation for layer l\n",
    "    :return: activations for z\n",
    "    \n",
    "    :seealso:\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/4dDC1/activation-functions\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/OASKH/why-do-you-need-non-linear-activation-functions\n",
    "    =#\n",
    "    \n",
    "    1 ./ (1 .+ exp.(-z))\n",
    "    \n",
    "    end;\n",
    "\n",
    "function ReLU(z)\n",
    "    \n",
    "    #=\n",
    "    ReLU implementation\n",
    "    \n",
    "    :param z: result of the linear transformation for layer l\n",
    "    :return: activations for z\n",
    "    \n",
    "    :seealso:\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/4dDC1/activation-functions\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/OASKH/why-do-you-need-non-linear-activation-functions\n",
    "    =#\n",
    "    \n",
    "    ((z .> 0) * 1) .* z\n",
    "    \n",
    "    end;\n",
    "\n",
    "function crossentropy_cost(y, yhat)\n",
    "    \n",
    "    #=\n",
    "    Crossentropy cost function for m classes\n",
    "    \n",
    "    :param y: actual (one-hot encoded) values for y\n",
    "    :param yhat: predicted (one-hot encoded) values for yhat\n",
    "    \n",
    "    :return: loss value\n",
    "    \n",
    "    :seealso: https://www.coursera.org/learn/neural-networks-deep-learning/lecture/yWaRd/logistic-regression-cost-function\n",
    "    =#\n",
    "    \n",
    "    loss = sum(transpose(y) .* log.(yhat))\n",
    "    n = size(y)[1]\n",
    "    \n",
    "    return(-(1/n) * loss)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized dropout\n",
    "function dropout(X, keep_prob = 0.8)\n",
    "    \n",
    "    #=\n",
    "    Dropout implementation. Randomly sets the weights of some hidden units to 0\n",
    "    \n",
    "    :param X: input data\n",
    "    :param keep_prob: probability of keeping a hidden unit\n",
    "    \n",
    "    :return: dropout matrix containing 1 for each unit that should be kept and 0 for each unit that should be \n",
    "              dropped. also returns X in which some units are dropped and the remaining units are scaled by \n",
    "              drop_chance.\n",
    "    \n",
    "    :seealso:\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/eM33A/dropout-regularization\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/YaGbR/understanding-dropout\n",
    "    =#\n",
    "    \n",
    "    n = size(X)[1]\n",
    "    k = size(X)[2]\n",
    "    \n",
    "    # Uniformly distributed probabilities\n",
    "    a = rand(n, k)\n",
    "    \n",
    "    # Evaluate against dropout probability\n",
    "    #  run the mask and turn boolean into integer\n",
    "    D = (a .< keep_prob) * 1\n",
    "\n",
    "    # Element-wise multiplication\n",
    "    X = D .* X\n",
    "\n",
    "    # Scale\n",
    "    X = X ./ keep_prob\n",
    "    \n",
    "    # Return\n",
    "    return((D, X))\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "\n",
    "<img src=\"img/dims.png\" width = \"300px\">\n",
    "\n",
    "<img src=\"img/forwardprop.png\" width = \"800px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "function forward_prop(layers, X)\n",
    "    \n",
    "    #=\n",
    "    Forward propagation\n",
    "    \n",
    "    :param parameters:  dict containing weights and bias vectors for each layer\n",
    "    :param cache:       dict containing computations, dropout vectors and exponentially weighted gradients\n",
    "    :param X:           input data\n",
    "    :param keep_prob:   probability of keeping a hidden unit\n",
    "    \n",
    "    :return: updated cache with dropout vectors and the computations for each forward pass for the current \n",
    "              minibatch\n",
    "    \n",
    "    :seealso:\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/4WdOY/computation-graph\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/tyAGh/computing-a-neural-networks-output\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/MijzH/forward-propagation-in-a-deep-network\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/Rz47X/getting-your-matrix-dimensions-right\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/znwiG/forward-and-backward-propagation\n",
    "    =#\n",
    "    \n",
    "    # For each layer, compute forward propagation step\n",
    "    for layer in layers\n",
    "        \n",
    "        # Layer number\n",
    "        layer_number = layer[\"layer\"]\n",
    "        # Last layer?\n",
    "        last_layer = layer[\"last_layer\"]\n",
    "        # Activation function\n",
    "        activation = layer[\"activation\"]\n",
    "        \n",
    "        ## Compute the linear transformation Z\n",
    "        \n",
    "        # If first layer, then use X (also known as A_0)\n",
    "        # Else: use A_{layer_number - 1}\n",
    "        \n",
    "        if layer_number == 1\n",
    "                        \n",
    "            # Multiply weights times X\n",
    "            layer[\"Z\"] = broadcast(+, layer[\"W\"] * transpose(X), layer[\"b\"], 1)\n",
    "            \n",
    "        else\n",
    "            \n",
    "            # Multiply weights times the activation of the previous layer\n",
    "            layer[\"Z\"] = broadcast(+, layer[\"W\"] * layers[layer_number - 1][\"A\"], layer[\"b\"], 1)\n",
    "            \n",
    "            end;\n",
    "        \n",
    "        ## Activation functions over Z ==> raise error if unknown\n",
    "        \n",
    "        if activation == \"ReLU\"\n",
    "            \n",
    "            A = ReLU(layer[\"Z\"])\n",
    "            \n",
    "        elseif activation == \"sigmoid\"\n",
    "            \n",
    "            A = sigmoid(layer[\"Z\"])\n",
    "            \n",
    "        elseif activation == \"tanh\"\n",
    "            \n",
    "            A = tanh.(layer[\"Z\"])\n",
    "            \n",
    "        elseif activation == \"softmax\"\n",
    "            \n",
    "            A = softmax(layer[\"Z\"])\n",
    "            \n",
    "        else\n",
    "            \n",
    "            # Raise error\n",
    "            error(string(\"Don't know how to handle activation function '\", activation, \"'\"))\n",
    "        \n",
    "            end;\n",
    "        \n",
    "        ## If last layer, then simply store the activations\n",
    "\n",
    "        if last_layer\n",
    "            \n",
    "            layer[\"A\"] = A\n",
    "\n",
    "        ## Else, we apply dropout to the layer\n",
    "        else\n",
    "            \n",
    "            D, A = dropout(A, layer[\"keep_prob\"])\n",
    "            \n",
    "            # Store the data in the layer\n",
    "            layer[\"D\"] = D\n",
    "            layer[\"A\"] = A\n",
    "            \n",
    "            end;\n",
    "        \n",
    "        end;\n",
    "    \n",
    "    # Return layers\n",
    "    return(layers)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One step of forward propagation\n",
    "X_tst = train_x[1:1000,:]\n",
    "layers = forward_prop(layers_multiple, X_tst);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Any} with 13 entries:\n",
       "  \"Z\"               => [0.166539 -0.699417 … 0.0807724 -1.7125; 0.135446 -1.574…\n",
       "  \"W\"               => [0.00860859 0.128189 … 0.0100919 -0.170718; 0.0105652 0.…\n",
       "  \"b\"               => [0.0; 0.0; … ; 0.0; 0.0]\n",
       "  \"vdW\"             => [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0…\n",
       "  \"sdb\"             => [0.0; 0.0; … ; 0.0; 0.0]\n",
       "  \"hidden_current\"  => 10\n",
       "  \"layer\"           => 3\n",
       "  \"activation\"      => \"softmax\"\n",
       "  \"sdW\"             => [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0…\n",
       "  \"A\"               => [0.00023732 0.0142732 … 0.0243917 0.000126885; 0.0002300…\n",
       "  \"hidden_previous\" => 128\n",
       "  \"last_layer\"      => true\n",
       "  \"vdb\"             => [0.0; 0.0; … ; 0.0; 0.0]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivatives of activation functions\n",
    "function dSigmoid(z)\n",
    "    \n",
    "    #=\n",
    "    Derivative of the sigmoid function\n",
    "    \n",
    "    :param z: result of the linear transformation computed during forward propagation. Generally:\n",
    "                   Z = W_lA_{l-1} + b_l\n",
    "    :return: derivative of the activation function\n",
    "    \n",
    "    :seealso: \n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/qcG1j/derivatives-of-activation-functions\n",
    "      - https://deepnotes.io/softmax-crossentropy\n",
    "      - https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "    =#\n",
    "    \n",
    "    sigmoid(z) .* (1 .- sigmoid(z))\n",
    "    \n",
    "    end;\n",
    "\n",
    "function dReLU(z)\n",
    "    \n",
    "    #=\n",
    "    Derivative of ReLU function\n",
    "    \n",
    "    :param z: result of the linear transformation computed during forward propagation. Generally:\n",
    "                   Z = W_lA_{l-1} + b_l\n",
    "    :return: derivative of the activation function\n",
    "    \n",
    "    :seealso: https://www.coursera.org/learn/neural-networks-deep-learning/lecture/qcG1j/derivatives-of-activation-functions\n",
    "    =#\n",
    "    \n",
    "    (z .> 0) * 1\n",
    "    \n",
    "    end;\n",
    "\n",
    "function dTanh(z)\n",
    "    \n",
    "    #=\n",
    "    Derivative of tanh function\n",
    "    \n",
    "    :param z: result of the linear transformation computed during forward propagation. Generally:\n",
    "                   Z = W_lA_{l-1} + b_l\n",
    "    :return: derivative of the activation function\n",
    "    \n",
    "    :seealso: https://www.coursera.org/learn/neural-networks-deep-learning/lecture/qcG1j/derivatives-of-activation-functions\n",
    "    =# \n",
    "    \n",
    "    1 .- (tanh.(z) .* tanh.(z))\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "function backward_prop(layers, X, Y)\n",
    "    \n",
    "    #=\n",
    "    Backward propagation\n",
    "    \n",
    "    :param parameters:  dict containing weights and bias vectors for each layer\n",
    "    :param cache:       dict containing computations, dropout vectors and exponentially weighted gradients\n",
    "    :param X:           input data\n",
    "    :param drop_chance: probability of keeping a hidden unit\n",
    "    \n",
    "    :return: updated cache with new gradients for the current minibatch\n",
    "    \n",
    "    :seealso:\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/0ULGt/derivatives\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/oEcPT/more-derivative-examples\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/4WdOY/computation-graph\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/0VSHe/derivatives-with-a-computation-graph\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/6dDj7/backpropagation-intuition-optional\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/znwiG/forward-and-backward-propagation\n",
    "      Docs about dims argument for sum()\n",
    "      - https://docs.julialang.org/en/v0.6.1/stdlib/collections/#Base.sum\n",
    "    =#\n",
    "    \n",
    "    # Dims\n",
    "    n = size(Y)[1]\n",
    "    \n",
    "    # For layers (reverse)\n",
    "    for layer_number = length(layers):-1:1\n",
    "        \n",
    "        # Last layer?\n",
    "        last_layer = layers[layer_number][\"last_layer\"]\n",
    "        # Activation function\n",
    "        activation = layers[layer_number][\"activation\"]\n",
    "        \n",
    "        ## Last layer derivatives (softmax)\n",
    "        \n",
    "        if last_layer\n",
    "            \n",
    "            # Softmax derivative\n",
    "            layers[layer_number][\"dZ\"] = layers[layer_number][\"A\"] .- transpose(Y)\n",
    "            layers[layer_number][\"dW\"] = (1/n) .* (layers[layer_number][\"dZ\"] * transpose(layers[layer_number - 1][\"A\"]))\n",
    "            layers[layer_number][\"db\"] = (1/n) .* sum(layers[layer_number][\"dZ\"], dims=2)\n",
    "            \n",
    "        ## First layer derivatives\n",
    "            \n",
    "        elseif layer_number == 1\n",
    "            \n",
    "            # Activations for layer 1\n",
    "            dA = transpose(layers[layer_number + 1][\"W\"]) * layers[layer_number + 1][\"dZ\"]\n",
    "            # Apply dropout\n",
    "            dA = (layers[layer_number][\"D\"] .* dA) ./ layers[layer_number][\"keep_prob\"] \n",
    "            \n",
    "            # Derivative of activation function\n",
    "            if activation == \"ReLU\"\n",
    "\n",
    "                layers[layer_number][\"dZ\"] = dA .* dReLU(layers[layer_number][\"Z\"])\n",
    "\n",
    "            # Sigmoid derivative\n",
    "            elseif activation == \"sigmoid\"\n",
    "\n",
    "                layers[layer_number][\"dZ\"] = dA .* dSigmoid(layers[layer_number][\"Z\"])\n",
    "\n",
    "            elseif activation == \"tanh\"\n",
    "\n",
    "                layers[layer_number][\"dZ\"] = dA .* dTanh(layers[layer_number][\"Z\"])\n",
    "\n",
    "                end;  \n",
    "        \n",
    "            layers[layer_number][\"dA\"] = dA\n",
    "            # Linear combination derivative\n",
    "            layers[layer_number][\"dW\"] = (1/n) .* (layers[layer_number][\"dZ\"] * X)\n",
    "            layers[layer_number][\"db\"] = (1/n) .* sum(layers[layer_number][\"dZ\"], dims=2)            \n",
    "            \n",
    "        ## Intermediate layer derivatives\n",
    "            \n",
    "        else\n",
    "            \n",
    "            dA = transpose(layers[layer_number + 1][\"W\"]) * layers[layer_number + 1][\"dZ\"]\n",
    "            # Apply dropout\n",
    "            dA = (layers[layer_number][\"D\"] .* dA) ./ layers[layer_number][\"keep_prob\"]\n",
    "            \n",
    "            # Derivative of activation function\n",
    "            if activation == \"ReLU\"\n",
    "\n",
    "                layers[layer_number][\"dZ\"] = dA .* dReLU(layers[layer_number][\"Z\"])\n",
    "\n",
    "            # Sigmoid derivative\n",
    "            elseif activation == \"sigmoid\"\n",
    "\n",
    "                layers[layer_number][\"dZ\"] = dA .* dSigmoid(layers[layer_number][\"Z\"])\n",
    "\n",
    "            elseif activation == \"tanh\"\n",
    "\n",
    "                layers[layer_number][\"dZ\"] = dA .* dTanh(layers[layer_number][\"Z\"])\n",
    "\n",
    "                end;\n",
    "        \n",
    "            layers[layer_number][\"dA\"] = dA\n",
    "            # Linear combination derivative\n",
    "            layers[layer_number][\"dW\"] = (1/n) .* (layers[layer_number][\"dZ\"] * transpose(layers[layer_number-1][\"A\"]))\n",
    "            layers[layer_number][\"db\"] = (1/n) .* sum(layers[layer_number][\"dZ\"], dims=2)\n",
    "            \n",
    "            end;\n",
    "        \n",
    "        end;\n",
    "\n",
    "    return(layers)\n",
    "\n",
    "    end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = backward_prop(layers, X_tst, y_train_ohe[1:1000, :], 0.8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Any} with 19 entries:\n",
       "  \"Z\"               => [2.24715 1.32048 … -0.14567 1.48374; -1.7845 -1.65666 … …\n",
       "  \"W\"               => [0.0222939 0.0241593 … -0.075905 0.0511557; -0.0940551 0…\n",
       "  \"dZ\"              => [-0.0 -0.0 … -0.0 -0.0687068; -0.0 -0.0 … 0.0 -0.0; … ; …\n",
       "  \"dW\"              => [-0.0412696 -0.0512823 … -0.0463351 -0.0621909; -0.00356…\n",
       "  \"b\"               => [0.00255677; 0.00216383; … ; -0.00256122; 0.0019188]\n",
       "  \"keep_prob\"       => 0.6\n",
       "  \"vdW\"             => [-0.00318153 -0.00393388 … -0.00355589 -0.00478182; -0.0…\n",
       "  \"sdb\"             => [1.76322e-6; 2.47475e-8; … ; 4.58368e-6; 1.27294e-8]\n",
       "  \"hidden_current\"  => 128\n",
       "  \"layer\"           => 2\n",
       "  \"activation\"      => \"ReLU\"\n",
       "  \"db\"              => [-0.0443403; -0.00512316; … ; 0.0652232; -0.00528962]\n",
       "  \"sdW\"             => [1.53969e-6 2.35399e-6 … 1.92335e-6 3.47814e-6; 1.20455e…\n",
       "  \"A\"               => [0.0 0.0 … -0.0 2.4729; -0.0 -0.0 … -0.0 -0.0; … ; 6.542…\n",
       "  \"hidden_previous\" => 256\n",
       "  \"D\"               => [0 0 … 1 1; 0 0 … 1 1; … ; 1 0 … 1 0; 1 1 … 1 1]\n",
       "  \"dA\"              => [-0.0 -0.0 … -0.0155662 -0.0687068; -0.0 -0.0 … 0.029184…\n",
       "  \"last_layer\"      => false\n",
       "  \"vdb\"             => [-0.00340466; -0.000403353; … ; 0.00548942; -0.000289283]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating weights: Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Adam(layers; alpha = 0.001, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8)\n",
    "    \n",
    "    #=\n",
    "    Update parameters using Adam optimization algorithm\n",
    "    \n",
    "    :param parameters:  dict containing weights and bias vectors for each layer\n",
    "    :param cache:       dict containing computations, dropout vectors and exponentially weighted gradients\n",
    "    :param alpha:       learning rate\n",
    "    :param beta:        exponential weighting value. Usually set at 0.9 <MORE>\n",
    "    :param epsilon:     small value to prevent divide by zero\n",
    "    \n",
    "    :return: updates weights in cache for current minibatch\n",
    "    \n",
    "    :seealso:\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/A0tBd/gradient-descent\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/udiAq/gradient-descent-on-m-examples\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/Wh8NI/gradient-descent-for-neural-networks\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/duStO/exponentially-weighted-averages\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/Ud7t0/understanding-exponentially-weighted-averages\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/XjuhD/bias-correction-in-exponentially-weighted-averages\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/y0m1f/gradient-descent-with-momentum\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/BhJlm/rmsprop\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/w9VCZ/adam-optimization-algorithm\n",
    "      - https://www.coursera.org/learn/deep-neural-network/notebook/eNLYh/optimization\n",
    "    =#\n",
    "    \n",
    "    # For each layer, update parameters\n",
    "    for layer in layers\n",
    "        \n",
    "        # Momentum\n",
    "        layer[\"vdW\"] = (beta1 .* layer[\"vdW\"] .+ (1 - beta1) .* layer[\"dW\"]) ./ (1 / beta1^2)\n",
    "        layer[\"vdb\"] = (beta1 .* layer[\"vdb\"] .+ (1 - beta1) .* layer[\"db\"]) ./ (1 / beta1^2)\n",
    "        \n",
    "        # RMSprop\n",
    "        layer[\"sdW\"] = (beta2 .* layer[\"sdW\"] .+ (1 - beta2) .* layer[\"dW\"].^2) ./ (1 / beta2^2)\n",
    "        layer[\"sdb\"] = (beta2 .* layer[\"sdb\"] .+ (1 - beta2) .* layer[\"db\"].^2) ./ (1 / beta2^2)\n",
    "        \n",
    "        # Update parameters\n",
    "        layer[\"W\"] = layer[\"W\"] .- (alpha .* (layer[\"vdW\"] ./ (sqrt.(layer[\"sdW\"] .+ epsilon))))\n",
    "        layer[\"b\"] = layer[\"b\"] .- (alpha .* (layer[\"vdb\"] ./ (sqrt.(layer[\"sdb\"] .+ epsilon))))\n",
    "        \n",
    "        end;\n",
    "    \n",
    "    # Return layers\n",
    "    return(layers)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = Adam(layers);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Any} with 19 entries:\n",
       "  \"Z\"               => [2.24715 1.32048 … -0.14567 1.48374; -1.7845 -1.65666 … …\n",
       "  \"W\"               => [0.0254372 0.0273053 … -0.0727598 0.0543023; -0.0914036 …\n",
       "  \"dZ\"              => [-0.0 -0.0 … -0.0 -0.0687068; -0.0 -0.0 … 0.0 -0.0; … ; …\n",
       "  \"dW\"              => [-0.0412696 -0.0512823 … -0.0463351 -0.0621909; -0.00356…\n",
       "  \"b\"               => [0.00570151; 0.00503769; … ; -0.00568662; 0.00476061]\n",
       "  \"keep_prob\"       => 0.6\n",
       "  \"vdW\"             => [-0.00566218 -0.00702166 … -0.00634538 -0.00852341; -0.0…\n",
       "  \"sdb\"             => [3.72008e-6; 5.08676e-8; … ; 8.8155e-6; 4.06154e-8]\n",
       "  \"hidden_current\"  => 128\n",
       "  \"layer\"           => 2\n",
       "  \"activation\"      => \"ReLU\"\n",
       "  \"db\"              => [-0.0443403; -0.00512316; … ; 0.0652232; -0.00528962]\n",
       "  \"sdW\"             => [3.23486e-6 4.97154e-6 … 4.06023e-6 7.32769e-6; 2.46872e…\n",
       "  \"A\"               => [0.0 0.0 … -0.0 2.4729; -0.0 -0.0 … -0.0 -0.0; … ; 6.542…\n",
       "  \"hidden_previous\" => 256\n",
       "  \"D\"               => [0 0 … 1 1; 0 0 … 1 1; … ; 1 0 … 1 0; 1 1 … 1 1]\n",
       "  \"dA\"              => [-0.0 -0.0 … -0.0155662 -0.0687068; -0.0 -0.0 … 0.029184…\n",
       "  \"last_layer\"      => false\n",
       "  \"vdb\"             => [-0.00607356; -0.00070902; … ; 0.00928486; -0.000639347]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "function learning_rate_decay(alpha, epoch, decay = 1)\n",
    "    \n",
    "    #= \n",
    "    Decays the value of alpha (learning rate) as number of epochs increases\n",
    "    \n",
    "    :param alpha: learning rate \n",
    "    :param epoch: current epoch number\n",
    "    :param decay: decay value\n",
    "    \n",
    "    :return: updated learning rate\n",
    "    \n",
    "    :seealso: https://www.coursera.org/learn/deep-neural-network/lecture/hjgIA/learning-rate-decay\n",
    "    =#\n",
    "    \n",
    "    alpha_new = 1 / (1 + decay * epoch) * alpha\n",
    "    \n",
    "    # Return\n",
    "    return(alpha_new)\n",
    "    \n",
    "    end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(X, layers)\n",
    "    \n",
    "    #=\n",
    "    Predict Y given X and the parameters\n",
    "    \n",
    "    :param X:          input data\n",
    "    :param parameters: parameters for each layer\n",
    "    \n",
    "    :return: predicted values given X and parameters\n",
    "    =#\n",
    "    \n",
    "    # Define scope for A_previous and A\n",
    "    local A_prev\n",
    "    local A\n",
    "    \n",
    "    # For each layer, compute forward propagation step\n",
    "    for layer in layers\n",
    "        \n",
    "        # Layer number\n",
    "        layer_number = layer[\"layer\"]\n",
    "        # Last layer?\n",
    "        last_layer = layer[\"last_layer\"]\n",
    "        # Activation function\n",
    "        activation = layer[\"activation\"]\n",
    "        \n",
    "        # If first layer\n",
    "        if layer_number == 1\n",
    "            \n",
    "            # Multiply weights times X\n",
    "            Z = broadcast(+, layer[\"W\"] * transpose(X), layer[\"b\"], 1)\n",
    "            \n",
    "        else\n",
    "            \n",
    "            # Multiply weights times the activation of the previous layer\n",
    "            Z = broadcast(+, layer[\"W\"] * A_prev, layer[\"b\"], 1)\n",
    "            \n",
    "            end;\n",
    "        \n",
    "        # Activation function\n",
    "        if activation == \"ReLU\"\n",
    "            \n",
    "            A = ReLU(Z)\n",
    "            A_prev = A\n",
    "            \n",
    "        elseif activation == \"sigmoid\"\n",
    "            \n",
    "            A = sigmoid(Z)\n",
    "            A_prev = A\n",
    "            \n",
    "        elseif activation == \"tanh\"\n",
    "            \n",
    "            A = tanh.(Z)\n",
    "            A_prev = A\n",
    "            \n",
    "        elseif activation == \"softmax\"\n",
    "            \n",
    "            A = softmax(Z)\n",
    "            \n",
    "            end;\n",
    "                \n",
    "        end;\n",
    "    \n",
    "    # Return\n",
    "    return(A)\n",
    "    \n",
    "    end;\n",
    "\n",
    "function accuracy(Y, Yhat)\n",
    "    \n",
    "    #=\n",
    "    Calculates accuracy\n",
    "    \n",
    "    :param Y:    actual outcome values (as one-hot encoded) \n",
    "    :param Yhat: predicted outcome values (as one-hot encoded)\n",
    "    \n",
    "    :return: accuracy of the predictions as a float between 0 and 1\n",
    "    =# \n",
    "    \n",
    "    # Predictions --> labels\n",
    "    vals, inds = findmax(Yhat, dims = 1);\n",
    "    Yhat_labels = map(x -> x[1] - 1, inds)\n",
    "\n",
    "    # Y ohe --> labels\n",
    "    vals, inds = findmax(transpose(Y), dims=1)\n",
    "    Y_labels = map(x -> x[1] - 1, inds)\n",
    "    \n",
    "    # Accuracy\n",
    "    return(sum(Y_labels .== Yhat_labels) / length(Y_labels))\n",
    "    \n",
    "    end;\n",
    "\n",
    "function evaluate_model(X, Y, layers; return_accuracy = false)\n",
    "    \n",
    "    #= \n",
    "    Evaluate the model on the development & train sets\n",
    "    \n",
    "    :param X:               input data \n",
    "    :param Y:               output data\n",
    "    :param parameters:      dict containing parameters for each layer\n",
    "    :param return_accuracy: boolean. If true, returns loss and accuracy. If false, only returns loss\n",
    "    \n",
    "    :return: either loss and accuracy or only loss value\n",
    "    =#\n",
    "    \n",
    "    # Predict\n",
    "    Yhat = predict(X, layers)\n",
    "    \n",
    "    # Loss\n",
    "    loss = crossentropy_cost(Y, Yhat)\n",
    "    \n",
    "    # Accuracy\n",
    "    if return_accuracy\n",
    "        acc = accuracy(Y, Yhat)\n",
    "        return((loss, acc))\n",
    "        \n",
    "        end;\n",
    "    \n",
    "    # Return\n",
    "    return(loss)\n",
    "    \n",
    "    end;\n",
    "\n",
    "using Plots\n",
    "\n",
    "function plot_history(history)\n",
    "    \n",
    "    #=\n",
    "    Plot loss across epochs\n",
    "    \n",
    "    :param history: nn history returned by nnet function\n",
    "    \n",
    "    :return: this function plots the history but does not return a value \n",
    "    \n",
    "    :seealso: https://docs.juliaplots.org/latest/tutorial/#plot-recipes-and-recipe-libraries\n",
    "    =#\n",
    "    \n",
    "    # Retrieve data\n",
    "    epoch = [x[1] for x in history]\n",
    "    train_loss = [x[2] for x in history]\n",
    "    dev_loss = [x[3] for x in history]\n",
    "    # Train and dev loss in one array (2 columns)\n",
    "    loss = hcat(train_loss, dev_loss);\n",
    "    \n",
    "    # Plot epochs versus loss\n",
    "    plot(epoch, loss, seriestype=:line, xlabel = \"epoch\", ylabel = \"cost\", lab = [\"Train\" \"Dev\"],\n",
    "         lw = 2)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6575001180296347"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yhat = predict(X_tst, layers)\n",
    "crossentropy_cost(y_train_ohe[1:1000, :], Yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling it all together: the nnet() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "function nnet(X, Y; n_h = 128, activations = \"tanh\", validation_split = 0.1, \n",
    "              alpha = 0.001, beta1=0.9, beta2 = 0.999, epsilon=10e-8, epochs = 10, batch_size=128, \n",
    "              lr_decay=1, dropout_prop=0.3)\n",
    "    \n",
    "    #= \n",
    "    Implements a two-layer neural network\n",
    "    \n",
    "    :param X:                   input data\n",
    "    :param Y:                   actual outcome values (as one-hot encoded) \n",
    "    :param n_h1:                hidden units for layer 1\n",
    "    :param n_h2:                hidden units for layer 2\n",
    "    :param validation_split:    decimal indicating what percentage of the data should be reserved for \n",
    "                                 validation split\n",
    "    :param alpha:               learning rate \n",
    "    :param beta:                exponential weighting value. Usually set at 0.9 <MORE>\n",
    "    :param epsilon:             small value to prevent divide by zero in RMSprop \n",
    "    :param epochs:              number of passes (epochs) to train the network\n",
    "    :param batch_size:          size of the minibatches\n",
    "    :param lr_decay:            decay value for learning rate (alpha)\n",
    "    :param dropout_prop:        probability of dropping a hidden unit when performing dropout regularization. \n",
    "                                 A value of 0 keeps all hidden units\n",
    "    \n",
    "    :return: final parameters for the trained model and training history\n",
    "    =#\n",
    "    \n",
    "    ## Checks\n",
    "    \n",
    "    # hidden units, activations and dropout must all be tuple\n",
    "    if all((isa(n_h, Tuple), isa(activations, Tuple), isa(dropout_prop, Tuple)))\n",
    "        \n",
    "        @assert length(n_h) == length(activations) \"Number of hidden units must equal number of activations\"\n",
    "        @assert length(n_h) == length(dropout_prop) \"Number of hidden units must equal the number of dropout probabilities\"\n",
    "        \n",
    "        hidden_layers = length(n_h)\n",
    "        \n",
    "    else\n",
    "        \n",
    "        hidden_layers = 1\n",
    "        \n",
    "        end;\n",
    "    \n",
    "    # Set dimensions\n",
    "    n = size(X)[1]\n",
    "    n_x = size(X)[2]\n",
    "    n_y = size(Y)[2]\n",
    "\n",
    "    # Save alpha value\n",
    "    alpha_initial = alpha\n",
    "    \n",
    "    # Turn dropout proportion into keep proportion\n",
    "    dropout_keep_prop = 1 .- dropout_prop\n",
    "\n",
    "    # Initialize parameters\n",
    "    layers = initialize_parameters(n_x, n_h, n_y, activations, hidden_layers, dropout_keep_prop)\n",
    "    \n",
    "    # Open list for history\n",
    "    history = []\n",
    "    \n",
    "    # Validation and train set\n",
    "    data = cross_validation(X, Y, validation_split)\n",
    "\n",
    "    # Unroll\n",
    "    X_train = data[\"X_train\"]\n",
    "    Y_train = data[\"Y_train\"]\n",
    "    X_dev = data[\"X_dev\"]\n",
    "    Y_dev = data[\"Y_dev\"]\n",
    "\n",
    "    # Loop through epochs\n",
    "    for i = 1:epochs\n",
    "\n",
    "        # Set up number of batches\n",
    "        batches = create_batches(X_train, Y_train, batch_size)\n",
    "        \n",
    "        ## Mini-batches (inner loop)\n",
    "        for batch in batches\n",
    "\n",
    "            # Unroll X and y\n",
    "            X = batch[1]\n",
    "            Y = batch[2]\n",
    "\n",
    "            # Forward prop\n",
    "            layers = forward_prop(layers, X)\n",
    "\n",
    "            # Backward propagation\n",
    "            layers = backward_prop(layers, X, Y)\n",
    "            \n",
    "            # Update params\n",
    "            layers = Adam(layers, alpha = alpha, beta1 = beta1, beta2 = beta2, epsilon = epsilon)\n",
    "\n",
    "            end;\n",
    "\n",
    "        # Learning rate decay\n",
    "        alpha = learning_rate_decay(alpha_initial, i, lr_decay)\n",
    "\n",
    "        # Evaluate\n",
    "        eval_train = evaluate_model(X_train, Y_train, layers)\n",
    "        eval_dev, eval_acc = evaluate_model(X_dev, Y_dev, layers, return_accuracy=true)\n",
    "        \n",
    "        # Round\n",
    "        eval_train = round(eval_train, digits=3)\n",
    "        eval_dev = round(eval_dev, digits=3)\n",
    "\n",
    "        # Print\n",
    "        println(\"Epoch \", i, \":\\n\",\n",
    "                \"\\t[train cost: \", eval_train, \n",
    "                \"] ==> [validation cost: \", eval_dev, \"]\",\n",
    "                \"\\n\\tvalidation accuracy: \", round(eval_acc, digits=3))\n",
    "        \n",
    "        # Add to history\n",
    "        push!(history, [i, eval_train, eval_dev])\n",
    "\n",
    "        end;\n",
    "    \n",
    "    # Return the final parameters\n",
    "    return((layers, history))\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the network\n",
    "\n",
    "### Hyperparameter tuning\n",
    "\n",
    "See: https://stats.stackexchange.com/questions/369104/what-is-a-sensible-order-for-parameter-tuning-in-neural-networks\n",
    "\n",
    "see: https://towardsdatascience.com/a-guide-to-an-efficient-way-to-build-neural-network-architectures-part-i-hyper-parameter-8129009f131b\n",
    "\n",
    "see: https://www.jeremyjordan.me/nn-learning-rate/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For clarity\n",
    "X = train_x\n",
    "Y = y_train_ohe\n",
    "\n",
    "# Split test into dev (70%) test (30%)\n",
    "Random.seed!(9726)\n",
    "split = cross_validation(test_x, y_test_ohe, 0.3)\n",
    "X_dev = split[\"X_train\"]\n",
    "Y_dev = split[\"Y_train\"]\n",
    "X_test = split[\"X_dev\"]\n",
    "Y_test = split[\"Y_dev\"];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\t[train cost: 0.274] ==> [validation cost: 0.283]\n",
      "\tvalidation accuracy: 0.915\n",
      "Epoch 2:\n",
      "\t[train cost: 0.188] ==> [validation cost: 0.199]\n",
      "\tvalidation accuracy: 0.939\n",
      "Epoch 3:\n",
      "\t[train cost: 0.148] ==> [validation cost: 0.167]\n",
      "\tvalidation accuracy: 0.947\n",
      "Epoch 4:\n",
      "\t[train cost: 0.116] ==> [validation cost: 0.14]\n",
      "\tvalidation accuracy: 0.96\n",
      "Epoch 5:\n",
      "\t[train cost: 0.104] ==> [validation cost: 0.132]\n",
      "\tvalidation accuracy: 0.96\n",
      "Epoch 6:\n",
      "\t[train cost: 0.087] ==> [validation cost: 0.115]\n",
      "\tvalidation accuracy: 0.966\n",
      "Epoch 7:\n",
      "\t[train cost: 0.076] ==> [validation cost: 0.107]\n",
      "\tvalidation accuracy: 0.968\n",
      "Epoch 8:\n",
      "\t[train cost: 0.068] ==> [validation cost: 0.103]\n",
      "\tvalidation accuracy: 0.97\n",
      "Epoch 9:\n",
      "\t[train cost: 0.064] ==> [validation cost: 0.098]\n",
      "\tvalidation accuracy: 0.972\n",
      "Epoch 10:\n",
      "\t[train cost: 0.062] ==> [validation cost: 0.096]\n",
      "\tvalidation accuracy: 0.971\n",
      "Epoch 11:\n",
      "\t[train cost: 0.052] ==> [validation cost: 0.097]\n",
      "\tvalidation accuracy: 0.972\n",
      "Epoch 12:\n",
      "\t[train cost: 0.044] ==> [validation cost: 0.09]\n",
      "\tvalidation accuracy: 0.974\n",
      "Epoch 13:\n",
      "\t[train cost: 0.041] ==> [validation cost: 0.089]\n",
      "\tvalidation accuracy: 0.975\n",
      "Epoch 14:\n",
      "\t[train cost: 0.04] ==> [validation cost: 0.091]\n",
      "\tvalidation accuracy: 0.975\n",
      "Epoch 15:\n",
      "\t[train cost: 0.035] ==> [validation cost: 0.085]\n",
      "\tvalidation accuracy: 0.976\n",
      "Epoch 16:\n",
      "\t[train cost: 0.037] ==> [validation cost: 0.093]\n",
      "\tvalidation accuracy: 0.975\n",
      "Epoch 17:\n",
      "\t[train cost: 0.033] ==> [validation cost: 0.09]\n",
      "\tvalidation accuracy: 0.977\n",
      "Epoch 18:\n",
      "\t[train cost: 0.03] ==> [validation cost: 0.085]\n",
      "\tvalidation accuracy: 0.977\n",
      "Epoch 19:\n",
      "\t[train cost: 0.028] ==> [validation cost: 0.082]\n",
      "\tvalidation accuracy: 0.979\n",
      "Epoch 20:\n",
      "\t[train cost: 0.026] ==> [validation cost: 0.081]\n",
      "\tvalidation accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "layers, history = nnet(X, Y, n_h = (256, 128, 128), activations = (\"ReLU\", \"ReLU\", \"ReLU\"),\n",
    "                       dropout_prop=(0.6, 0.5, 0.4), batch_size = 64, alpha = 0.001, epochs=20,\n",
    "                       lr_decay=0, validation_split = 0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip6000\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2000\" height=\"2000\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<defs>\n",
       "  <clipPath id=\"clip6001\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip6001)\" points=\"\n",
       "0,1600 2400,1600 2400,0 0,0 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip6002\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip6001)\" points=\"\n",
       "251.149,1440.48 2321.26,1440.48 2321.26,47.2441 251.149,47.2441 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip6003\">\n",
       "    <rect x=\"251\" y=\"47\" width=\"2071\" height=\"1394\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip6003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  720.881,1440.48 720.881,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1234.81,1440.48 1234.81,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1748.74,1440.48 1748.74,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2262.67,1440.48 2262.67,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  251.149,1278.31 2321.26,1278.31 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  251.149,1022.59 2321.26,1022.59 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  251.149,766.878 2321.26,766.878 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  251.149,511.163 2321.26,511.163 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  251.149,255.447 2321.26,255.447 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,1440.48 2321.26,1440.48 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,1440.48 251.149,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  720.881,1440.48 720.881,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1234.81,1440.48 1234.81,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1748.74,1440.48 1748.74,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2262.67,1440.48 2262.67,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,1278.31 282.2,1278.31 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,1022.59 282.2,1022.59 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,766.878 282.2,766.878 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,511.163 282.2,511.163 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,255.447 282.2,255.447 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip6001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 720.881, 1494.48)\" x=\"720.881\" y=\"1494.48\">5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1234.81, 1494.48)\" x=\"1234.81\" y=\"1494.48\">10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1748.74, 1494.48)\" x=\"1748.74\" y=\"1494.48\">15</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2262.67, 1494.48)\" x=\"2262.67\" y=\"1494.48\">20</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 227.149, 1295.81)\" x=\"227.149\" y=\"1295.81\">0.05</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 227.149, 1040.09)\" x=\"227.149\" y=\"1040.09\">0.10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 227.149, 784.378)\" x=\"227.149\" y=\"784.378\">0.15</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 227.149, 528.663)\" x=\"227.149\" y=\"528.663\">0.20</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 227.149, 272.947)\" x=\"227.149\" y=\"272.947\">0.25</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1286.2, 1590.4)\" x=\"1286.2\" y=\"1590.4\">epoch</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 57.6, 743.863)\" x=\"57.6\" y=\"743.863\">cost</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip6003)\" style=\"stroke:#009af9; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  309.737,132.704 412.523,572.534 515.309,777.106 618.095,940.764 720.881,1002.14 823.667,1089.08 926.453,1145.34 1029.24,1186.25 1132.03,1206.71 1234.81,1216.94 \n",
       "  1337.6,1268.08 1440.38,1308.99 1543.17,1324.34 1645.96,1329.45 1748.74,1355.02 1851.53,1344.79 1954.31,1365.25 2057.1,1380.59 2159.89,1390.82 2262.67,1401.05 \n",
       "  \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6003)\" style=\"stroke:#e26f46; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  309.737,86.6754 412.523,516.277 515.309,679.935 618.095,818.021 720.881,858.935 823.667,945.878 926.453,986.793 1029.24,1007.25 1132.03,1032.82 1234.81,1043.05 \n",
       "  1337.6,1037.94 1440.38,1073.74 1543.17,1078.85 1645.96,1068.62 1748.74,1099.31 1851.53,1058.39 1954.31,1073.74 2057.1,1099.31 2159.89,1114.65 2262.67,1119.76 \n",
       "  \n",
       "  \"/>\n",
       "<polygon clip-path=\"url(#clip6001)\" points=\"\n",
       "1899.61,312.204 2249.26,312.204 2249.26,130.764 1899.61,130.764 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1899.61,312.204 2249.26,312.204 2249.26,130.764 1899.61,130.764 1899.61,312.204 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#009af9; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  1923.61,191.244 2067.61,191.244 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip6001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2091.61, 208.744)\" x=\"2091.61\" y=\"208.744\">Train</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#e26f46; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  1923.61,251.724 2067.61,251.724 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip6001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2091.61, 269.224)\" x=\"2091.61\" y=\"269.224\">Dev</text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot history\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9912666666666666"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train accuracy\n",
    "Yhat = predict(X, layers);\n",
    "acc = accuracy(Y, Yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9798571428571429"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dev accuracy\n",
    "Yhat_dev = predict(X_dev, layers);\n",
    "# Accuracy\n",
    "acc_dev = accuracy(Y_dev, Yhat_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test accuracy\n",
    "Yhat_tst = predict(X_test, layers);\n",
    "# Accuracy\n",
    "acc_tst = accuracy(Y_test, Yhat_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
