{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNET\n",
    "\n",
    "This is my Jupyter notebook with notes / implementation of a neural network in Julia. All materials are based on deeplearning.ai's Deep Learning Specialization. I refer to specific videos and other materials where applicable. **The notebook is not yet finished**\n",
    "\n",
    "I created this notebook to review some of the concepts I learned in the first three courses of the deep learning specialization. Its purpose is primarily for me to process the information and produce the implementation from my own understanding rather than simply copying it from another place. As a result, you may find that the implementation notes are somewhat long-winded or tangential.\n",
    "\n",
    "If you find a mistake or think this code can be improved, it would be great if you could fork the repository and make a pull request. You could also leave a comment in the issues section.\n",
    "\n",
    "### This notebook contains\n",
    "\n",
    "- Loading & pre-processing the MNIST dataset\n",
    "- forward and backward propagation using a n-layer neural network\n",
    "- ReLU / sigmoid / 'safe' softmax activation functions\n",
    "- Inverted dropout implementation\n",
    "- Xavier / He weight initialization \n",
    "- Adam\n",
    "- Learning rate decay\n",
    "- mini-batch gradient descent\n",
    "\n",
    "### References\n",
    "\n",
    "- I refer to the deep learning specialization course materials where applicable\n",
    "- I translated some of the numpy code from Jonathan Weisberg's \"Building a Neural Network from Scratch\" [part I](https://jonathanweisberg.org/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%201/) and [part II](https://jonathanweisberg.org/post/A%20Neural%20Network%20from%20Scratch%20-%20Part%202/) to Julia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the MNIST dataset\n",
    "\n",
    "The MNIST dataset contains $60.000$ images of digits $1-10$. We use the MLDatasets module to retrieve and load the data. Normally, we would [normalize](https://www.coursera.org/learn/deep-neural-network/lecture/lXv6U/normalizing-inputs) the input data such that it has mean $0$ and standard deviation $1$ (also known as a standard normal distribution). However, this has already been done for this version of the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets\n",
    "\n",
    "# Train & test data\n",
    "train_x, train_y = MNIST.traindata();\n",
    "test_x, test_y = MNIST.testdata();\n",
    "\n",
    "# Save original data\n",
    "Xtr = train_x\n",
    "Xtest = test_x;\n",
    "\n",
    "# Note that the data have been normalized already\n",
    "# See: https://juliaml.github.io/MLDatasets.jl/latest/datasets/MNIST/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "function plot_image(X, label)\n",
    "    \n",
    "    #= \n",
    "    Plot an image\n",
    "    =#\n",
    "    \n",
    "    println(\"Label: \", label)\n",
    "    \n",
    "    # Plot\n",
    "    img = colorview(Gray, transpose(X));\n",
    "    p1 = plot(X, legend=false)\n",
    "    p2 = plot(img)\n",
    "    plot(p1,p2)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 6\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip5000\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2000\" height=\"2000\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<defs>\n",
       "  <clipPath id=\"clip5001\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip5001)\" points=\"\n",
       "0,1600 2400,1600 2400,0 0,0 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip5002\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip5001)\" points=\"\n",
       "188.156,1503.47 1141.33,1503.47 1141.33,47.2441 188.156,47.2441 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip5003\">\n",
       "    <rect x=\"188\" y=\"47\" width=\"954\" height=\"1457\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  348.351,1503.47 348.351,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  514.874,1503.47 514.874,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  681.396,1503.47 681.396,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  847.919,1503.47 847.919,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1014.44,1503.47 1014.44,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  188.156,1462.26 1141.33,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  188.156,1118.81 1141.33,1118.81 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  188.156,775.359 1141.33,775.359 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  188.156,431.909 1141.33,431.909 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  188.156,88.4582 1141.33,88.4582 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  188.156,1503.47 1141.33,1503.47 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  188.156,1503.47 188.156,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  348.351,1503.47 348.351,1481.63 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  514.874,1503.47 514.874,1481.63 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  681.396,1503.47 681.396,1481.63 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  847.919,1503.47 847.919,1481.63 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1014.44,1503.47 1014.44,1481.63 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  188.156,1462.26 202.454,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  188.156,1118.81 202.454,1118.81 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  188.156,775.359 202.454,775.359 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  188.156,431.909 202.454,431.909 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  188.156,88.4582 202.454,88.4582 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip5001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 348.351, 1557.47)\" x=\"348.351\" y=\"1557.47\">5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 514.874, 1557.47)\" x=\"514.874\" y=\"1557.47\">10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 681.396, 1557.47)\" x=\"681.396\" y=\"1557.47\">15</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 847.919, 1557.47)\" x=\"847.919\" y=\"1557.47\">20</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1014.44, 1557.47)\" x=\"1014.44\" y=\"1557.47\">25</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 164.156, 1479.76)\" x=\"164.156\" y=\"1479.76\">0.00</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 164.156, 1136.31)\" x=\"164.156\" y=\"1136.31\">0.25</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 164.156, 792.859)\" x=\"164.156\" y=\"792.859\">0.50</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 164.156, 449.409)\" x=\"164.156\" y=\"449.409\">0.75</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 164.156, 105.958)\" x=\"164.156\" y=\"105.958\">1.00</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,1462.26 514.874,1462.26 \n",
       "  548.178,1462.26 581.483,1462.26 614.787,1462.26 648.092,1462.26 681.396,1462.26 714.701,1462.26 748.005,1462.26 781.31,1462.26 814.614,1462.26 847.919,1462.26 \n",
       "  881.223,1462.26 914.528,1462.26 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,1462.26 514.874,1462.26 \n",
       "  548.178,1462.26 581.483,1462.26 614.787,1462.26 648.092,1462.26 681.396,1462.26 714.701,1462.26 748.005,1462.26 781.31,1462.26 814.614,1462.26 847.919,1462.26 \n",
       "  881.223,1462.26 914.528,1462.26 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#3da44d; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,1462.26 514.874,1462.26 \n",
       "  548.178,1462.26 581.483,1462.26 614.787,1462.26 648.092,1462.26 681.396,1462.26 714.701,1462.26 748.005,1462.26 781.31,1462.26 814.614,1462.26 847.919,1462.26 \n",
       "  881.223,1462.26 914.528,1462.26 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#c271d2; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,1462.26 514.874,1462.26 \n",
       "  548.178,1462.26 581.483,1462.26 614.787,1462.26 648.092,1462.26 681.396,1289.86 714.701,648.754 748.005,185.432 781.31,1068.98 814.614,1446.1 847.919,1462.26 \n",
       "  881.223,1462.26 914.528,1462.26 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#ac8d18; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,1462.26 514.874,1462.26 \n",
       "  548.178,1462.26 581.483,1462.26 614.787,1462.26 648.092,1451.49 681.396,551.78 714.701,93.8456 748.005,93.8456 781.31,93.8456 814.614,1381.45 847.919,1462.26 \n",
       "  881.223,1462.26 914.528,1462.26 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#00a9ad; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,1462.26 514.874,1462.26 \n",
       "  548.178,1462.26 581.483,1462.26 614.787,1413.77 648.092,551.78 681.396,93.8456 714.701,93.8456 748.005,282.407 781.31,136.945 814.614,837.315 847.919,1462.26 \n",
       "  881.223,1462.26 914.528,1462.26 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#ed5d92; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,1462.26 514.874,1462.26 \n",
       "  548.178,1462.26 581.483,1429.94 614.787,557.167 648.092,93.8456 681.396,185.432 714.701,761.891 748.005,1451.49 781.31,422.481 814.614,815.765 847.919,1462.26 \n",
       "  881.223,1462.26 914.528,1462.26 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#c68125; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,1462.26 514.874,1462.26 \n",
       "  548.178,1419.16 581.483,783.441 614.787,93.8456 648.092,93.8456 681.396,713.404 714.701,1462.26 748.005,1462.26 781.31,724.179 814.614,1376.06 847.919,1462.26 \n",
       "  881.223,1462.26 914.528,1462.26 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#00a98d; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,1462.26 514.874,1462.26 \n",
       "  548.178,901.965 581.483,88.4582 614.787,93.8456 648.092,675.691 681.396,1462.26 714.701,1462.26 748.005,1462.26 781.31,1462.26 814.614,1462.26 847.919,1462.26 \n",
       "  881.223,1462.26 914.528,1462.26 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#8e971d; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,1462.26 514.874,1300.64 \n",
       "  548.178,142.333 581.483,93.8456 614.787,654.142 648.092,1435.32 681.396,1462.26 714.701,1462.26 748.005,1462.26 781.31,1462.26 814.614,1462.26 847.919,1462.26 \n",
       "  881.223,1462.26 914.528,1462.26 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#00a8cb; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,1462.26 514.874,621.817 \n",
       "  548.178,93.8456 581.483,147.72 614.787,1295.25 648.092,1462.26 681.396,1462.26 714.701,1462.26 748.005,1462.26 781.31,1462.26 814.614,1462.26 847.919,1462.26 \n",
       "  881.223,1462.26 914.528,1462.26 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#9b7fe8; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,1079.75 514.874,110.008 \n",
       "  548.178,88.4582 581.483,1117.46 614.787,1462.26 648.092,1462.26 681.396,1462.26 714.701,1462.26 748.005,1462.26 781.31,1462.26 814.614,1462.26 847.919,1462.26 \n",
       "  881.223,1462.26 914.528,1462.26 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#608cf6; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,1009.71 514.874,93.8456 \n",
       "  548.178,169.27 581.483,1327.57 614.787,1462.26 648.092,1462.26 681.396,1462.26 714.701,1462.26 748.005,1462.26 781.31,1462.26 814.614,1462.26 847.919,1462.26 \n",
       "  881.223,1462.26 914.528,1462.26 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#f05f73; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,788.828 514.874,93.8456 \n",
       "  548.178,487.13 581.483,1171.34 614.787,368.606 648.092,368.606 681.396,368.606 714.701,368.606 748.005,918.127 781.31,1327.57 814.614,1462.26 847.919,1462.26 \n",
       "  881.223,1462.26 914.528,1462.26 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#dd64b5; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,907.352 514.874,93.8456 \n",
       "  548.178,260.857 581.483,169.27 614.787,93.8456 648.092,93.8456 681.396,93.8456 714.701,93.8456 748.005,93.8456 781.31,110.008 814.614,411.706 847.919,912.74 \n",
       "  881.223,1462.26 914.528,1462.26 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#6b9e32; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,449.418 514.874,93.8456 \n",
       "  548.178,93.8456 581.483,93.8456 614.787,93.8456 648.092,492.518 681.396,567.942 714.701,567.942 748.005,99.2331 781.31,93.8456 814.614,93.8456 847.919,368.606 \n",
       "  881.223,1214.44 914.528,1462.26 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#718c69; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,977.389 514.874,93.8456 \n",
       "  548.178,93.8456 581.483,503.293 614.787,1187.5 648.092,1446.1 681.396,1462.26 714.701,1462.26 748.005,1381.45 781.31,955.839 814.614,131.558 847.919,93.8456 \n",
       "  881.223,233.92 914.528,1462.26 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#47988b; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,1009.71 514.874,93.8456 \n",
       "  548.178,93.8456 581.483,1295.25 614.787,1462.26 648.092,1462.26 681.396,1462.26 714.701,1462.26 748.005,1462.26 781.31,1462.26 814.614,169.27 847.919,93.8456 \n",
       "  881.223,169.27 914.528,1322.19 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#1ea68c; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,1009.71 514.874,93.8456 \n",
       "  548.178,88.4582 581.483,1295.25 614.787,1462.26 648.092,1462.26 681.396,1462.26 714.701,1462.26 748.005,1440.71 781.31,497.905 814.614,99.2331 847.919,93.8456 \n",
       "  881.223,99.2331 914.528,1052.81 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#af78dd; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,1128.24 514.874,120.783 \n",
       "  548.178,93.8456 581.483,519.455 614.787,1332.96 648.092,1462.26 681.396,1462.26 714.701,1316.8 748.005,470.968 781.31,93.8456 814.614,93.8456 847.919,93.8456 \n",
       "  881.223,584.105 914.528,1462.26 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#868d87; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,1462.26 514.874,934.29 \n",
       "  548.178,131.558 581.483,93.8456 614.787,217.757 648.092,756.503 681.396,497.905 714.701,206.982 748.005,93.8456 781.31,93.8456 814.614,120.783 847.919,724.179 \n",
       "  881.223,1359.9 914.528,1462.26 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#47a065; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,1462.26 514.874,1462.26 \n",
       "  548.178,1316.8 581.483,465.58 614.787,196.207 648.092,93.8456 681.396,93.8456 714.701,93.8456 748.005,93.8456 781.31,325.506 814.614,961.227 847.919,1462.26 \n",
       "  881.223,1462.26 914.528,1462.26 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#7683af; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,1462.26 514.874,1462.26 \n",
       "  548.178,1462.26 581.483,1462.26 614.787,928.902 648.092,411.706 681.396,93.8456 714.701,93.8456 748.005,643.367 781.31,1262.92 814.614,1462.26 847.919,1462.26 \n",
       "  881.223,1462.26 914.528,1462.26 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#b08086; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,1462.26 514.874,1462.26 \n",
       "  548.178,1462.26 581.483,1462.26 614.787,1462.26 648.092,1462.26 681.396,1462.26 714.701,1462.26 748.005,1462.26 781.31,1462.26 814.614,1462.26 847.919,1462.26 \n",
       "  881.223,1462.26 914.528,1462.26 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#309ac1; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,1462.26 514.874,1462.26 \n",
       "  548.178,1462.26 581.483,1462.26 614.787,1462.26 648.092,1462.26 681.396,1462.26 714.701,1462.26 748.005,1462.26 781.31,1462.26 814.614,1462.26 847.919,1462.26 \n",
       "  881.223,1462.26 914.528,1462.26 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#788490; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,1462.26 514.874,1462.26 \n",
       "  548.178,1462.26 581.483,1462.26 614.787,1462.26 648.092,1462.26 681.396,1462.26 714.701,1462.26 748.005,1462.26 781.31,1462.26 814.614,1462.26 847.919,1462.26 \n",
       "  881.223,1462.26 914.528,1462.26 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#e561a4; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,1462.26 514.874,1462.26 \n",
       "  548.178,1462.26 581.483,1462.26 614.787,1462.26 648.092,1462.26 681.396,1462.26 714.701,1462.26 748.005,1462.26 781.31,1462.26 814.614,1462.26 847.919,1462.26 \n",
       "  881.223,1462.26 914.528,1462.26 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5003)\" style=\"stroke:#99902c; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  215.133,1462.26 248.438,1462.26 281.742,1462.26 315.047,1462.26 348.351,1462.26 381.656,1462.26 414.96,1462.26 448.265,1462.26 481.569,1462.26 514.874,1462.26 \n",
       "  548.178,1462.26 581.483,1462.26 614.787,1462.26 648.092,1462.26 681.396,1462.26 714.701,1462.26 748.005,1462.26 781.31,1462.26 814.614,1462.26 847.919,1462.26 \n",
       "  881.223,1462.26 914.528,1462.26 947.832,1462.26 981.137,1462.26 1014.44,1462.26 1047.75,1462.26 1081.05,1462.26 1114.35,1462.26 \n",
       "  \"/>\n",
       "<polygon clip-path=\"url(#clip5001)\" points=\"\n",
       "1368.08,1503.47 2321.26,1503.47 2321.26,47.2441 1368.08,47.2441 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip5004\">\n",
       "    <rect x=\"1368\" y=\"47\" width=\"954\" height=\"1457\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip5004)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1509.3,47.2441 1509.3,1503.47 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5004)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1685.81,47.2441 1685.81,1503.47 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5004)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1862.32,47.2441 1862.32,1503.47 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5004)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2038.84,47.2441 2038.84,1503.47 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5004)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2215.35,47.2441 2215.35,1503.47 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5004)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1368.08,262.982 2321.26,262.982 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5004)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1368.08,532.654 2321.26,532.654 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5004)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1368.08,802.327 2321.26,802.327 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5004)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1368.08,1072 2321.26,1072 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5004)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1368.08,1341.67 2321.26,1341.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1368.08,1503.47 2321.26,1503.47 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1368.08,47.2441 1368.08,1503.47 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1509.3,1503.47 1509.3,1481.63 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1685.81,1503.47 1685.81,1481.63 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1862.32,1503.47 1862.32,1481.63 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2038.84,1503.47 2038.84,1481.63 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2215.35,1503.47 2215.35,1481.63 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1368.08,262.982 1382.38,262.982 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1368.08,532.654 1382.38,532.654 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1368.08,802.327 1382.38,802.327 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1368.08,1072 1382.38,1072 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1368.08,1341.67 1382.38,1341.67 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip5001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1509.3, 1557.47)\" x=\"1509.3\" y=\"1557.47\">5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1685.81, 1557.47)\" x=\"1685.81\" y=\"1557.47\">10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1862.32, 1557.47)\" x=\"1862.32\" y=\"1557.47\">15</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2038.84, 1557.47)\" x=\"2038.84\" y=\"1557.47\">20</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2215.35, 1557.47)\" x=\"2215.35\" y=\"1557.47\">25</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 1344.08, 280.482)\" x=\"1344.08\" y=\"280.482\">5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 1344.08, 550.154)\" x=\"1344.08\" y=\"550.154\">10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 1344.08, 819.827)\" x=\"1344.08\" y=\"819.827\">15</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 1344.08, 1089.5)\" x=\"1344.08\" y=\"1089.5\">20</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 1344.08, 1359.17)\" x=\"1344.08\" y=\"1359.17\">25</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5004)\">\n",
       "<image width=\"953\" height=\"1456\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAA7kAAAWwCAYAAACYeIYyAAAgAElEQVR4nOzZr6sf9B7H8fdxR7eF\n",
       "TQym+TO6JAaLXYtg8UwQQRAELdq0DY2KYcHhNvwFioJFkGFYMQmLYhGDYWXqQHQLlsPYTfsyLtwg\n",
       "eu/nnOd9PP6B94tv+X6f38/WzNwYAAAACLht9QAAAAD4p4hcAAAAMkQuAAAAGSIXAACADJELAABA\n",
       "hsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIA\n",
       "AJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5\n",
       "AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAy\n",
       "RC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAA\n",
       "gAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgF\n",
       "AAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAh\n",
       "cgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAA\n",
       "ZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4A\n",
       "AAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyR\n",
       "CwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAg\n",
       "Q+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEA\n",
       "AMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhc\n",
       "AAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZ\n",
       "IhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAA\n",
       "QIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QC\n",
       "AACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAEDG9uoBAPD/4oEHHlg9YWZmXn/9\n",
       "9dUTNk6cOLH0/nPPPbf0/k0XLlxYPWHj+vXrqycA/C1ecgEAAMgQuQAAAGSIXAAAADJELgAAABki\n",
       "FwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABA\n",
       "hsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIA\n",
       "AJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5\n",
       "AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAy\n",
       "RC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADK2ZubG6hEA8N90\n",
       "22174z/dzz//fPWEmZnZ2dlZPYF/c9ddd62esHH16tXVEwD+lr3xrQ8AAAD/AJELAABAhsgFAAAg\n",
       "Q+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEA\n",
       "AMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhc\n",
       "AAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZ\n",
       "IhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAA\n",
       "QIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QC\n",
       "AACQsb16AABNhw8fXj1h4+OPP149YWZmdnZ2Vk/Yc3766ael948dO7b0/k0vvvji6gkb77zzzuoJ\n",
       "AH+Ll1wAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJE\n",
       "LgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACA\n",
       "DJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUA\n",
       "ACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFy\n",
       "AQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABk\n",
       "bK8eAMB/xx133LH0/kcffbT0/q12dnZWT5iZmd9//331hJmZOX369OoJG2+88cbS+998883S+zdt\n",
       "b/tJBvBP8ZILAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsA\n",
       "AECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPk\n",
       "AgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADI\n",
       "ELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAA\n",
       "ADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIX\n",
       "AACADJELAABAhsgFAAAgQ+QCAACQsb16AEDNoUOHVk+YmZnXXntt6f0TJ04svb8Xffjhh6snzMzM\n",
       "yZMnV0/YMy5evLh6wszMnDlzZvUEgAwvuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAA\n",
       "IEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIB\n",
       "AADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSI\n",
       "XAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAA\n",
       "GSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsA\n",
       "AECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgIytmbmxegRAyUsvvbR6\n",
       "wszMvPfee0vv37ixd75e3n333dUTZmbmlVdeWT0BAPK85AIAAJAhcgEAAMgQuQAAAGSIXAAAADJE\n",
       "LgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACA\n",
       "DJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUA\n",
       "ACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFy\n",
       "AQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABk\n",
       "iFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGRszcyN1SMA/q77\n",
       "7rtv9YSNH374YfWEmZk5fPjw0vtnzpxZev9Wr7766uoJMzOzu7u7egIA5HnJBQAAIEPkAgAAkCFy\n",
       "AQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABk\n",
       "iFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAA\n",
       "ABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJEL\n",
       "AABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD\n",
       "5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAA\n",
       "yBC5AAAAZGyvHgDwTzh58uTqCRuHDx9ePWFmZv7888+l9996662l92+1u7u7egIA8D/iJRcAAIAM\n",
       "kQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAA\n",
       "IEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIB\n",
       "AADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSI\n",
       "XAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAA\n",
       "GSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsA\n",
       "AECGyAUAACBje/UAYH974oknVk+YmZnnn39+9YSNra2t1RNmZubJJ59cev/SpUtL7wMA/5+85AIA\n",
       "AJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5\n",
       "AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAy\n",
       "RC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAA\n",
       "gAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgF\n",
       "AAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJCx\n",
       "vXoAsL8dOXJk9YSZmTlw4MDqCRtXr15dPWFmZn788cfVEwAA/ue85AIAAJAhcgEAAMgQuQAAAGSI\n",
       "XAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAA\n",
       "GSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsA\n",
       "AECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPk\n",
       "AgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADI\n",
       "ELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGRsrx4A\n",
       "7G8PPfTQ6gl7zrfffrt6wszM3HvvvUvvf/XVV0vvsz+cO3du6f3z588vvX/Tzz//vHoCQIaXXAAA\n",
       "ADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIX\n",
       "AACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECG\n",
       "yAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAA\n",
       "kCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkA\n",
       "AABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJE\n",
       "LgAAABkiFwAAgIzt1QOA/e3KlSurJ+w5x48fXz1hZmYuXLiw9P7Ro0eX3md/OHv27NL7169fX3r/\n",
       "posXL66esPHJJ5+snjAzM+fOnVs9AdinvOQCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcA\n",
       "AIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbI\n",
       "BQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQ\n",
       "IXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAA\n",
       "AGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQu\n",
       "AAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAytlcPAPa3nZ2d1RP2\n",
       "nPvvv3/1BP7N119/vXrCzMxcu3Zt9YQ955lnnll6/8CBA0vv3/TYY4+tnrBx6NCh1RNmZubixYur\n",
       "J8zMzPfff796AvAXeckFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsA\n",
       "AECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPk\n",
       "AgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADI\n",
       "ELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAA\n",
       "ADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIX\n",
       "AACADJELAABAhsgFAAAgQ+QCAACQIXIBAADI2JqZG6tHAPvXs88+u3rCzMx8+umnqyfsOefPn196\n",
       "/80331x6/1bffffd6gkzM3P9+vXVE/acO++8c+n9p556aun9m86ePbt6wsbBgwdXT5iZmcuXL6+e\n",
       "MDMz99xzz+oJwF/kJRcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAA\n",
       "ABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJEL\n",
       "AABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD\n",
       "5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAA\n",
       "yBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwA\n",
       "AAAyRC4AAAAZIhcAAICMrZm5sXoEsH89/fTTqyfMzMwXX3yxesKe8+CDDy69f+nSpaX3YT/5448/\n",
       "Vk/YOHr06OoJMzNz7dq11RNmZub48eOrJ2xcvnx59QTYF7zkAgAAkCFyAQAAyBC5AAAAZIhcAAAA\n",
       "MkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcA\n",
       "AIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbI\n",
       "BQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQ\n",
       "IXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAA\n",
       "AGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZGyvHgBQs7W1\n",
       "tXoC7Bvb22t/inz55ZdL79905MiR1RP2nL3ymTzyyCOrJ2xcvnx59QTYF7zkAgAAkCFyAQAAyBC5\n",
       "AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAy\n",
       "RC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAA\n",
       "gAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgF\n",
       "AAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAh\n",
       "cgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAA\n",
       "ZIhcAAAAMrZXDwD2tytXrqyeMDMzu7u7qyds3H777asnzMzMqVOnlt5/+eWXl96/1S+//LJ6wp5y\n",
       "7Nix1RM2Pvjgg6X3H3/88aX3+c8+++yz1RNmZub8+fOrJwB/kZdcAAAAMkQuAAAAGSIXAACADJEL\n",
       "AABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD\n",
       "5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAA\n",
       "yBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwA\n",
       "AAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABki\n",
       "FwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACAjK2ZubF6\n",
       "BMDf9cILL6yesHH69OnVE2Zm5uDBg0vv//rrr0vv3+r9999fPWFmZh5++OHVE2Zm5tFHH109YePu\n",
       "u+9ePWFP2N3dXT1h49SpU6snzMzM22+/vXrCzMz89ttvqycA/2rPjk0TAKMwij5nsHQLW5cR3EZw\n",
       "AQsbZ7JyGFNF0oYEfrmcs8D72sv7JZ9cAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQ\n",
       "IXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAA\n",
       "AGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQu\n",
       "AAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAM\n",
       "kQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAA\n",
       "IEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACAjM3MvFaPACjZ7XarJ8zMzO12W3p/\n",
       "v98vvf/TdrtdPYEP9Xg8Vk+YmZnz+bx6wtv9fl89AeBPfHIBAADIELkAAABkiFwAAAAyRC4AAAAZ\n",
       "IhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAA\n",
       "QIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QC\n",
       "AACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQ\n",
       "uQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAA\n",
       "MkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZm5l5\n",
       "rR4BQM/pdFo94e14PK6eMDMzh8Nh9YSPc71el96/XC5L7397Pp+rJwBk+OQCAACQIXIBAADIELkA\n",
       "AABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJE\n",
       "LgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACA\n",
       "DJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUA\n",
       "ACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADrVnqMAAAZVSURBVDJELgAAABkiFwAAgAyRCwAAQIbI\n",
       "BQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgYzMzr9Uj\n",
       "AAAA4D/45AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAA\n",
       "kCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkA\n",
       "AABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJE\n",
       "LgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACA\n",
       "DJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUA\n",
       "ACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFy\n",
       "AQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABk\n",
       "iFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAA\n",
       "ABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJEL\n",
       "AABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD\n",
       "5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAA\n",
       "yBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwA\n",
       "AAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABki\n",
       "FwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABA\n",
       "hsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIA\n",
       "AJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5\n",
       "AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAy\n",
       "RC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAA\n",
       "gAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgF\n",
       "AAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAh\n",
       "cgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAA\n",
       "ZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4A\n",
       "AAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEAAMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyR\n",
       "CwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhcAAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAg\n",
       "Q+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZIhcAAIAMkQsAAECGyAUAACBD5AIAAJAhcgEA\n",
       "AMgQuQAAAGSIXAAAADJELgAAABkiFwAAgAyRCwAAQIbIBQAAIEPkAgAAkCFyAQAAyBC5AAAAZIhc\n",
       "AAAAMkQuAAAAGSIXAACADJELAABAhsgFAAAgQ+QCAACQIXIBAADIELkAAABkiFwAAAAyRC4AAAAZ\n",
       "X0k67qklmTD0AAAAAElFTkSuQmCC\n",
       "\" transform=\"translate(1368, 47)\"/>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 107\n",
    "plot_image(Xtr[:,:,i], train_y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shapes of the data are as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 60000)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "## Get shapes\n",
    "println(size(train_x))\n",
    "println(size(train_y))\n",
    "trainsize = size(train_x)[3];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I find it easier to think about the data if the examples are in the row dimension. We will reshape the data such that $X$ is a tensor of shape $(60.000, 784)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape x\n",
    "train_x = transpose(reshape(train_x, 28 * 28, size(train_x)[3]));\n",
    "test_x = transpose(reshape(test_x, 28 * 28, size(test_x)[3]));\n",
    "\n",
    "# Reshape Y\n",
    "train_y = reshape(train_y, size(train_y)[1], 1);\n",
    "test_y = reshape(test_y, size(test_y)[1], 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "function normalize_data(X)\n",
    "    \n",
    "    X_tmp = zeros(size(X))\n",
    "    \n",
    "    for i = 1:size(X)[2]\n",
    "        \n",
    "        X_tmp[:,i] = (X[:,i] .- mean(X[:,i])) ./ sqrt(var(X[:,i]))\n",
    "        \n",
    "        end;\n",
    "    \n",
    "    return(X)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = normalize_data(train_x);\n",
    "test_x = normalize_data(test_x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Y$ has $10$ unique labels ranging from $0-9$. However, this is actually a recoding of the data. MORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1)"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Int64,1}:\n",
       " 5\n",
       " 0\n",
       " 4\n",
       " 1\n",
       " 9\n",
       " 2\n",
       " 3\n",
       " 6\n",
       " 7\n",
       " 8"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data until now\n",
    "\n",
    "- $X$ is a tensor with dimension $(60.000, 784)$\n",
    "- $Y$ is a tensor with dimension $(60.000, 1)$\n",
    "\n",
    "<img src=\"img/XandY2.png\" width = \"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a one-hot encoding for $Y$\n",
    "\n",
    "In order to train the neural network, we need to one-hot-encode <link> the outcome variable $Y$.\n",
    "\n",
    "The trick to doing this is as follows:\n",
    "\n",
    "- Create a $k \\times k$ identity matrix. In our case, we have $k=10$, so:\n",
    "\n",
    "$$\n",
    "I = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & \\dots & 0 \\\\\n",
    "0 & 1 & \\dots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & 0 \\\\\n",
    "0 & 0 & \\dots & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- For each example in $Y$, *index the column of the value of the label*. For example, if the the label equals '5', we index the $5^{th}$ column, which has a $1$ on the $5^{th}$ element and zeroes elsewhere\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Because we do this $n$ times (the number of training examples), we will end up with a tensor of size $(10, 60.000)$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & \\dots & 1 \\\\\n",
    "1 & 1 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 1 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 0 & \\dots & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- Again, I like to think in terms of (examples, dimensions) so we will transpose the one-hot-encoded matrix such that it has the dimensions $(60.000, 10)$\n",
    "\n",
    "<img src=\"img/y_one_hot.png\" width = \"300px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for y\n",
    "# We need a shape for outcome variable of size [num_labels, num_examples]\n",
    "\n",
    "# Number of distinct classes\n",
    "k = length(unique(train_y));\n",
    "# Number of training examples\n",
    "n = size(train_y)[1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "function one_hot_encode(y_labels)\n",
    "    \n",
    "    #=\n",
    "    From multiclass labels to one-hot encoding\n",
    "    \n",
    "    :param y_labels: column vector of shape (number of examples, 1) containing k > 2 classes\n",
    "    \n",
    "    :return: one-hot encoded labels\n",
    "    =#\n",
    "    \n",
    "    # Number of distinct classes\n",
    "    k = length(unique(y_labels));\n",
    "    # Number of rows\n",
    "    n = size(y_labels)[1];\n",
    "    \n",
    "    # 1. Create k x k identity matrix \n",
    "\n",
    "    # Arrange the identity matrix such that, for each class, we get a one-hot encoded vector.\n",
    "    # This means that the rows are of length k (the number of distinct classes)\n",
    "    # The columns are of length n (the number of examples).\n",
    "    y_ohe = Matrix{Float64}(I, k, k)\n",
    "\n",
    "    # 2. Organize such that we get a k x m matrix. We do this by letting the label index\n",
    "    #     the column value. Since we have m labels, we index the columns m times.\n",
    "    #     So for m = 1.000 where m_1000 = 3, we index the kth column [0,0,1,0,...,k]\n",
    "    \n",
    "    # We have to add +1 to the classes (because Julia indexes from 1)\n",
    "    # Unlike e.g. numpy we need to explicitly call 'broadcast' to match shapes between\n",
    "    # two elements that we're adding\n",
    "    y_ohe = y_ohe[:, broadcast(+, y_labels, 1)][:,:];\n",
    "    \n",
    "    # Return\n",
    "    return(y_ohe)\n",
    "    \n",
    "    end;\n",
    "\n",
    "# One-hot encoding\n",
    "y_train_ohe = transpose(one_hot_encode(train_y));\n",
    "y_test_ohe = transpose(one_hot_encode(test_y));\n",
    "\n",
    "println(size(y_train_ohe))\n",
    "println(size(y_test_ohe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just to make sure that the one-hot encoding was done properly, we can randomly sample a number of values and check if the one-hot encoded examples line up with the original labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random\n",
    "\n",
    "function sanity(y_ohe, train_y; n_sample = 5)\n",
    "    \n",
    "    #=\n",
    "    Ensure that one-hot encoded labels are encoded properly by transforming them back into\n",
    "     labels.\n",
    "    \n",
    "    :param y_ohe: one-hot encoded training labels\n",
    "    :param train_y: array of size (examples, 1) containing the labels\n",
    "    :param n_sample: number of examples to sample randomly\n",
    "    \n",
    "    :return: This function does not return a value\n",
    "    =#\n",
    "    \n",
    "    # Shapes\n",
    "    n, k = size(y_ohe)\n",
    "    \n",
    "    # Pick a random example\n",
    "    ind_shuffled = shuffle(1:n)\n",
    "    \n",
    "    # Subset\n",
    "    ind = ind_shuffled[1:n_sample]\n",
    "    \n",
    "    # For each, print OHE + convert back to class\n",
    "    for i in ind\n",
    "        \n",
    "        # Find position of 1\n",
    "        pos = findall(x -> x==1, y_ohe[i, :])\n",
    "        \n",
    "        # Subtract 1\n",
    "        pos = pos[1] - 1\n",
    "        \n",
    "        # Print\n",
    "        println(\"Example \", i, \":\\n\", \n",
    "                \"\\t[OHE position: \", pos, \"] ==> [label: \", train_y[i,1], \"]\")\n",
    "        \n",
    "    end\n",
    "    \n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 26532:\n",
      "\t[OHE position: 1] ==> [label: 1]\n",
      "Example 26997:\n",
      "\t[OHE position: 8] ==> [label: 8]\n",
      "Example 46392:\n",
      "\t[OHE position: 0] ==> [label: 0]\n",
      "Example 14515:\n",
      "\t[OHE position: 3] ==> [label: 3]\n",
      "Example 46501:\n",
      "\t[OHE position: 3] ==> [label: 3]\n"
     ]
    }
   ],
   "source": [
    "sanity(y_train_ohe, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNet functions\n",
    "\n",
    "Plan: build a neural net with one hidden layer and $n_h$ hidden units\n",
    "\n",
    "TODO:\n",
    "\n",
    "- Notation\n",
    "- Matrix calculus (see https://atmos.washington.edu/~dennis/MatrixCalculus.pdf)\n",
    "- Images of the neural network structure\n",
    "- Backprop computation graph image + derivatives\n",
    "    * Also when using Tanh\n",
    "- References\n",
    "    * Chollet Keras\n",
    "    * Coursera course\n",
    "- 'safe' softmax function\n",
    "- dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "function cross_validation(X, Y, split_prop = 0.05)\n",
    "    \n",
    "    #=\n",
    "    Create train/test split\n",
    "    \n",
    "    :param X: Input data X\n",
    "    :param Y: output data Y\n",
    "    :param split_prop: percentage of data to use as validation set\n",
    "    \n",
    "    :return: dictionary with train & validation data\n",
    "    :seealso: https://www.coursera.org/learn/deep-neural-network/lecture/cxG1s/train-dev-test-sets\n",
    "    =#\n",
    "    \n",
    "    # Number of training examples\n",
    "    trainsize = size(X)[1]\n",
    "    \n",
    "    # Number of validation examples\n",
    "    ndev = Integer(floor(trainsize * split_prop))\n",
    "    \n",
    "    # Shuffle indices\n",
    "    ind = shuffle(1:trainsize);\n",
    "    # Rearrange train x and y\n",
    "    X = X[ind, :];\n",
    "    Y = Y[ind, :];\n",
    "\n",
    "    # Validation split\n",
    "    dev_x = X[1:ndev, :];\n",
    "    dev_y = Y[1:ndev, :];\n",
    "\n",
    "    # Remove from train\n",
    "    train_x = X[ndev+1:end, :];\n",
    "    train_y = Y[ndev+1:end, :];\n",
    "    \n",
    "    # To dict\n",
    "    split = Dict(\n",
    "        \"X_train\" => train_x,\n",
    "        \"Y_train\" => train_y,\n",
    "        \"X_dev\" => dev_x,\n",
    "        \"Y_dev\" => dev_y\n",
    "    )\n",
    "    \n",
    "    # Return\n",
    "    return(split)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight initialization\n",
    "\n",
    "$$\n",
    "W^{[l]} = (n^{[l]},n^{[l-1]}) \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "function initialize_layer(n_h_current, n_h_last, activation, layer; drop_chance = 0.2, last=false)\n",
    "    \n",
    "    #=\n",
    "    Initialize a layer\n",
    "    =#\n",
    "    \n",
    "    # Weights\n",
    "    W = randn(n_h_current, n_h_last)\n",
    "    b = zeros(n_h_current, 1)\n",
    "\n",
    "    # Weight initialization\n",
    "    if activation == \"ReLU\"\n",
    "\n",
    "        W = W .* sqrt(2 / n_h_last)\n",
    "\n",
    "    else\n",
    "\n",
    "        W = W .* sqrt(1 / n_h_last)\n",
    "\n",
    "        end;\n",
    "    \n",
    "    # Initialization for Adam / RMSprop\n",
    "    vdW = zeros(n_h_current, n_h_last)\n",
    "    vdb = zeros(n_h_current, 1)\n",
    "    \n",
    "    sdW = zeros(n_h_current, n_h_last)\n",
    "    sdb = zeros(n_h_current, 1)\n",
    "    \n",
    "    # To dict\n",
    "    current_layer = Dict(\n",
    "        \"layer\" => layer,\n",
    "        \"last_layer\" => last,\n",
    "        \"activation\" => activation,\n",
    "        \"hidden_current\" => n_h_current,\n",
    "        \"hidden_previous\" => n_h_last,\n",
    "        \"W\" => W,\n",
    "        \"b\" => b,\n",
    "        \"vdW\" => vdW,\n",
    "        \"vdb\" => vdb,\n",
    "        \"sdW\" => sdW,\n",
    "        \"sdb\" => sdb\n",
    "    )\n",
    "    \n",
    "    # If last_layer == false, add drop chance\n",
    "    if last == false\n",
    "        \n",
    "        current_layer[\"keep_prob\"] = 1 - drop_chance\n",
    "        \n",
    "        end;\n",
    "    \n",
    "    # Return\n",
    "    return(current_layer)\n",
    "    \n",
    "    end;\n",
    "\n",
    "function initialize_parameters(n_x, n_h, n_y, activations, layers, dropout)\n",
    "    \n",
    "    #=\n",
    "    Initialize the weight matrices and bias vectors\n",
    "    \n",
    "    :param n_x: number of observations in the input data\n",
    "    :param n_h1: number of hidden units in the first layer\n",
    "    :param n_h2: number of hidden units in the second layer\n",
    "    :param n_y: number of labels in the output data\n",
    "    \n",
    "    :return:\n",
    "       - parameters: dict containing weights W and bias vectors b for each layer\n",
    "       - cache:      dict containing the moving average of the gradient (first moment) and moving average of the second gradient (second moment)\n",
    "    \n",
    "    :seealso: \n",
    "       - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/XtFPI/random-initialization\n",
    "       - https://www.coursera.org/learn/deep-neural-network/lecture/C9iQO/vanishing-exploding-gradients\n",
    "       - https://www.coursera.org/learn/deep-neural-network/lecture/RwqYe/weight-initialization-for-deep-networks\n",
    "       - https://stats.stackexchange.com/questions/47590/what-are-good-initial-weights-in-a-neural-network\n",
    "       - https://medium.com/usf-msds/deep-learning-best-practices-1-weight-initialization-14e5c0295b94\n",
    "    =#\n",
    "    \n",
    "    # For sigmoid / tanh / softmax --> Xavier weight initialization --> 1 / sqrt(units in previous layer)\n",
    "    # For ReLU weight initialization --> 2 / sqrt(units in previous layer)\n",
    "    \n",
    "    # List to hold results\n",
    "    layer_details = []\n",
    "    \n",
    "    # Add 1 to layers (last layer is the softmax layer)\n",
    "    layers = layers + 1\n",
    "    \n",
    "    # For each layer, initialize\n",
    "    for layer = 1:layers\n",
    "        \n",
    "        # If first layer, then pass the value of n_x\n",
    "        if layer == 1\n",
    "            \n",
    "            # Expect a tuple if layers > 2\n",
    "            current_layer = initialize_layer(ifelse(layers <= 2, \n",
    "                                                    n_h, \n",
    "                                                    n_h[1]), \n",
    "                                             n_x, \n",
    "                                             ifelse(layers <= 2, \n",
    "                                                    activations, \n",
    "                                                    activations[layer]), \n",
    "                                             layer,\n",
    "                                             drop_chance = ifelse(layers <= 2, \n",
    "                                                            dropout, \n",
    "                                                            dropout[layer]))\n",
    "          \n",
    "        # If last layer, then make it a softmax and pass n_y\n",
    "        elseif(layer == layers)\n",
    "            \n",
    "            current_layer = initialize_layer(n_y, \n",
    "                                             ifelse(layers <= 2,\n",
    "                                                    n_h,\n",
    "                                                    n_h[layer-1]), \n",
    "                                             \"softmax\",\n",
    "                                             layer, \n",
    "                                             last=true)\n",
    "        \n",
    "        # Else, we have the hidden layers in between input X and output Y\n",
    "        else\n",
    "            \n",
    "            current_layer = initialize_layer(n_h[layer], n_h[layer-1], activations[layer], layer,\n",
    "                                             drop_chance = dropout[layer])\n",
    "            \n",
    "            end;\n",
    "        \n",
    "        # Push layer\n",
    "        push!(layer_details, current_layer)\n",
    "        \n",
    "        end;\n",
    "    \n",
    "    # Return\n",
    "    return(layer_details)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple layers\n",
    "layers_multiple = initialize_parameters(784, (256, 128), 10, (\"ReLU\", \"ReLU\"), 2, (0.2, 0.4));\n",
    "# Single layer\n",
    "layers_single = initialize_parameters(784, 256, 10, \"ReLU\", 1, 0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Any} with 12 entries:\n",
       "  \"W\"               => [-0.0622553 0.0894943 … 0.011838 0.0137901; 0.0470461 -0…\n",
       "  \"b\"               => [0.0; 0.0; … ; 0.0; 0.0]\n",
       "  \"keep_prob\"       => 0.6\n",
       "  \"vdW\"             => [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0…\n",
       "  \"sdb\"             => [0.0; 0.0; … ; 0.0; 0.0]\n",
       "  \"hidden_current\"  => 128\n",
       "  \"layer\"           => 2\n",
       "  \"activation\"      => \"ReLU\"\n",
       "  \"sdW\"             => [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0…\n",
       "  \"hidden_previous\" => 256\n",
       "  \"last_layer\"      => false\n",
       "  \"vdb\"             => [0.0; 0.0; … ; 0.0; 0.0]"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_multiple[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Any} with 12 entries:\n",
       "  \"W\"               => [-0.0224387 0.0336928 … 0.00509727 -0.0330679; 0.104484 …\n",
       "  \"b\"               => [0.0; 0.0; … ; 0.0; 0.0]\n",
       "  \"keep_prob\"       => 0.5\n",
       "  \"vdW\"             => [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0…\n",
       "  \"sdb\"             => [0.0; 0.0; … ; 0.0; 0.0]\n",
       "  \"hidden_current\"  => 256\n",
       "  \"layer\"           => 1\n",
       "  \"activation\"      => \"ReLU\"\n",
       "  \"sdW\"             => [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0…\n",
       "  \"hidden_previous\" => 784\n",
       "  \"last_layer\"      => false\n",
       "  \"vdb\"             => [0.0; 0.0; … ; 0.0; 0.0]"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_single[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch gradient descent\n",
    "\n",
    "An algorithm for splitting the data into batches is given below. Here:\n",
    "\n",
    "- $n$ is the number of training examples\n",
    "- $k$ is the batch size\n",
    "- $j$ is the number of total batches that can be created **using the batch size $k$**\n",
    "\n",
    "That is, $j$ is the integer part of the division of $\\frac{n}{k}$.\n",
    "\n",
    "<img src=\"img/minibatch.png\" width = \"500px\">\n",
    "\n",
    "This toy example is given as an algorithm below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch 1\n",
      "\t[Begin 1] ==> [end 10]\n",
      "Minibatch 2\n",
      "\t[Begin 11] ==> [end 20]\n",
      "Minibatch 3\n",
      "\t[Begin 21] ==> [end 30]\n",
      "Minibatch 4\n",
      "\t[Begin 31] ==> [end 40]\n",
      "Minibatch 5\n",
      "\t[Begin 41] ==> [end 50]\n",
      "Minibatch 6\n",
      "\t[Begin 51] ==> [end 60]\n",
      "Minibatch 7\n",
      "\t[Begin 61] ==> [end 70]\n",
      "Minibatch 8\n",
      "\t[Begin 71] ==> [end 80]\n",
      "Minibatch 9\n",
      "\t[Begin 81] ==> [end 90]\n",
      "Minibatch 10\n",
      "\t[Begin 91] ==> [end 100]\n",
      "Minibatch 11\n",
      "\t[Begin 101] ==> [end 101]\n"
     ]
    }
   ],
   "source": [
    "# Number of examples\n",
    "n = 101;\n",
    "# Batch size\n",
    "k = 10;\n",
    "# Number of batches of size k (discounting any remainders)\n",
    "j = floor(n/k);\n",
    "\n",
    "# For each minibatch\n",
    "for i = 1:j\n",
    "    \n",
    "    # Start at 1 if minibatch is 1\n",
    "    if i == 1\n",
    "        index_begin = 1\n",
    "    else\n",
    "        # Else start at the end of the previous minibatch plus one\n",
    "        index_begin = Integer(((i - 1) * k) + 1)\n",
    "        end;\n",
    "    \n",
    "    # End \n",
    "    index_end = Integer((i * k))\n",
    "    \n",
    "    println(\"Minibatch \", Integer(i), \"\\n\\t[Begin \", index_begin, \"] ==> [end \", index_end, \"]\")\n",
    "    \n",
    "    end;\n",
    "\n",
    "# If there is a remainder\n",
    "if n - (j * k) > 0\n",
    "    \n",
    "    index_begin = Integer((j * k) + 1)\n",
    "    index_end = n\n",
    "    \n",
    "    println(\"Minibatch \", Integer(j+1), \"\\n\\t[Begin \", index_begin, \"] ==> [end \", index_end, \"]\")\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below implements mini-batches for real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "function create_batches(X, Y, batch_size = 128)\n",
    "    \n",
    "    #=\n",
    "    Creates b batches of size batch_size\n",
    "    \n",
    "    :param X:          input data X\n",
    "    :param Y:          output data Y\n",
    "    :param batch_size: size of each batch\n",
    "    \n",
    "    :return: list containing m batches of length batch_size and possibly 1 batch of size batch_size_remainder < batch_size\n",
    "    :seealso: \n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/qcogH/mini-batch-gradient-descent\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/lBXu8/understanding-mini-batch-gradient-descent\n",
    "    =#\n",
    "    \n",
    "    # Shuffle training examples randomly\n",
    "    n = size(X)[1]\n",
    "    ind = shuffle(1:n)\n",
    "    \n",
    "    # Rearrange data in X and Y\n",
    "    X = X[ind, :]\n",
    "    Y = Y[ind, :]\n",
    "    \n",
    "    # List to store minibatches\n",
    "    mbatches = []\n",
    "    \n",
    "    # Number of complete training examples. \n",
    "    #  This means: n / 128 leaves a remainder (most likely)\n",
    "    #  Therefore, there will be (b - 1) batches with size batch_size\n",
    "    #  and 1 batch with size last_batch_size < batch_size\n",
    "    b_first = Integer(floor(n / batch_size))\n",
    "    \n",
    "    # First, loop through the (b_first - 1) examples\n",
    "    #  We need to loop through b_first - 1 because we construct \n",
    "    #    @ index_begin => i * batch_size (e.g. 9 * 128 = 1152)\n",
    "    #    @ index_end => (i + 1) * batch_size (e.g. 10 * 128 = 1280)\n",
    "    #  index_end needs (i + 1). If i == k where k is the last possible index, we cannot subset index_end\n",
    "    #  because it would require (k + 1) indices.\n",
    "    for i = 1:b_first\n",
    "        \n",
    "        # Beginning and end indices\n",
    "        if i == 1\n",
    "            index_begin = 1\n",
    "        else\n",
    "            index_begin = Integer(((i - 1) * batch_size) + 1)\n",
    "            end;\n",
    "        \n",
    "        index_end = Integer(i * (batch_size))\n",
    "        \n",
    "        X_current_batch = X[index_begin:index_end, :]\n",
    "        Y_current_batch = Y[index_begin:index_end, :]\n",
    "\n",
    "        # Add to array of minibatches\n",
    "        push!(mbatches, [X_current_batch, Y_current_batch])\n",
    "        \n",
    "        end;\n",
    "    \n",
    "    # Then, if necessary, make a batch for the remainder\n",
    "    b_rem = n - (b_first * batch_size)\n",
    "    if b_rem != 0\n",
    "        \n",
    "        # Subset X & Y\n",
    "        index_begin = Integer(((b_first) * batch_size) + 1) # i+1 is from the for-loop above (i.e. the last 'full' minibatch of size batch_size)\n",
    "        index_end = n \n",
    "        \n",
    "        X_current_batch = X[index_begin:index_end, :]\n",
    "        Y_current_batch = Y[index_begin:index_end, :]\n",
    "        \n",
    "        # Append\n",
    "        push!(mbatches, [X_current_batch, Y_current_batch])\n",
    "        \n",
    "        end;\n",
    "    \n",
    "    # Return mini batches\n",
    "    return(mbatches)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions and Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "function softmax(z)\n",
    "    \n",
    "    #=\n",
    "    Softmax function\n",
    "    \n",
    "    :param z: result of the linear transformation of the final layer\n",
    "    :return: activations for z\n",
    "    \n",
    "    :seealso:\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/4dDC1/activation-functions\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/OASKH/why-do-you-need-non-linear-activation-functions\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/HRy7y/softmax-regression\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/LCsCH/training-a-softmax-classifier\n",
    "    =#\n",
    "    \n",
    "    # Subtract the max of z to avoid exp(z) getting too large\n",
    "    z = z .- maximum(z)\n",
    "    \n",
    "    # Softmax & return\n",
    "    return(exp.(z) ./ sum(exp.(z),dims=1))\n",
    "    \n",
    "    end;\n",
    "\n",
    "function sigmoid(z)\n",
    "    \n",
    "    #=\n",
    "    Sigmoid function\n",
    "    \n",
    "    :param z: result of the linear transformation for layer l\n",
    "    :return: activations for z\n",
    "    \n",
    "    :seealso:\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/4dDC1/activation-functions\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/OASKH/why-do-you-need-non-linear-activation-functions\n",
    "    =#\n",
    "    \n",
    "    1 ./ (1 .+ exp.(-z))\n",
    "    \n",
    "    end;\n",
    "\n",
    "function ReLU(z)\n",
    "    \n",
    "    #=\n",
    "    ReLU implementation\n",
    "    \n",
    "    :param z: result of the linear transformation for layer l\n",
    "    :return: activations for z\n",
    "    \n",
    "    :seealso:\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/4dDC1/activation-functions\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/OASKH/why-do-you-need-non-linear-activation-functions\n",
    "    =#\n",
    "    \n",
    "    ((z .> 0) * 1) .* z\n",
    "    \n",
    "    end;\n",
    "\n",
    "function crossentropy_cost(y, yhat)\n",
    "    \n",
    "    #=\n",
    "    Crossentropy cost function for m classes\n",
    "    \n",
    "    :param y: actual (one-hot encoded) values for y\n",
    "    :param yhat: predicted (one-hot encoded) values for yhat\n",
    "    \n",
    "    :return: loss value\n",
    "    \n",
    "    :seealso: https://www.coursera.org/learn/neural-networks-deep-learning/lecture/yWaRd/logistic-regression-cost-function\n",
    "    =#\n",
    "    \n",
    "    loss = sum(transpose(y) .* log.(yhat))\n",
    "    n = size(y)[1]\n",
    "    \n",
    "    return(-(1/n) * loss)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorized dropout\n",
    "function dropout(X, keep_prob = 0.8)\n",
    "    \n",
    "    #=\n",
    "    Dropout implementation. Randomly sets the weights of some hidden units to 0\n",
    "    \n",
    "    :param X: input data\n",
    "    :param keep_prob: probability of keeping a hidden unit\n",
    "    \n",
    "    :return: dropout matrix containing 1 for each unit that should be kept and 0 for each unit that should be \n",
    "              dropped. also returns X in which some units are dropped and the remaining units are scaled by \n",
    "              drop_chance.\n",
    "    \n",
    "    :seealso:\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/eM33A/dropout-regularization\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/YaGbR/understanding-dropout\n",
    "    =#\n",
    "    \n",
    "    n = size(X)[1]\n",
    "    k = size(X)[2]\n",
    "    \n",
    "    # Uniformly distributed probabilities\n",
    "    a = rand(n, k)\n",
    "    \n",
    "    # Evaluate against dropout probability\n",
    "    #  run the mask and turn boolean into integer\n",
    "    D = (a .< keep_prob) * 1\n",
    "\n",
    "    # Element-wise multiplication\n",
    "    X = D .* X\n",
    "\n",
    "    # Scale\n",
    "    X = X ./ keep_prob\n",
    "    \n",
    "    # Return\n",
    "    return((D, X))\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "\n",
    "<img src=\"img/dims.png\" width = \"300px\">\n",
    "\n",
    "<img src=\"img/forwardprop.png\" width = \"800px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "function forward_prop(layers, X)\n",
    "    \n",
    "    #=\n",
    "    Forward propagation\n",
    "    \n",
    "    :param parameters:  dict containing weights and bias vectors for each layer\n",
    "    :param cache:       dict containing computations, dropout vectors and exponentially weighted gradients\n",
    "    :param X:           input data\n",
    "    :param keep_prob:   probability of keeping a hidden unit\n",
    "    \n",
    "    :return: updated cache with dropout vectors and the computations for each forward pass for the current \n",
    "              minibatch\n",
    "    \n",
    "    :seealso:\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/4WdOY/computation-graph\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/tyAGh/computing-a-neural-networks-output\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/MijzH/forward-propagation-in-a-deep-network\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/Rz47X/getting-your-matrix-dimensions-right\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/znwiG/forward-and-backward-propagation\n",
    "    =#\n",
    "    \n",
    "    # For each layer, compute forward propagation step\n",
    "    for layer in layers\n",
    "        \n",
    "        # Layer number\n",
    "        layer_number = layer[\"layer\"]\n",
    "        # Last layer?\n",
    "        last_layer = layer[\"last_layer\"]\n",
    "        # Activation function\n",
    "        activation = layer[\"activation\"]\n",
    "        \n",
    "        ## Compute the linear transformation Z\n",
    "        \n",
    "        # If first layer, then use X (also known as A_0)\n",
    "        # Else: use A_{layer_number - 1}\n",
    "        \n",
    "        if layer_number == 1\n",
    "                        \n",
    "            # Multiply weights times X\n",
    "            layer[\"Z\"] = broadcast(+, layer[\"W\"] * transpose(X), layer[\"b\"], 1)\n",
    "            \n",
    "        else\n",
    "            \n",
    "            # Multiply weights times the activation of the previous layer\n",
    "            layer[\"Z\"] = broadcast(+, layer[\"W\"] * layers[layer_number - 1][\"A\"], layer[\"b\"], 1)\n",
    "            \n",
    "            end;\n",
    "        \n",
    "        ## Activation functions over Z ==> raise error if unknown\n",
    "        \n",
    "        if activation == \"ReLU\"\n",
    "            \n",
    "            A = ReLU(layer[\"Z\"])\n",
    "            \n",
    "        elseif activation == \"sigmoid\"\n",
    "            \n",
    "            A = sigmoid(layer[\"Z\"])\n",
    "            \n",
    "        elseif activation == \"tanh\"\n",
    "            \n",
    "            A = tanh.(layer[\"Z\"])\n",
    "            \n",
    "        elseif activation == \"softmax\"\n",
    "            \n",
    "            A = softmax(layer[\"Z\"])\n",
    "            \n",
    "        else\n",
    "            \n",
    "            # Raise error\n",
    "            error(string(\"Don't know how to handle activation function '\", activation, \"'\"))\n",
    "        \n",
    "            end;\n",
    "        \n",
    "        ## If last layer, then simply store the activations\n",
    "\n",
    "        if last_layer\n",
    "            \n",
    "            layer[\"A\"] = A\n",
    "\n",
    "        ## Else, we apply dropout to the layer\n",
    "        else\n",
    "            \n",
    "            D, A = dropout(A, layer[\"keep_prob\"])\n",
    "            \n",
    "            # Store the data in the layer\n",
    "            layer[\"D\"] = D\n",
    "            layer[\"A\"] = A\n",
    "            \n",
    "            end;\n",
    "        \n",
    "        end;\n",
    "    \n",
    "    # Return layers\n",
    "    return(layers)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One step of forward propagation\n",
    "X_tst = train_x[1:1000,:]\n",
    "layers = forward_prop(layers_multiple, X_tst);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Any} with 13 entries:\n",
       "  \"Z\"               => [1.6216 0.383201 … -0.970097 2.94524; 5.212 4.72808 … 2.…\n",
       "  \"W\"               => [-0.0735648 -0.0902234 … -0.104103 -0.0375596; 0.206812 …\n",
       "  \"b\"               => [0.0; 0.0; … ; 0.0; 0.0]\n",
       "  \"vdW\"             => [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0…\n",
       "  \"sdb\"             => [0.0; 0.0; … ; 0.0; 0.0]\n",
       "  \"hidden_current\"  => 10\n",
       "  \"layer\"           => 3\n",
       "  \"activation\"      => \"softmax\"\n",
       "  \"sdW\"             => [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0…\n",
       "  \"A\"               => [0.0237935 0.00344209 … 0.00432173 0.16789; 0.862479 0.2…\n",
       "  \"hidden_previous\" => 128\n",
       "  \"last_layer\"      => true\n",
       "  \"vdb\"             => [0.0; 0.0; … ; 0.0; 0.0]"
      ]
     },
     "execution_count": 588,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivatives of activation functions\n",
    "function dSigmoid(z)\n",
    "    \n",
    "    #=\n",
    "    Derivative of the sigmoid function\n",
    "    \n",
    "    :param z: result of the linear transformation computed during forward propagation. Generally:\n",
    "                   Z = W_lA_{l-1} + b_l\n",
    "    :return: derivative of the activation function\n",
    "    \n",
    "    :seealso: \n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/qcG1j/derivatives-of-activation-functions\n",
    "      - https://deepnotes.io/softmax-crossentropy\n",
    "      - https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "    =#\n",
    "    \n",
    "    sigmoid(z) .* (1 .- sigmoid(z))\n",
    "    \n",
    "    end;\n",
    "\n",
    "function dReLU(z)\n",
    "    \n",
    "    #=\n",
    "    Derivative of ReLU function\n",
    "    \n",
    "    :param z: result of the linear transformation computed during forward propagation. Generally:\n",
    "                   Z = W_lA_{l-1} + b_l\n",
    "    :return: derivative of the activation function\n",
    "    \n",
    "    :seealso: https://www.coursera.org/learn/neural-networks-deep-learning/lecture/qcG1j/derivatives-of-activation-functions\n",
    "    =#\n",
    "    \n",
    "    (z .> 0) * 1\n",
    "    \n",
    "    end;\n",
    "\n",
    "function dTanh(z)\n",
    "    \n",
    "    #=\n",
    "    Derivative of tanh function\n",
    "    \n",
    "    :param z: result of the linear transformation computed during forward propagation. Generally:\n",
    "                   Z = W_lA_{l-1} + b_l\n",
    "    :return: derivative of the activation function\n",
    "    \n",
    "    :seealso: https://www.coursera.org/learn/neural-networks-deep-learning/lecture/qcG1j/derivatives-of-activation-functions\n",
    "    =# \n",
    "    \n",
    "    1 .- (tanh.(z) .* tanh.(z))\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "function backward_prop(layers, X, Y)\n",
    "    \n",
    "    #=\n",
    "    Backward propagation\n",
    "    \n",
    "    :param parameters:  dict containing weights and bias vectors for each layer\n",
    "    :param cache:       dict containing computations, dropout vectors and exponentially weighted gradients\n",
    "    :param X:           input data\n",
    "    :param drop_chance: probability of keeping a hidden unit\n",
    "    \n",
    "    :return: updated cache with new gradients for the current minibatch\n",
    "    \n",
    "    :seealso:\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/0ULGt/derivatives\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/oEcPT/more-derivative-examples\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/4WdOY/computation-graph\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/0VSHe/derivatives-with-a-computation-graph\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/6dDj7/backpropagation-intuition-optional\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/znwiG/forward-and-backward-propagation\n",
    "      Docs about dims argument for sum()\n",
    "      - https://docs.julialang.org/en/v0.6.1/stdlib/collections/#Base.sum\n",
    "    =#\n",
    "    \n",
    "    # Dims\n",
    "    n = size(Y)[1]\n",
    "    \n",
    "    # For layers (reverse)\n",
    "    for layer_number = length(layers):-1:1\n",
    "        \n",
    "        # Last layer?\n",
    "        last_layer = layers[layer_number][\"last_layer\"]\n",
    "        # Activation function\n",
    "        activation = layers[layer_number][\"activation\"]\n",
    "        \n",
    "        ## Last layer derivatives (softmax)\n",
    "        \n",
    "        if last_layer\n",
    "            \n",
    "            # Softmax derivative\n",
    "            layers[layer_number][\"dZ\"] = layers[layer_number][\"A\"] .- transpose(Y)\n",
    "            layers[layer_number][\"dW\"] = (1/n) .* (layers[layer_number][\"dZ\"] * transpose(layers[layer_number - 1][\"A\"]))\n",
    "            layers[layer_number][\"db\"] = (1/n) .* sum(layers[layer_number][\"dZ\"], dims=2)\n",
    "            \n",
    "        ## First layer derivatives\n",
    "            \n",
    "        elseif layer_number == 1\n",
    "            \n",
    "            # Activations for layer 1\n",
    "            dA = transpose(layers[layer_number + 1][\"W\"]) * layers[layer_number + 1][\"dZ\"]\n",
    "            # Apply dropout\n",
    "            dA = (layers[layer_number][\"D\"] .* dA) ./ layers[layer_number][\"keep_prob\"] \n",
    "            \n",
    "            # Derivative of activation function\n",
    "            if activation == \"ReLU\"\n",
    "\n",
    "                layers[layer_number][\"dZ\"] = dA .* dReLU(layers[layer_number][\"Z\"])\n",
    "\n",
    "            # Sigmoid derivative\n",
    "            elseif activation == \"sigmoid\"\n",
    "\n",
    "                layers[layer_number][\"dZ\"] = dA .* dSigmoid(layers[layer_number][\"Z\"])\n",
    "\n",
    "            elseif activation == \"tanh\"\n",
    "\n",
    "                layers[layer_number][\"dZ\"] = dA .* dTanh(layers[layer_number][\"Z\"])\n",
    "\n",
    "                end;  \n",
    "        \n",
    "            layers[layer_number][\"dA\"] = dA\n",
    "            # Linear combination derivative\n",
    "            layers[layer_number][\"dW\"] = (1/n) .* (layers[layer_number][\"dZ\"] * X)\n",
    "            layers[layer_number][\"db\"] = (1/n) .* sum(layers[layer_number][\"dZ\"], dims=2)            \n",
    "            \n",
    "        ## Intermediate layer derivatives\n",
    "            \n",
    "        else\n",
    "            \n",
    "            dA = transpose(layers[layer_number + 1][\"W\"]) * layers[layer_number + 1][\"dZ\"]\n",
    "            # Apply dropout\n",
    "            dA = (layers[layer_number][\"D\"] .* dA) ./ layers[layer_number][\"keep_prob\"]\n",
    "            \n",
    "            # Derivative of activation function\n",
    "            if activation == \"ReLU\"\n",
    "\n",
    "                layers[layer_number][\"dZ\"] = dA .* dReLU(layers[layer_number][\"Z\"])\n",
    "\n",
    "            # Sigmoid derivative\n",
    "            elseif activation == \"sigmoid\"\n",
    "\n",
    "                layers[layer_number][\"dZ\"] = dA .* dSigmoid(layers[layer_number][\"Z\"])\n",
    "\n",
    "            elseif activation == \"tanh\"\n",
    "\n",
    "                layers[layer_number][\"dZ\"] = dA .* dTanh(layers[layer_number][\"Z\"])\n",
    "\n",
    "                end;\n",
    "        \n",
    "            layers[layer_number][\"dA\"] = dA\n",
    "            # Linear combination derivative\n",
    "            layers[layer_number][\"dW\"] = (1/n) .* (layers[layer_number][\"dZ\"] * transpose(layers[layer_number-1][\"A\"]))\n",
    "            layers[layer_number][\"db\"] = (1/n) .* sum(layers[layer_number][\"dZ\"], dims=2)\n",
    "            \n",
    "            end;\n",
    "        \n",
    "        end;\n",
    "\n",
    "    return(layers)\n",
    "\n",
    "    end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = backward_prop(layers, X_tst, y_train_ohe[1:1000, :]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Any} with 19 entries:\n",
       "  \"Z\"               => [3.58824 3.32758 … 1.87696 1.16991; 1.70244 0.625872 … 2…\n",
       "  \"W\"               => [-0.0622553 0.0894943 … 0.011838 0.0137901; 0.0470461 -0…\n",
       "  \"dZ\"              => [0.105987 0.12436 … 0.0 0.216388; 0.0260865 0.0 … 0.0 -0…\n",
       "  \"dW\"              => [-0.00714246 -0.0099535 … -0.0057749 -0.00217071; 0.0399…\n",
       "  \"b\"               => [0.0; 0.0; … ; 0.0; 0.0]\n",
       "  \"keep_prob\"       => 0.6\n",
       "  \"vdW\"             => [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0…\n",
       "  \"sdb\"             => [0.0; 0.0; … ; 0.0; 0.0]\n",
       "  \"hidden_current\"  => 128\n",
       "  \"layer\"           => 2\n",
       "  \"activation\"      => \"ReLU\"\n",
       "  \"db\"              => [0.00944583; 0.0531046; … ; 0.00504288; -0.00548234]\n",
       "  \"sdW\"             => [0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0…\n",
       "  \"A\"               => [5.98041 5.54597 … 0.0 1.94984; 2.8374 0.0 … 0.0 3.31238…\n",
       "  \"hidden_previous\" => 256\n",
       "  \"D\"               => [1 1 … 0 1; 1 0 … 0 1; … ; 0 0 … 1 1; 0 0 … 0 1]\n",
       "  \"dA\"              => [0.105987 0.12436 … 0.0 0.216388; 0.0260865 0.0 … 0.0 -0…\n",
       "  \"last_layer\"      => false\n",
       "  \"vdb\"             => [0.0; 0.0; … ; 0.0; 0.0]"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating weights: Adam & RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Adam(layers; alpha = 0.001, beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8)\n",
    "    \n",
    "    #=\n",
    "    Update parameters using Adam optimization algorithm\n",
    "    \n",
    "    :param parameters:  dict containing weights and bias vectors for each layer\n",
    "    :param cache:       dict containing computations, dropout vectors and exponentially weighted gradients\n",
    "    :param alpha:       learning rate\n",
    "    :param beta:        exponential weighting value. Usually set at 0.9 <MORE>\n",
    "    :param epsilon:     small value to prevent divide by zero\n",
    "    \n",
    "    :return: updates weights in cache for current minibatch\n",
    "    \n",
    "    :seealso:\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/A0tBd/gradient-descent\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/udiAq/gradient-descent-on-m-examples\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/Wh8NI/gradient-descent-for-neural-networks\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/duStO/exponentially-weighted-averages\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/Ud7t0/understanding-exponentially-weighted-averages\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/XjuhD/bias-correction-in-exponentially-weighted-averages\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/y0m1f/gradient-descent-with-momentum\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/BhJlm/rmsprop\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/w9VCZ/adam-optimization-algorithm\n",
    "      - https://www.coursera.org/learn/deep-neural-network/notebook/eNLYh/optimization\n",
    "    =#\n",
    "    \n",
    "    # For each layer, update parameters\n",
    "    for layer in layers\n",
    "        \n",
    "        # Momentum\n",
    "        layer[\"vdW\"] = (beta1 .* layer[\"vdW\"] .+ (1 - beta1) .* layer[\"dW\"]) ./ (1 / beta1^2)\n",
    "        layer[\"vdb\"] = (beta1 .* layer[\"vdb\"] .+ (1 - beta1) .* layer[\"db\"]) ./ (1 / beta1^2)\n",
    "        \n",
    "        # RMSprop\n",
    "        layer[\"sdW\"] = (beta2 .* layer[\"sdW\"] .+ (1 - beta2) .* layer[\"dW\"].^2) ./ (1 / beta2^2)\n",
    "        layer[\"sdb\"] = (beta2 .* layer[\"sdb\"] .+ (1 - beta2) .* layer[\"db\"].^2) ./ (1 / beta2^2)\n",
    "        \n",
    "        # Update parameters\n",
    "        layer[\"W\"] = layer[\"W\"] .- (alpha .* (layer[\"vdW\"] ./ (sqrt.(layer[\"sdW\"] .+ epsilon))))\n",
    "        layer[\"b\"] = layer[\"b\"] .- (alpha .* (layer[\"vdb\"] ./ (sqrt.(layer[\"sdb\"] .+ epsilon))))\n",
    "        \n",
    "        end;\n",
    "    \n",
    "    # Return layers\n",
    "    return(layers)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [],
   "source": [
    "function RMSprop(layers; alpha = 0.001, beta = 0.9, epsilon = 1e-8)\n",
    "    \n",
    "    #=\n",
    "    Update parameters using RMSProp optimization algorithm\n",
    "    \n",
    "    :param parameters:  dict containing weights and bias vectors for each layer\n",
    "    :param cache:       dict containing computations, dropout vectors and exponentially weighted gradients\n",
    "    :param alpha:       learning rate\n",
    "    :param beta:        exponential weighting value. Usually set at 0.9 <MORE>\n",
    "    :param epsilon:     small value to prevent divide by zero\n",
    "    \n",
    "    :return: updates weights in cache for current minibatch\n",
    "    \n",
    "    :seealso:\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/A0tBd/gradient-descent\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/udiAq/gradient-descent-on-m-examples\n",
    "      - https://www.coursera.org/learn/neural-networks-deep-learning/lecture/Wh8NI/gradient-descent-for-neural-networks\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/duStO/exponentially-weighted-averages\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/Ud7t0/understanding-exponentially-weighted-averages\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/XjuhD/bias-correction-in-exponentially-weighted-averages\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/y0m1f/gradient-descent-with-momentum\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/BhJlm/rmsprop\n",
    "      - https://www.coursera.org/learn/deep-neural-network/lecture/w9VCZ/adam-optimization-algorithm\n",
    "      - https://www.coursera.org/learn/deep-neural-network/notebook/eNLYh/optimization\n",
    "    =#\n",
    "    \n",
    "    # For each layer, update parameters\n",
    "    for layer in layers\n",
    "        \n",
    "        # RMSprop\n",
    "        layer[\"sdW\"] = (beta .* layer[\"sdW\"] .+ (1 - beta) .* layer[\"dW\"].^2) ./ (1 / beta^2)\n",
    "        layer[\"sdb\"] = (beta .* layer[\"sdb\"] .+ (1 - beta) .* layer[\"db\"].^2) ./ (1 / beta^2)\n",
    "        \n",
    "        # Update parameters\n",
    "        layer[\"W\"] = layer[\"W\"] .- (alpha .* (layer[\"dW\"] ./ (sqrt.(layer[\"sdW\"] .+ epsilon))))\n",
    "        layer[\"b\"] = layer[\"b\"] .- (alpha .* (layer[\"db\"] ./ (sqrt.(layer[\"sdb\"] .+ epsilon))))\n",
    "        \n",
    "        end;\n",
    "    \n",
    "    # Return layers\n",
    "    return(layers)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = Adam(layers);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Any} with 19 entries:\n",
       "  \"Z\"               => [3.58824 3.32758 … 1.87696 1.16991; 1.70244 0.625872 … 2…\n",
       "  \"W\"               => [-0.0599112 0.0919378 … 0.0140864 0.0152401; 0.0444901 -…\n",
       "  \"dZ\"              => [0.105987 0.12436 … 0.0 0.216388; 0.0260865 0.0 … 0.0 -0…\n",
       "  \"dW\"              => [-0.00714246 -0.0099535 … -0.0057749 -0.00217071; 0.0399…\n",
       "  \"b\"               => [-0.00243113; -0.00255947; … ; -0.00217163; 0.00222046]\n",
       "  \"keep_prob\"       => 0.6\n",
       "  \"vdW\"             => [-0.000578539 -0.000806233 … -0.000467767 -0.000175828; …\n",
       "  \"sdb\"             => [8.90453e-8; 2.81446e-6; … ; 2.53799e-8; 2.9996e-8]\n",
       "  \"hidden_current\"  => 128\n",
       "  \"layer\"           => 2\n",
       "  \"activation\"      => \"ReLU\"\n",
       "  \"db\"              => [0.00944583; 0.0531046; … ; 0.00504288; -0.00548234]\n",
       "  \"sdW\"             => [5.09127e-8 9.88741e-8 … 3.32828e-8 4.70256e-9; 1.58937e…\n",
       "  \"A\"               => [5.98041 5.54597 … 0.0 1.94984; 2.8374 0.0 … 0.0 3.31238…\n",
       "  \"hidden_previous\" => 256\n",
       "  \"D\"               => [1 1 … 0 1; 1 0 … 0 1; … ; 0 0 … 1 1; 0 0 … 0 1]\n",
       "  \"dA\"              => [0.105987 0.12436 … 0.0 0.216388; 0.0260865 0.0 … 0.0 -0…\n",
       "  \"last_layer\"      => false\n",
       "  \"vdb\"             => [0.000765112; 0.00430147; … ; 0.000408474; -0.00044407]"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "function learning_rate_decay(alpha, epoch, decay = 1)\n",
    "    \n",
    "    #= \n",
    "    Decays the value of alpha (learning rate) as number of epochs increases\n",
    "    \n",
    "    :param alpha: learning rate \n",
    "    :param epoch: current epoch number\n",
    "    :param decay: decay value\n",
    "    \n",
    "    :return: updated learning rate\n",
    "    \n",
    "    :seealso: https://www.coursera.org/learn/deep-neural-network/lecture/hjgIA/learning-rate-decay\n",
    "    =#\n",
    "    \n",
    "    alpha_new = 1 / (1 + decay * epoch) * alpha\n",
    "    \n",
    "    # Return\n",
    "    return(alpha_new)\n",
    "    \n",
    "    end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(X, layers)\n",
    "    \n",
    "    #=\n",
    "    Predict Y given X and the parameters\n",
    "    \n",
    "    :param X:          input data\n",
    "    :param parameters: parameters for each layer\n",
    "    \n",
    "    :return: predicted values given X and parameters\n",
    "    =#\n",
    "    \n",
    "    # Define scope for A_previous and A\n",
    "    local A_prev\n",
    "    local A\n",
    "    \n",
    "    # For each layer, compute forward propagation step\n",
    "    for layer in layers\n",
    "        \n",
    "        # Layer number\n",
    "        layer_number = layer[\"layer\"]\n",
    "        # Last layer?\n",
    "        last_layer = layer[\"last_layer\"]\n",
    "        # Activation function\n",
    "        activation = layer[\"activation\"]\n",
    "        \n",
    "        # If first layer\n",
    "        if layer_number == 1\n",
    "            \n",
    "            # Multiply weights times X\n",
    "            Z = broadcast(+, layer[\"W\"] * transpose(X), layer[\"b\"], 1)\n",
    "            \n",
    "        else\n",
    "            \n",
    "            # Multiply weights times the activation of the previous layer\n",
    "            Z = broadcast(+, layer[\"W\"] * A_prev, layer[\"b\"], 1)\n",
    "            \n",
    "            end;\n",
    "        \n",
    "        # Activation function\n",
    "        if activation == \"ReLU\"\n",
    "            \n",
    "            A = ReLU(Z)\n",
    "            A_prev = A\n",
    "            \n",
    "        elseif activation == \"sigmoid\"\n",
    "            \n",
    "            A = sigmoid(Z)\n",
    "            A_prev = A\n",
    "            \n",
    "        elseif activation == \"tanh\"\n",
    "            \n",
    "            A = tanh.(Z)\n",
    "            A_prev = A\n",
    "            \n",
    "        elseif activation == \"softmax\"\n",
    "            \n",
    "            A = softmax(Z)\n",
    "            \n",
    "            end;\n",
    "                \n",
    "        end;\n",
    "    \n",
    "    # Return\n",
    "    return(A)\n",
    "    \n",
    "    end;\n",
    "\n",
    "function accuracy(Y, Yhat)\n",
    "    \n",
    "    #=\n",
    "    Calculates accuracy\n",
    "    \n",
    "    :param Y:    actual outcome values (as one-hot encoded) \n",
    "    :param Yhat: predicted outcome values (as one-hot encoded)\n",
    "    \n",
    "    :return: accuracy of the predictions as a float between 0 and 1\n",
    "    =# \n",
    "    \n",
    "    # Predictions --> labels\n",
    "    vals, inds = findmax(Yhat, dims = 1);\n",
    "    Yhat_labels = map(x -> x[1] - 1, inds)\n",
    "\n",
    "    # Y ohe --> labels\n",
    "    vals, inds = findmax(transpose(Y), dims=1)\n",
    "    Y_labels = map(x -> x[1] - 1, inds)\n",
    "    \n",
    "    # Accuracy\n",
    "    return(sum(Y_labels .== Yhat_labels) / length(Y_labels))\n",
    "    \n",
    "    end;\n",
    "\n",
    "function evaluate_model(X, Y, layers; return_accuracy = false)\n",
    "    \n",
    "    #= \n",
    "    Evaluate the model on the development & train sets\n",
    "    \n",
    "    :param X:               input data \n",
    "    :param Y:               output data\n",
    "    :param parameters:      dict containing parameters for each layer\n",
    "    :param return_accuracy: boolean. If true, returns loss and accuracy. If false, only returns loss\n",
    "    \n",
    "    :return: either loss and accuracy or only loss value\n",
    "    =#\n",
    "    \n",
    "    # Predict\n",
    "    Yhat = predict(X, layers)\n",
    "    \n",
    "    # Loss\n",
    "    loss = crossentropy_cost(Y, Yhat)\n",
    "    \n",
    "    # Accuracy\n",
    "    if return_accuracy\n",
    "        acc = accuracy(Y, Yhat)\n",
    "        return((loss, acc))\n",
    "        \n",
    "        end;\n",
    "    \n",
    "    # Return\n",
    "    return(loss)\n",
    "    \n",
    "    end;\n",
    "\n",
    "using Plots\n",
    "\n",
    "function plot_history(history)\n",
    "    \n",
    "    #=\n",
    "    Plot loss across epochs\n",
    "    \n",
    "    :param history: nn history returned by nnet function\n",
    "    \n",
    "    :return: this function plots the history but does not return a value \n",
    "    \n",
    "    :seealso: https://docs.juliaplots.org/latest/tutorial/#plot-recipes-and-recipe-libraries\n",
    "    =#\n",
    "    \n",
    "    # Retrieve data\n",
    "    epoch = [x[1] for x in history]\n",
    "    train_loss = [x[2] for x in history]\n",
    "    dev_loss = [x[3] for x in history]\n",
    "    # Train and dev loss in one array (2 columns)\n",
    "    loss = hcat(train_loss, dev_loss);\n",
    "    \n",
    "    # Plot epochs versus loss\n",
    "    plot(epoch, loss, seriestype=:line, xlabel = \"epoch\", ylabel = \"cost\", lab = [\"Train\" \"Dev\"],\n",
    "         lw = 2)\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a single layer with relu activation and no dropout\n",
    "layers = initialize_parameters(784, 256, 10, \"tanh\", 1, 0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cost: 2.087984369284104] ==> [Accuracy: 0.351]\n",
      "[Cost: 1.781064622062307] ==> [Accuracy: 0.504]\n",
      "[Cost: 1.5148449463573979] ==> [Accuracy: 0.724]\n",
      "[Cost: 1.3123111229640145] ==> [Accuracy: 0.742]\n",
      "[Cost: 1.1492129707541072] ==> [Accuracy: 0.795]\n"
     ]
    }
   ],
   "source": [
    "# For clarity\n",
    "X = train_x\n",
    "Y = y_train_ohe\n",
    "\n",
    "# Train the model for 5 steps to see if the cost goes down\n",
    "for i = 1:5\n",
    "    \n",
    "    layers = forward_prop(layers, X[1:1000,:])\n",
    "    layers = backward_prop(layers, X[1:1000,:], Y[1:1000,:])\n",
    "    layers = RMSprop(layers)\n",
    "    cost, acc = evaluate_model(X[1:1000,:], Y[1:1000,:], layers, return_accuracy=true)\n",
    "    \n",
    "    # Print\n",
    "    println(\"[Cost: \", cost, \"] ==> [Accuracy: \", acc, \"]\")\n",
    "    \n",
    "    end;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling it all together: the nnet() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "function nnet(X, Y; n_h = 128, activations = \"tanh\", validation_split = 0.1, optimizer = \"Adam\", \n",
    "              alpha = 0.001, beta1=0.9, beta2 = 0.999, epsilon=10e-8, epochs = 10, batch_size=128, \n",
    "              lr_decay=1, dropout_prop=0.3)\n",
    "    \n",
    "    #= \n",
    "    Implements a two-layer neural network\n",
    "    \n",
    "    :param X:                   input data\n",
    "    :param Y:                   actual outcome values (as one-hot encoded) \n",
    "    :param n_h1:                hidden units for layer 1\n",
    "    :param n_h2:                hidden units for layer 2\n",
    "    :param validation_split:    decimal indicating what percentage of the data should be reserved for \n",
    "                                 validation split\n",
    "    :param alpha:               learning rate \n",
    "    :param beta:                exponential weighting value. Usually set at 0.9 <MORE>\n",
    "    :param epsilon:             small value to prevent divide by zero in RMSprop \n",
    "    :param epochs:              number of passes (epochs) to train the network\n",
    "    :param batch_size:          size of the minibatches\n",
    "    :param lr_decay:            decay value for learning rate (alpha)\n",
    "    :param dropout_prop:        probability of dropping a hidden unit when performing dropout regularization. \n",
    "                                 A value of 0 keeps all hidden units\n",
    "    \n",
    "    :return: final parameters for the trained model and training history\n",
    "    =#\n",
    "    \n",
    "    ## Checks\n",
    "    \n",
    "    # hidden units, activations and dropout must all be tuple\n",
    "    if all((isa(n_h, Tuple), isa(activations, Tuple), isa(dropout_prop, Tuple)))\n",
    "        \n",
    "        @assert length(n_h) == length(activations) \"Number of hidden units must equal number of activations\"\n",
    "        @assert length(n_h) == length(dropout_prop) \"Number of hidden units must equal the number of dropout probabilities\"\n",
    "        \n",
    "        hidden_layers = length(n_h)\n",
    "        \n",
    "    else\n",
    "        \n",
    "        hidden_layers = 1\n",
    "        \n",
    "        end;\n",
    "    \n",
    "    # Set dimensions\n",
    "    n = size(X)[1]\n",
    "    n_x = size(X)[2]\n",
    "    n_y = size(Y)[2]\n",
    "\n",
    "    # Save alpha value\n",
    "    alpha_initial = alpha\n",
    "\n",
    "    # Initialize parameters\n",
    "    layers = initialize_parameters(n_x, n_h, n_y, activations, hidden_layers, dropout_prop)\n",
    "    \n",
    "    # Open list for history\n",
    "    history = []\n",
    "    \n",
    "    # Validation and train set\n",
    "    data = cross_validation(X, Y, validation_split)\n",
    "\n",
    "    # Unroll\n",
    "    X_train = data[\"X_train\"]\n",
    "    Y_train = data[\"Y_train\"]\n",
    "    X_dev = data[\"X_dev\"]\n",
    "    Y_dev = data[\"Y_dev\"]\n",
    "\n",
    "    # Loop through epochs\n",
    "    for i = 1:epochs\n",
    "\n",
    "        # Set up number of batches\n",
    "        batches = create_batches(X_train, Y_train, batch_size)\n",
    "        \n",
    "        ## Mini-batches (inner loop)\n",
    "        for batch in batches\n",
    "\n",
    "            # Unroll X and y\n",
    "            X = batch[1]\n",
    "            Y = batch[2]\n",
    "            \n",
    "            # Forward prop\n",
    "            layers = forward_prop(layers, X)\n",
    "            \n",
    "            # Backward propagation\n",
    "            layers = backward_prop(layers, X, Y)\n",
    "            \n",
    "            # Update params\n",
    "            if optimizer == \"Adam\"\n",
    "                \n",
    "                layers = Adam(layers, alpha = alpha, beta1 = beta1, beta2 = beta2, epsilon = epsilon)\n",
    "                \n",
    "            else\n",
    "                \n",
    "                layers = RMSprop(layers, alpha = alpha, beta = beta1, epsilon = epsilon)\n",
    "                \n",
    "                end;\n",
    "            \n",
    "            end;\n",
    "        \n",
    "        # Learning rate decay\n",
    "        alpha = learning_rate_decay(alpha_initial, i, lr_decay)\n",
    "\n",
    "        # Evaluate\n",
    "        eval_train = evaluate_model(X_train, Y_train, layers)\n",
    "        eval_dev, eval_acc = evaluate_model(X_dev, Y_dev, layers, return_accuracy=true)\n",
    "        \n",
    "        # Round\n",
    "        eval_train = round(eval_train, digits=3)\n",
    "        eval_dev = round(eval_dev, digits=3)\n",
    "\n",
    "        # Print\n",
    "        println(\"Epoch \", i, \":\\n\",\n",
    "                \"\\t[train cost: \", eval_train, \n",
    "                \"] ==> [validation cost: \", eval_dev, \"]\",\n",
    "                \"\\n\\tvalidation accuracy: \", round(eval_acc, digits=3))\n",
    "        \n",
    "        # Add to history\n",
    "        push!(history, [i, eval_train, eval_dev])\n",
    "\n",
    "        end;\n",
    "    \n",
    "    # Return the final parameters\n",
    "    return((layers, history))\n",
    "    \n",
    "    end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the network\n",
    "\n",
    "### Hyperparameter tuning\n",
    "\n",
    "See: https://stats.stackexchange.com/questions/369104/what-is-a-sensible-order-for-parameter-tuning-in-neural-networks\n",
    "\n",
    "see: https://towardsdatascience.com/a-guide-to-an-efficient-way-to-build-neural-network-architectures-part-i-hyper-parameter-8129009f131b\n",
    "\n",
    "see: https://www.jeremyjordan.me/nn-learning-rate/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split test into dev (70%) test (30%)\n",
    "Random.seed!(9726)\n",
    "split = cross_validation(test_x, y_test_ohe, 0.3)\n",
    "X_dev = split[\"X_train\"]\n",
    "Y_dev = split[\"Y_train\"]\n",
    "X_test = split[\"X_dev\"]\n",
    "Y_test = split[\"Y_dev\"];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\t[train cost: 0.494] ==> [validation cost: 0.484]\n",
      "\tvalidation accuracy: 0.892\n",
      "Epoch 2:\n",
      "\t[train cost: 0.357] ==> [validation cost: 0.349]\n",
      "\tvalidation accuracy: 0.913\n",
      "Epoch 3:\n",
      "\t[train cost: 0.302] ==> [validation cost: 0.296]\n",
      "\tvalidation accuracy: 0.924\n",
      "Epoch 4:\n",
      "\t[train cost: 0.27] ==> [validation cost: 0.267]\n",
      "\tvalidation accuracy: 0.927\n",
      "Epoch 5:\n",
      "\t[train cost: 0.248] ==> [validation cost: 0.248]\n",
      "\tvalidation accuracy: 0.932\n",
      "Epoch 6:\n",
      "\t[train cost: 0.228] ==> [validation cost: 0.23]\n",
      "\tvalidation accuracy: 0.933\n",
      "Epoch 7:\n",
      "\t[train cost: 0.214] ==> [validation cost: 0.219]\n",
      "\tvalidation accuracy: 0.938\n",
      "Epoch 8:\n",
      "\t[train cost: 0.2] ==> [validation cost: 0.206]\n",
      "\tvalidation accuracy: 0.94\n",
      "Epoch 9:\n",
      "\t[train cost: 0.189] ==> [validation cost: 0.196]\n",
      "\tvalidation accuracy: 0.944\n",
      "Epoch 10:\n",
      "\t[train cost: 0.179] ==> [validation cost: 0.187]\n",
      "\tvalidation accuracy: 0.946\n"
     ]
    }
   ],
   "source": [
    "layers, history = nnet(X, Y, n_h = 64, activations = \"sigmoid\",\n",
    "                       dropout_prop=0, batch_size = 128, alpha = 0.001, epochs=10,\n",
    "                       lr_decay=0, validation_split = 0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip3200\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2000\" height=\"2000\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<defs>\n",
       "  <clipPath id=\"clip3201\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip3201)\" points=\"\n",
       "0,1600 2400,1600 2400,0 0,0 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip3202\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip3201)\" points=\"\n",
       "224.386,1440.48 2321.26,1440.48 2321.26,47.2441 224.386,47.2441 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip3203\">\n",
       "    <rect x=\"224\" y=\"47\" width=\"2098\" height=\"1394\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip3203)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  503.53,1440.48 503.53,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3203)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  943.126,1440.48 943.126,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3203)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1382.72,1440.48 1382.72,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3203)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1822.32,1440.48 1822.32,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3203)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2261.91,1440.48 2261.91,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3203)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,1313.43 2321.26,1313.43 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3203)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,896.164 2321.26,896.164 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3203)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,478.902 2321.26,478.902 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3203)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  224.386,61.6396 2321.26,61.6396 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3201)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,1440.48 2321.26,1440.48 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3201)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,1440.48 224.386,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3201)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  503.53,1440.48 503.53,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3201)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  943.126,1440.48 943.126,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3201)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1382.72,1440.48 1382.72,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3201)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1822.32,1440.48 1822.32,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3201)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2261.91,1440.48 2261.91,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3201)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,1313.43 255.839,1313.43 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3201)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,896.164 255.839,896.164 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3201)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,478.902 255.839,478.902 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3201)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  224.386,61.6396 255.839,61.6396 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip3201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 503.53, 1494.48)\" x=\"503.53\" y=\"1494.48\">2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip3201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 943.126, 1494.48)\" x=\"943.126\" y=\"1494.48\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip3201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1382.72, 1494.48)\" x=\"1382.72\" y=\"1494.48\">6</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip3201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1822.32, 1494.48)\" x=\"1822.32\" y=\"1494.48\">8</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip3201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2261.91, 1494.48)\" x=\"2261.91\" y=\"1494.48\">10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip3201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 1330.93)\" x=\"200.386\" y=\"1330.93\">0.2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip3201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 913.664)\" x=\"200.386\" y=\"913.664\">0.3</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip3201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 496.402)\" x=\"200.386\" y=\"496.402\">0.4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip3201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 200.386, 79.1396)\" x=\"200.386\" y=\"79.1396\">0.5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip3201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1272.82, 1590.4)\" x=\"1272.82\" y=\"1590.4\">epoch</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip3201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 57.6, 743.863)\" x=\"57.6\" y=\"743.863\">cost</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip3203)\" style=\"stroke:#009af9; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  283.732,86.6754 503.53,658.325 723.328,887.819 943.126,1021.34 1162.92,1113.14 1382.72,1196.59 1602.52,1255.01 1822.32,1313.43 2042.12,1359.33 2261.91,1401.05 \n",
       "  \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3203)\" style=\"stroke:#e26f46; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  283.732,128.402 503.53,691.706 723.328,912.855 943.126,1033.86 1162.92,1113.14 1382.72,1188.25 1602.52,1234.15 1822.32,1288.39 2042.12,1330.12 2261.91,1367.67 \n",
       "  \n",
       "  \"/>\n",
       "<polygon clip-path=\"url(#clip3201)\" points=\"\n",
       "1899.61,312.204 2249.26,312.204 2249.26,130.764 1899.61,130.764 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip3201)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1899.61,312.204 2249.26,312.204 2249.26,130.764 1899.61,130.764 1899.61,312.204 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip3201)\" style=\"stroke:#009af9; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  1923.61,191.244 2067.61,191.244 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip3201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2091.61, 208.744)\" x=\"2091.61\" y=\"208.744\">Train</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip3201)\" style=\"stroke:#e26f46; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  1923.61,251.724 2067.61,251.724 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip3201)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2091.61, 269.224)\" x=\"2091.61\" y=\"269.224\">Dev</text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot history\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94875"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train accuracy\n",
    "Yhat = predict(X, layers);\n",
    "acc = accuracy(Y, Yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9488571428571428"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dev accuracy\n",
    "Yhat_dev = predict(X_dev, layers);\n",
    "# Accuracy\n",
    "acc_dev = accuracy(Y_dev, Yhat_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The single layer neural network with sigmoid activation function achieves a respectable dev set accuracy of $94.9$%. Towards the end of the training epochs, it starts to overfit on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single layer model with ReLU activation, low dropout probability and batch size 32\n",
    "\n",
    "We can probably improve on the above model by using a different activation function and by changing the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\t[train cost: 0.144] ==> [validation cost: 0.162]\n",
      "\tvalidation accuracy: 0.954\n",
      "Epoch 2:\n",
      "\t[train cost: 0.092] ==> [validation cost: 0.119]\n",
      "\tvalidation accuracy: 0.965\n",
      "Epoch 3:\n",
      "\t[train cost: 0.06] ==> [validation cost: 0.097]\n",
      "\tvalidation accuracy: 0.97\n",
      "Epoch 4:\n",
      "\t[train cost: 0.046] ==> [validation cost: 0.088]\n",
      "\tvalidation accuracy: 0.973\n",
      "Epoch 5:\n",
      "\t[train cost: 0.035] ==> [validation cost: 0.079]\n",
      "\tvalidation accuracy: 0.976\n",
      "Epoch 6:\n",
      "\t[train cost: 0.031] ==> [validation cost: 0.076]\n",
      "\tvalidation accuracy: 0.977\n",
      "Epoch 7:\n",
      "\t[train cost: 0.023] ==> [validation cost: 0.083]\n",
      "\tvalidation accuracy: 0.976\n",
      "Epoch 8:\n",
      "\t[train cost: 0.019] ==> [validation cost: 0.074]\n",
      "\tvalidation accuracy: 0.979\n",
      "Epoch 9:\n",
      "\t[train cost: 0.015] ==> [validation cost: 0.078]\n",
      "\tvalidation accuracy: 0.979\n",
      "Epoch 10:\n",
      "\t[train cost: 0.014] ==> [validation cost: 0.083]\n",
      "\tvalidation accuracy: 0.978\n"
     ]
    }
   ],
   "source": [
    "layers, history = nnet(X, Y, n_h = 256, activations = \"ReLU\", dropout_prop=0.2, batch_size = 32, \n",
    "                       alpha = 0.0014, epochs=10, lr_decay=0, validation_split = 0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip5600\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2000\" height=\"2000\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<defs>\n",
       "  <clipPath id=\"clip5601\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip5601)\" points=\"\n",
       "0,1600 2400,1600 2400,0 0,0 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip5602\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip5601)\" points=\"\n",
       "251.149,1440.48 2321.26,1440.48 2321.26,47.2441 251.149,47.2441 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip5603\">\n",
       "    <rect x=\"251\" y=\"47\" width=\"2071\" height=\"1394\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip5603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  526.729,1440.48 526.729,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  960.715,1440.48 960.715,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1394.7,1440.48 1394.7,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1828.69,1440.48 1828.69,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2262.67,1440.48 2262.67,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  251.149,1258.96 2321.26,1258.96 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  251.149,992.529 2321.26,992.529 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  251.149,726.102 2321.26,726.102 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  251.149,459.674 2321.26,459.674 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5603)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  251.149,193.246 2321.26,193.246 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,1440.48 2321.26,1440.48 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,1440.48 251.149,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  526.729,1440.48 526.729,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  960.715,1440.48 960.715,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1394.7,1440.48 1394.7,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1828.69,1440.48 1828.69,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2262.67,1440.48 2262.67,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,1258.96 282.2,1258.96 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,992.529 282.2,992.529 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,726.102 282.2,726.102 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,459.674 282.2,459.674 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  251.149,193.246 282.2,193.246 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip5601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 526.729, 1494.48)\" x=\"526.729\" y=\"1494.48\">2</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 960.715, 1494.48)\" x=\"960.715\" y=\"1494.48\">4</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1394.7, 1494.48)\" x=\"1394.7\" y=\"1494.48\">6</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1828.69, 1494.48)\" x=\"1828.69\" y=\"1494.48\">8</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 2262.67, 1494.48)\" x=\"2262.67\" y=\"1494.48\">10</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 227.149, 1276.46)\" x=\"227.149\" y=\"1276.46\">0.03</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 227.149, 1010.03)\" x=\"227.149\" y=\"1010.03\">0.06</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 227.149, 743.602)\" x=\"227.149\" y=\"743.602\">0.09</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 227.149, 477.174)\" x=\"227.149\" y=\"477.174\">0.12</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 227.149, 210.746)\" x=\"227.149\" y=\"210.746\">0.15</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1286.2, 1590.4)\" x=\"1286.2\" y=\"1590.4\">epoch</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip5601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 57.6, 743.863)\" x=\"57.6\" y=\"743.863\">cost</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip5603)\" style=\"stroke:#009af9; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  309.737,246.532 526.729,708.34 743.722,992.529 960.715,1116.86 1177.71,1214.55 1394.7,1250.08 1611.69,1321.12 1828.69,1356.65 2045.68,1392.17 2262.67,1401.05 \n",
       "  \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5603)\" style=\"stroke:#e26f46; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  309.737,86.6754 526.729,468.555 743.722,663.935 960.715,743.863 1177.71,823.792 1394.7,850.434 1611.69,788.268 1828.69,868.196 2045.68,832.673 2262.67,788.268 \n",
       "  \n",
       "  \"/>\n",
       "<polygon clip-path=\"url(#clip5601)\" points=\"\n",
       "1899.61,312.204 2249.26,312.204 2249.26,130.764 1899.61,130.764 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip5601)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1899.61,312.204 2249.26,312.204 2249.26,130.764 1899.61,130.764 1899.61,312.204 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip5601)\" style=\"stroke:#009af9; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  1923.61,191.244 2067.61,191.244 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip5601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2091.61, 208.744)\" x=\"2091.61\" y=\"208.744\">Train</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip5601)\" style=\"stroke:#e26f46; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  1923.61,251.724 2067.61,251.724 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip5601)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2091.61, 269.224)\" x=\"2091.61\" y=\"269.224\">Dev</text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot history\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9942666666666666"
      ]
     },
     "execution_count": 623,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train accuracy\n",
    "Yhat = predict(X, layers);\n",
    "acc = accuracy(Y, Yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9811428571428571"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dev accuracy\n",
    "Yhat_dev = predict(X_dev, layers);\n",
    "# Accuracy\n",
    "acc_dev = accuracy(Y_dev, Yhat_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we achieve a higher development set accuracy ($97.7$%), it should be clear from the plot that we are overfitting rather heavily on the training data. Still, it is good performance for a small network with only 1 hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple layers\n",
    "\n",
    "The overfitting problem will become more pressing as we build deeper models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\t[train cost: 0.124] ==> [validation cost: 0.144]\n",
      "\tvalidation accuracy: 0.956\n",
      "Epoch 2:\n",
      "\t[train cost: 0.072] ==> [validation cost: 0.102]\n",
      "\tvalidation accuracy: 0.968\n",
      "Epoch 3:\n",
      "\t[train cost: 0.049] ==> [validation cost: 0.096]\n",
      "\tvalidation accuracy: 0.972\n",
      "Epoch 4:\n",
      "\t[train cost: 0.043] ==> [validation cost: 0.09]\n",
      "\tvalidation accuracy: 0.975\n",
      "Epoch 5:\n",
      "\t[train cost: 0.035] ==> [validation cost: 0.092]\n",
      "\tvalidation accuracy: 0.974\n",
      "Epoch 6:\n",
      "\t[train cost: 0.022] ==> [validation cost: 0.07]\n",
      "\tvalidation accuracy: 0.981\n",
      "Epoch 7:\n",
      "\t[train cost: 0.021] ==> [validation cost: 0.085]\n",
      "\tvalidation accuracy: 0.978\n",
      "Epoch 8:\n",
      "\t[train cost: 0.016] ==> [validation cost: 0.075]\n",
      "\tvalidation accuracy: 0.981\n",
      "Epoch 9:\n",
      "\t[train cost: 0.012] ==> [validation cost: 0.075]\n",
      "\tvalidation accuracy: 0.981\n",
      "Epoch 10:\n",
      "\t[train cost: 0.01] ==> [validation cost: 0.079]\n",
      "\tvalidation accuracy: 0.98\n",
      "Epoch 11:\n",
      "\t[train cost: 0.01] ==> [validation cost: 0.09]\n",
      "\tvalidation accuracy: 0.98\n",
      "Epoch 12:\n",
      "\t[train cost: 0.008] ==> [validation cost: 0.083]\n",
      "\tvalidation accuracy: 0.981\n"
     ]
    }
   ],
   "source": [
    "layers, history = nnet(X, Y, n_h = (512, 256, 256, 128), activations = (\"ReLU\", \"ReLU\", \"ReLU\", \"ReLU\"), \n",
    "                        optimizer = \"Adam\", dropout_prop=(0.2, 0.2, 0.2, 0.2), batch_size = 32, alpha = 0.002, \n",
    "                        epochs=12, lr_decay=0.1, validation_split = 0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip6000\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2000\" height=\"2000\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<defs>\n",
       "  <clipPath id=\"clip6001\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip6001)\" points=\"\n",
       "0,1600 2400,1600 2400,0 0,0 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip6002\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polygon clip-path=\"url(#clip6001)\" points=\"\n",
       "277.911,1440.48 2321.26,1440.48 2321.26,47.2441 277.911,47.2441 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip6003\">\n",
       "    <rect x=\"277\" y=\"47\" width=\"2044\" height=\"1394\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip6003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  598.608,1440.48 598.608,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1036.72,1440.48 1036.72,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1474.83,1440.48 1474.83,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1912.94,1440.48 1912.94,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  277.911,1236.75 2321.26,1236.75 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  277.911,995.141 2321.26,995.141 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  277.911,753.528 2321.26,753.528 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  277.911,511.915 2321.26,511.915 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6003)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  277.911,270.301 2321.26,270.301 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  277.911,1440.48 2321.26,1440.48 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  277.911,1440.48 277.911,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  598.608,1440.48 598.608,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1036.72,1440.48 1036.72,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1474.83,1440.48 1474.83,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1912.94,1440.48 1912.94,1419.58 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  277.911,1236.75 308.561,1236.75 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  277.911,995.141 308.561,995.141 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  277.911,753.528 308.561,753.528 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  277.911,511.915 308.561,511.915 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  277.911,270.301 308.561,270.301 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip6001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 598.608, 1494.48)\" x=\"598.608\" y=\"1494.48\">2.5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1036.72, 1494.48)\" x=\"1036.72\" y=\"1494.48\">5.0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1474.83, 1494.48)\" x=\"1474.83\" y=\"1494.48\">7.5</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:middle;\" transform=\"rotate(0, 1912.94, 1494.48)\" x=\"1912.94\" y=\"1494.48\">10.0</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 253.911, 1254.25)\" x=\"253.911\" y=\"1254.25\">0.025</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 253.911, 1012.64)\" x=\"253.911\" y=\"1012.64\">0.050</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 253.911, 771.028)\" x=\"253.911\" y=\"771.028\">0.075</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 253.911, 529.415)\" x=\"253.911\" y=\"529.415\">0.100</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:end;\" transform=\"rotate(0, 253.911, 287.801)\" x=\"253.911\" y=\"287.801\">0.125</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(0, 1299.59, 1590.4)\" x=\"1299.59\" y=\"1590.4\">epoch</text>\n",
       "</g>\n",
       "<g clip-path=\"url(#clip6001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:66px; text-anchor:middle;\" transform=\"rotate(-90, 57.6, 743.863)\" x=\"57.6\" y=\"743.863\">cost</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip6003)\" style=\"stroke:#009af9; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  335.741,279.966 510.986,782.521 686.23,1004.81 861.474,1062.79 1036.72,1140.11 1211.96,1265.75 1387.21,1275.41 1562.45,1323.74 1737.7,1362.39 1912.94,1381.72 \n",
       "  2088.18,1381.72 2263.43,1401.05 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6003)\" style=\"stroke:#e26f46; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  335.741,86.6754 510.986,492.586 686.23,550.573 861.474,608.56 1036.72,589.231 1211.96,801.851 1387.21,656.883 1562.45,753.528 1737.7,753.528 1912.94,714.87 \n",
       "  2088.18,608.56 2263.43,676.212 \n",
       "  \"/>\n",
       "<polygon clip-path=\"url(#clip6001)\" points=\"\n",
       "1899.61,312.204 2249.26,312.204 2249.26,130.764 1899.61,130.764 \n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1899.61,312.204 2249.26,312.204 2249.26,130.764 1899.61,130.764 1899.61,312.204 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#009af9; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  1923.61,191.244 2067.61,191.244 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip6001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2091.61, 208.744)\" x=\"2091.61\" y=\"208.744\">Train</text>\n",
       "</g>\n",
       "<polyline clip-path=\"url(#clip6001)\" style=\"stroke:#e26f46; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  1923.61,251.724 2067.61,251.724 \n",
       "  \"/>\n",
       "<g clip-path=\"url(#clip6001)\">\n",
       "<text style=\"fill:#000000; fill-opacity:1; font-family:Arial,Helvetica Neue,Helvetica,sans-serif; font-size:48px; text-anchor:start;\" transform=\"rotate(0, 2091.61, 269.224)\" x=\"2091.61\" y=\"269.224\">Dev</text>\n",
       "</g>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot history\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9959166666666667"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train accuracy\n",
    "Yhat = predict(X, layers);\n",
    "acc = accuracy(Y, Yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9821428571428571"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dev accuracy\n",
    "Yhat_dev = predict(X_dev, layers);\n",
    "# Accuracy\n",
    "acc_dev = accuracy(Y_dev, Yhat_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$98.2$%, but heavily overfitting on the data and we can't really justify the extra cost in terms of running time when compared to the $0.5$% increase on the single layer model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9803333333333333"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test accuracy\n",
    "Yhat_tst = predict(X_test, layers);\n",
    "# Accuracy\n",
    "acc_tst = accuracy(Y_test, Yhat_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
